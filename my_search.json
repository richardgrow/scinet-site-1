[
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "ARS Training",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/ars",
      "description" : "ARS - specific opportunities",
      "date" : "",
      "content" : "This page is currently in development"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "FAQs",
      "category" : "Support",
      "subcategory" : "",
      "set" : "",
      "url" : "/support/faq",
      "description" : "Frequently asked Questions",
      "date" : "",
      "content" : "General InformationWhat is the difference between Ceres and SCINet?SCINet is a high-speed network connecting equipment like compute servers, data transfer nodes, scientific instruments, and workstations. Ceres is a high-performance compute cluster at the Ames location that is accessible to all SCINet users.Who is eligible for a SCINet account?ARS employees and those collaborators that are covered under a formal agreement with the ARS.How do I find documentation on Ceres and SCINet?The most up-to-date documentation is available on this website although some information is still in the process of being posted. If you can’t find what you need here, please also check the SCINet Forum (must have SCINet account to access).How much does a Ceres account cost?Ceres is currently free to USDA-ARS researchers and collaborators.How much does Amazon Web Services (AWS) cost?SCINet provides AWS at no cost to SCINet users with suitable workloads. A limited amount of funding is available annually. Interested SCINet users must submit a SCINet AWS Project Request to be considered for AWS funding.Who manages SCINet?Day to day operations and user support are provided by the Virtual Research Support Core (see What is the Virtual Research Support Core (VRSC)?). Higher level policy and development are managed by the SCINet Executive Committee, including the SCINet program manager Rob Butler, with input from the Scientific Advisory Committee (SAC; see What is the Scientific Advisory Committee (SAC)?)Who can I contact for help using SCINet?Single user operational questions should be emailed to the VRSC scinet_vrsc@usda.gov (see also How do I contact the VRSC for assistance?).Discussion that is relevant to other SCINet users such as questions about the best practices for research computations should be posted on the SCINet VRSC Forum. Access to SCINet Forum is provided during the SCINet account application process.Policy and development questions should be directed to the SCINet program manager, Rob Butler, or to your area representatives on the SCINet Scientific Advisory Committee (SAC), see the SCINet Organization page.How do I use Basecamp?Basecamp  has been replaced by SCINet Forum. SCINet documentation previously available on Basecamp can now be found in the User Guides section of the SCINet website/guides.How do I acknowledge SCINet in my publications?Add the following sentence as an acknowledgment for using SCINet/Ceres as a resource in your manuscripts or datasets meant for publication:“This research used resources provided by the SCINet project and the AI Center of Excellence of the USDA Agricultural Research Service, ARS project number 0500-00093-001-00-D.”Accounts &amp; LoginHow do I get an account (I am an ARS employee)?Please fill out the SCINet Account Request Form. The request will be forwarded to your supervisor for approval. You may want to notify your supervisor so they are aware of the incoming email that requires a response. Once your supervisor has granted approval, an account will be created for you and you will receive an email from the Virtual Research Support Core (VRSC) with logon information.How do I get an account for non-ARS collaborators, students, or postdocs?All students, postdocs, visiting scientists, and collaborators must have an ARS sponsor. All requests must also be approved by the Research Leader.Please fill out the Non-ARS SCINet Account Request Form. The request will be forwarded to the sponsor’s supervisor for approval. You may want to notify your supervisor so they are aware of the incoming email that requires a response.Collaborators also have access to this website for user guides and upon account approval, access to the VRSC for operational assistance and the SCINet VRSC Forum user forum for user discussion.How do I reactivate my account?Users whose accounts have been deactivated for inactivity will need to submit a new SCINet account request form.If you have questions, please email the VRSC: scinet_vrsc@usda.govHow do I reset or change my password?If your password is expired you should be prompted to change your password when you attempt to login. If you are still able to login, do so and type “passwd” you will be prompted for your old password and asked for a new one.Read more about logging in to SCINet and changing your password here.Watch a video demonstration of changing your password here.[first.last@ceres-login ~] $ passwd Changing password for user first.last Current Password:  New password: Retype new password: passwd: all authentication tokens updated successfully. If you have forgotten your login password, please email the VRSC: scinet_vrsc@usda.govWhat are the SCINet password requirements?   AT LEAST 14 characters long   Your last 24 passwords cannot be reused. How do I login to Ceres?Command line access is available via Secure Shell (SSH) and Virtual Private Network (VPN). For more info, see the Quick Start Guide.I took my onboarding a long time ago, how do I get a refresher course?Email the VRSC scinet_vrsc@usda.gov for a time and day. You can also read the Quick Start Guide or Ceres User Guide.SoftwareWhat software is available on SCINet?See the Ceres Preinstalled Software List for a list of available software. You can also login to Ceres or Atlas and issue the command “module avail” to see the same list of available software modules.How do I request software to be loaded onto Ceres?For new software requests, go to the Request Software page and follow the instructions which include 1) checking if the software is already installed, 2) considering whether to install software yourself, and 3) requesting new software to be installed as a module by the VRSC. Software requests sent to the VRSC require USDA Security Operations Center (SOC) review and approval which takes a few weeks.How do I install my own software programs?You may install your own software in your own directories, however, we strongly encourage users to contact the VRSC team to ensure that their required tool(s) might not be better distributed as a shared package within the official software modules tree.The popular R, Perl and Python languages have many packages/modules available. Some of the programming-language-specific packages are installed on Ceres and are available with the r/perl/python_2/python_3 modules. See the list of installed packages on the Software Overview page or use “module avail” at the command line. To see more information on a specific module, issue the “module help  command. Also see the [Installing R/Python/Perl Packages guide](/guides/analysis/r-perl-python).Another resource for installing your own software programs is the Conda package manager. See the User-Installed Software on Ceres Using Conda guideExperienced command-line users can also install open-source software locally in their project. We recommend installing packages in the project directories since collaborators on the same project most probably would need the same packages. In addition, home quotas are much lower than project directories quotas.Alternatively, one can use Singularity to run software container images (including Docker images). For more information, see the Singularity on Ceres guide.How do I compile software?Ceres has development libraries available on all nodes. There is a system version of gcc which is well maintained by CentOS. The VRSC also makes available modules with newer versions of gcc and the intel compilers. See the Preinstalled Software List for available versions or use  module avail intel  or  module avail gcc  at the command line to see all of the versions that are available at any given time to make use of them. To access the software use  module load gcc  or  module load intel . Note that after using these compilers you will probably need to load the modules again in the future to run the code you compiled as well.What is Galaxy?Galaxy is a web-based interface to software on Ceres in which users can build analysis workflows. See Getting Started with Galaxy on SCINet. General information about Galaxy as well as training guides can be found at galaxyproject.org.How do I login to SCINet Galaxy?Go to galaxy.scinet.usda.gov. Enter your email address and SCINet password followed by Verification code. See Getting Started with Galaxy on SCINet for more details.How do I request software to be loaded onto SCINet Galaxy?Please submit software install request at https://e.arsnet.usda.gov/sites/OCIO/scinet/Pages/SCINet-New-Application.aspx and check next to Galaxy Application.Storage SpaceHow much data can I store on Ceres?Each user is allowed 10GB of data in the home directory.Additional storage can be provided on Ceres and/or Atlas in /project directories. To request more storage see How do I request an increase in storage space?. Project directory storage is large, fast, not backed up, and can be requested up to 1TB or larger if justified. Project directory storage is good for fast I/O to large data files from compute nodes. Keep directory storage is smaller and slower, but is backed up nightly and can be requested up to 100GB or larger if justified. Keep directory storage is good for medium-term storage of analysis results and project software/scripts.When requesting additional data storage you will be asked for a project summary, a project end date, and to detail your long-term plan for data in your project/keep directories after your project end date.Besides /project data can be store in the short-term storage /90daydata which has no quotas. For more information refer to the SCINet Storage Guide.How do I request an increase in storage space?Please fill out an application form at Request Project Storage (eAuthentication required, non-ARS users should contact their sponsor) to request more storage.How do I request access to a project directory?Only project directory managers can request access to their project space for other users.Data TransferHow do I get my data onto and off of Ceres?SCP, SFTP, lftp (to/from Box accounts) Globus, and hard drive shipment are supported. This is described in the File Transfers section of the User Guides. If you work near Beltsville, you may also use the SCINet lab located at the National Agricultural Library (NAL). Two computers with 10GB Internet2 SCINet connections are available.How do I get my data onto Ceres via SCINet Galaxy?See Getting Started with Galaxy on Ceres.Support, Policy, O&amp;MWhat is the Virtual Research Support Core (VRSC)?The Virtual Research Support Core is a team of Iowa State University and ARS personnel who manage the maintenance and operation of the Ceres HPC system and provide user support. See the Virtual Research Support Core page and How do I contact the VRSC for assistance? for more details.How do I contact the VRSC for assistance?Contact scinet_vrsc@usda.gov for operational issues affecting a single user such as:   I can’t login   I’m having problems with a batch script   I’m have a problem with storage   How do I use software package X?   Please update software application X on Ceres.   Please add user X to project Y. Use the SCINet Forum message board (must have SCINet account to access) for discussions/questions about the best practices for research computations.For Galaxy questions, please use the same email contact scinet_vrsc@usda.govWho is the SCINet program manager?The current acting SCINet program manager is Rob Butler.What is the Scientific Advisory Committee (SAC)?The Scientific Advisory Committee is composed of ARS scientists across all the five physical Areas, NAL, and a representative from the Area Statisticians. These scientists serve 3-year terms on the committee and represent a broad spectrum of the computational research efforts within ARS. The SAC, which is also referred to as the executive SAC (eSAC), interacts directly with the SCINet Executive Committee on SCINet policy, development, and training.There is a call for new SAC members every spring. If you would like serve on the SAC, please discuss your interest with your supervisor and email the SAC Chair. Current SAC members can be found on the SCINet Organization page.Parallel ComputingHow do I write a batch script to submit a compute job?Please use the Ceres job script generator. The Atlas user guide provides similar generator for the Atlas cluster.How do I compile MPI codes?Load the module for the MPI library you wish to use, generally open openmpi, but mpich is available as well. MPI is included by default with the intel compiler.module avail openmpi module load openmpi The table below summarizes the relevant GNU compiler names and command line flags for serial, OpenMP and MPI codes.                       Serial       MPI       OpenMP       MPI+OpenMP                       Fortran       gfortran       mpif90       gfortran -fopenmp       mpif90 -fopenmp                 C       gcc       mpicc       gcc -fopenmp       mpicc -fopenmp                 C++       g++       mpicxx       g++ -fopenmp       mpicxx -fopenmp         Technical IssuesMy terminal window keeps freezing. Is there something I can do to stop this?As a result of the current networking infrastructure, working at the command line can be difficult sometimes because displaying hundreds of lines freezes the display. The solution to this is to enable SSH compression. There are a few different ways to do ssh using compression: do  ssh -C  from the command line; or instead of ssh in Putty, click on SSH on the left, then check Enable Compression In Unix; or alter your ~/.ssh/config file to contain these lines:Host ceres.scinet.usda.gov   Compression yes I log in at the command line but the system keeps logging me out. Is there something I can do to stop this?On Linux or Mac OS just create or add the following to a ~/.ssh/config file. If you use Windows Powershell to login to Ceres the config file will be located at C:/Users/username/.ssh/config.Host ceres HostName ceres.scinet.usda.gov User  ForwardAgent yes ForwardX11 yes TCPKeepAlive yes ServerAliveInterval 20 ServerAliveCountMax 30 That will send a “keepalive” signal every 20 seconds and keep retrying for up to 30 failures. This also simplifies your login to just: ssh ceresIf you don’t want to use the config file method you can just add these options to the ssh command:ssh -o TCPKeepAlive=yes -o ServerAliveInterval=20 -o ServerAliveCountMax=30 @ceres.scinet.usda.gov -XA Using PuTTY on windows you can do the same via the Connections tab. Set the “Seconds between keepalives” to 20 and check the “Enable TCP keepalives”"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet-Longterm Agroecosystem Research (LTAR) Phenology Working Group",
      "category" : "Research",
      "subcategory" : "Working Groups",
      "set" : "",
      "url" : "/research/working-groups/LTARphenology",
      "description" : "summary of the working group",
      "date" : "",
      "content" : "This working group was formed as the result of an August 2019 SCINet-funded workshop hosted by Dawn Browning. The focus of the group is to learn how to use SCINet high performance computing resources to expand the scope and impact of on-going collaborative phenology research. The group is devising workflows to assemble multiple diverse data streams and using SCINet computing resources to optimize agroecosystem models.Contact Dawn Browning for more information on this research and collaborative opportunities."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Apply To Serve on the SCINet Scientific Advisory Committee",
      "category" : "Opportunities",
      "subcategory" : "",
      "set" : "",
      "url" : "/opportunities/SAC",
      "description" : "Annual SAC call for new members",
      "date" : "",
      "content" : "The SCINet Scientific Advisory Committee (SAC) is seeking a representative from the Southeast Area for a 3-year term.The ARS Scientific Computing Initiative is in the dominion of ARS researchers, thus the program is in a constant state of gathering and meeting their research needs. The Scientific Advisory Committee (SAC) is an effort to assure that happens. SAC members represent a broad breadth of scientific research at ARS and membership includes two researchers from each of the five ARS geographic areas, a statistician member, and an “at-large” member. The SAC meets monthly to provide input on planning, educational and communication activities of the SCINet Initiative.  There is also opportunity for input on planning and policy initiatives.  For example, SAC members created a quarterly SCINet newsletter, facilitated a user-needs virtual meeting with scientific points-of-contact across ARS research units, launched the SCINet website, and hold multiple computation workshops and trainings annually.  SAC progress and issues are elevated to the Executive Committee for review and approval.Please consider sharing your time and talents on the SAC.  This is a 3-year term which begins Spring 2023 and will end Spring 2026.  Please reply to Kathleen Yeater (Kathleen.yeater@usda.gov) with nomination(s) by Friday February 17, 2023.  Self-nominations are accepted.  Please provide e-mail contact information of nominees, and the SAC Membership committee will continue with the application process for the SEA representative to SAC."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "How to Access Training on Different Platforms",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/access",
      "description" : "How to Access Training on Different Platforms",
      "date" : "",
      "content" : "There are a variety of free training resources available.  Here are instructions on how to access these resources on some of the commonly used platforms.We also have a curated list of availale free trainings.AgLearn and LinkedIn LearningAgLearn hosts multiple free learning courses and learning platforms including LinkedIn Learning and Skillsoft Percipio.SCINet has created a number of collections and learning paths in LinkedIn Learning. Login to LinkedIn Learning and and search the keyword SCINet for all curated collections.All federal USDA permanent and term employees should be able to access the courses listed on this free online training page by logging into LinkedIn Learning via AgLearn then following the posted links.Non-federal USDA employees may access AgLearn/LinkedIn Learning using the following steps:   Go to aglearn.usda.gov but DO NOT LOG IN   On the “Welcome to AgLearn” page, click on LinkedIn Learning   Follow the instructions to “access your LinkedIn Learning account directly” by following the link and logging in with your eAuthentication credentials. (Do not log in to AgLearn)   Find any of the AgLearn/LinkedIn trainings listed on this page by typing the title of the course in the LinkedIn Learning search bar. Non-federal USDA employees (contractors) may be granted access to other AgLearn courses (i.e. Percipio/Softskills) on a case-by-case basis, but this is not guaranteed due to licensing restrictions. Please have your supervisor contact your Agency AgLearn Point of Contact and provide a justification stating how a particular training will help the researcher carry out the mission of the agency.Contact your Agency AgLearn Point of Contact with further questions about how to access AgLearn courses.Coursera and EdXMany Coursera and EdX courses can be audited for free (i.e. no certification which requires a fee). For Coursera, licenses are available to  ARS scientists and support staff through the SCINet Program (see below for details).For EdX click enroll on a particular course, create an EdX account, and choose the free audit option.For Coursera see the full list of free Coursera courses. These courses offer a ‘Full Course,  No Certificate’ option. This option lets a user see all course materials, submit required assessments, and get a final grade, but does not  provide a certificate.ARS scientists and staff also have the option to take some courses in audit mode, which provides visibility of most course materials for  free but does not provide access to graded assignments and a Certificate.If the audit option is not avaiable or a certificate is required, ARS Scientists and staff can request a  SCINet-sponsored Coursera license, which provides 3-months of full access to Coursera.  If more than 3 months is needed to complete a course, a new request will need to be submitted the following quarter to secure the license  for another 3 months.SCINet-funded Coursera Licenses: ARS scientists can request a Coursera license for a three month period to gain access to the full list of Coursera catalogue options related to scientific computing, statistics, and AI. This license also allows ARS scientists to get certifications. To request a license, please visit the SCINet Coursera page.SAS and JMPWhile links below to sas.com and jmp.com videos are immediately accessible, SAS and JMP course offerings on sas.com and jmp.com (not the courses offered through AgLearn) require the learner to create a free account with their email address and password.To start a course on sas.com:  Follow the link to the course from this page, scroll down the course overview page to the “Self-Paced e-Learning” section, under “Add to Cart” on the right click “Start”, follow the instructions to create a profile for a new user.To start a course on jmp.com:  Follow the link to the course from this page, from the JMP course page click “Enroll Now”, follow the instructions to create a profile for a new user."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Ag100Pest Initiative Working Group",
      "category" : "Research",
      "subcategory" : "Working Groups",
      "set" : "",
      "url" : "/research/working-groups/ag100pest",
      "description" : "summary of the working group",
      "date" : "",
      "content" : "As a subgroup of the Arthropod Genomics Research Working Group, the Ag100Pest Initiative’s goal is to produce annotated, reference quality genome assemblies for the top 100 US arthropod agricultural pests by leveraging ARS’s unique expertise in both arthropod pest management and agricultural genomics research. We are pushing the limits of current technology to produce long-read sequencing data from individual specimens, despite the challenges of small body sizes, and sometimes large genomes, presented by arthropods. Arthropod pests of US field crops, livestock, bees, trees, and stored products as well as foreign pest species considered potential invasive threats to US agriculture are being considered.Ag100Pest teams are developing best practices in many arenas from process and data management to extraction, sequencing, assembly, and annotation that will benefit the entire genomics community. As an example, we collaborated in the development of an open-source functional annotation workflow specifically tailored to arthropods, which users can find installed on Ceres or available through CyVerse, Docker, and GitHub.This initiative enhances the agency’s contribution to two international genome sequencing projects – i5K (the 5000 insect genomes initiative) and the Earth BioGenome Project (to sequence the genomes of all of the world’s 1.5 million animals and plants).               Executive Coordination Team       Role                       Anna Childers       Assembly Team Lead                 Sheina Sim       Genome Analysis Lead                 Brian Scheffler       Technical Lead                 Kevin Hackett       National Program Leader                        Core Leadership Group       Role                       Brad Coates       Technology Transfer Team Lead                 Scott Geib       Extraction/Pre-Sequencing Team Lead                 Brian Scheffler       Sequencing Team Co-Lead                 Tim Smith       Sequencing Team Co-Lead                 Anna Childers       Assembly and Analysis Team Lead                 Monica Poelchau       Post-Assembly Team Co-Lead                 Chris Childers       Post-Assembly Team Co-Lead                 Kevin Hackett       National Program Leader         Please contact a member of the Executive Coordination Team for more information."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "ARS AI Innovation Fund - FY2021 Awards",
      "category" : "Opportunities",
      "subcategory" : "AI Innovation Fund",
      "set" : "",
      "url" : "/opportunities/ai-innovation/fy21-awards",
      "description" : "Abstracts of AI Innovation Fund proposals funded in FY2021.",
      "date" : "",
      "content" : "The AI CoE was able to fund four AI Innovation Fund proposals in FY2021.  The program was very competitive, with many more proposals submitted (nearly 60!) than we could support.  Information about the funded projects is provided below.Funded proposalsExplainable Deep Learning-Based Image Analysis With Blackbird RGB Imaging Robot For Laboratory High Throughput Phenotyping   PI and Co-PIs: Lance Cadle-Davidson, Yu Jiang   Amount of award: $100,000.00   Abstract: We developed and in 2021 will commercialize the Blackbird computer vision platform, which images 200 samples per hour at 1-micron resolution for deep learning-based analysis of foliar disease severity, with better accuracy than manual microscopy.  To broaden its impact to other foliar traits (eg, trichomes, stomata), tissues (eg, roots, flowers, seeds, small fruits), and organisms (eg, insects, nematodes, fish embryos), we are adapting different lenses, sensors, and grids for imaging. Here we propose to develop a documented, containerized platform for model development and deployment and image analysis at SCINet. To test its implementation in ARS labs, this project will provide two Blackbird robots to labs selected by NP301 in consultation with Breeding Insight and will guide those labs through imaging and data analysis. In the process, we will develop training materials to facilitate widespread deployment of Blackbird in ARS and university labs as a uniform phenotyping platform. Automated Detection Of Prairie Dog Colonies From Airborne Imagery Using Deep Learning   PI and Co-PIs: David Augustine, Justin Derner, Lauren Porensky   Amount of award: $83,790.00   Abstract: Black-tailed prairie dogs are keystone species for rangeland ecosystem health in semi-arid environments, but their colonies and populations of individuals are susceptible to dramatic boom and bust cycles.  Prairie dogs alter the vegetation quantity with their grazing and burrowing activities, which presents competition for grazing livestock.  As a result, public lands managers use control measures to prevent the expansion of colonies to adjacent private properties, as well as to maintain adequate forage for grazing livestock. These management decisions require detailed information on how colony boundaries are changing over time, but such information is cost-prohibitive to collect on the ground to the dynamic nature of colonies and the vast spatial extent across which they are found. This hinders the effectiveness of interventions and makes it difficult to communicate with ranchers, conservation groups, and other stakeholders about why and how decisions are being made. We propose to develop an algorithm based on deep learning, specifically a deep convolutional neural network (DCNN), to delineate prairie dog colony boundaries from high spatial resolution airborne imagery. Since DCNN’s can detect high-level features in imagery based on multi-scale patterns and morphology, we hypothesize that an algorithm can be developed to detect the distinct spatial patterns created by prairie dog colonies through burrowing and defoliation. In addition to optimizing the DCNN architecture itself, we will also identify which input layers are needed and evaluate how performance changes with coarsening spatial resolution to identify appropriate sensors and/or flight altitudes to maximize the efficiency of future monitoring. Deep Learning-Based 3D Fruit-Tree Perception through Efficient Multi-Sensor Fusion for Robotic Harvesting of Apples   PI and Co-PIs: Renfu Lu, Zhaojian Li   Amount of award: $99,240.25   Abstract: Harvest automation is critically needed for addressing labor shortage and increasing labor cost, worker safety, and profitability and sustainability of the U.S. apple industry. This project is intended to develop a unified, efficient perception system to provide 3D fruit and tree branch information to support real-time robotic planning and controls. Built on our preliminary work for a single-sensor perception system, we will evaluate multi-sensor perception systems to substantially improve apple detection and localization performance by extending the 3D detection and localization capability to tree trunk and branches. To that end, we propose two sensing paradigms: 1) multiple RGB-D cameras from different angles of viewing; and 2) camera-LiDAR fusion. We will investigate novel pre-fusion and post-fusion strategies to systematically combine these sensors for enhanced fruit and tree branch detection and localization. We will develop algorithms for efficient dynamic moving window perception to support real-time robotic operation on a resource-limited platform. Extensive evaluations of the new multi-sensor systems will be performed in both artificial and real orchards under different lighting and canopy conditions. This research is expected to provide comprehensive orchard databases with multi-modal sensors and efficient deep learning based fruit-tree perception algorithms for robotic harvesting of apples. Adapting Deep Learning For Three-Dimensional Mapping Of Soil Carbon   PI and Co-PIs: Kristen Veum, Curtis Ransom, Ken Sudduth, Newell Kitchen   Amount of award: $86,345.00   Abstract: Agricultural lands can be a sink for carbon and play an important role to help the United States become carbon neutral. Current methods of measuring carbon sequestration -through repeated temporal soil samples- are costly and laborious. A promising alternative is using visible, near-infrared (VNIR) diffuse reflectance spectroscopy. However, VNIR data are complex, which requires several data processing steps and often yields inconsistent results, especially when using in situ VNIR measurements. Using a convolutional neural network (CNN) could bypass these steps and incorporate measurements from multiple sensors to predict three-dimensional carbon stocks. A CNN modeling framework will be developed to predict soil carbon by incorporating information from profile VNIR, apparent electrical conductivity (ECa), penetration resistance measurements, and spatial covariates (e.g., mobile-sensor ECa and topography). Improvements over traditional modeling methods will be reported. Additional objectives include evaluating the optimal spatial density of sensor measurements needed to use the CNN model to estimate soil carbon. These models will be used to develop three-dimensional (down to 1 m) estimates of soil carbon which will quantify carbon sequestration over five years from fields with contrasting management history. "
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "ARS AI Innovation Fund - FY2022 Awards",
      "category" : "Opportunities",
      "subcategory" : "AI Innovation Fund",
      "set" : "",
      "url" : "/opportunities/ai-innovation/fy22-awards",
      "description" : "Abstracts of AI Innovation Fund proposals funded in FY2022.",
      "date" : "",
      "content" : "The ARS AI Center of Excellence (AI-COE) funded five AI Innovation Fund proposals in FY2022.  The program was again very competitive, with many more proposals submitted (&gt;30) than we could support.  Information about the funded projects is provided below.Funded proposalsBeeMachine 2.0: development of machine learning approaches to rapidly identify bee museum specimens from digital images   PI and Co-PIs: Michael Branstetter, Brian Spiesman, Terry Griswold, Jonathan Koch   Amount of award: $52,555   Abstract: Bees have great value as pollinators of crops, natural plant communities, and backyard gardens. Identifying factors that affect bee diversity are therefore important for ensuring pollination services and food security. The global decline of bees, especially in agricultural regions, underscores the need for conservation research, including survey and monitoring efforts. A major impediment for studying bees, however, is that they are extremely diverse, with &gt;4,000 North American species, and they are challenging to identify, especially by non-specialists. One potential solution to solve the identification impediment is the automated identification of bee species from digital images using machine learning and computer vision approaches. Such methods have been recently tested using field images of U.S. bumble bee species. The project was successful and the results were deployed in a web application called “BeeMachine.” Although promising, extending the approach to all bee species will be a major undertaking requring the generation of many more digital images of expertly identified specimens. We propose a collaboration between BeeMachine, the U.S. National Pollinating Insects Collection, and SCINet to improve the ability of deep learning models to automate bee identification from images. Focusing on three bee genera of agricultural importance (Bombus, Osmia, Megachile), we will generate digital images of 15,000 museum specimens representing 300+ species, and we will train a convolutional neural network using SCINet resources. We will use a subset of the images to validate the approach and check the accuracy of the model. We will also compare image-based identification to DNA-based methods by sequencing the barcode gene for a subset of specimens. This research will move the bee research community closer to an all-bee classifier and expand bee-identification tools. All images will be archived and results will be integrated online in the BeeMachine web app. Geospatial Artificial Intelligence (GeoAI) for spatiotemporal modeling of pest insects   PI and Co-PIs: John Humphreys, Guofeng Cao, Bob Srygley, Dave Branson   Amount of award: $100,000   Abstract: Emerging GeoAI and deep-learning methods hold significant promise to revolutionize how agricultural problems are addressed, however the potential to adapt these methods to insect pest detection and management remains largely untapped and is often confounded by the statistical complexity of analyzing enormous quantities of multi-scale, multi-source data (Big Data) that exhibit complex spatiotemporal structure. We propose to leverage the SCINet high-performance computing infrastructure to aid in the development of new uncertainty-aware, GeoAI-based tools that employ neural network algorithms, deep learning methods, and related machine learning tools. These new tools will be used to characterize and model the complex spatiotemporal patterns connected to pest grasshopper outbreaks across the Western US. Grasshoppers are the most impactful rangeland insect pests in the US, cause rangeland forage losses of about $1.67 billion annually, and routinely inflict major crop damage. Grasshopper outbreaks in 2021 occurred at near record levels, caused serious economic injury in many states, and prompted massive public interest in agricultural pest management.  For 2022, more than 80 million acres in the West are anticipated to be at economic risk with millions of dollars already requested for treatments and chemical control on federal lands. The combined use of multispectral imagery, derived environmental data sets, and our novel uncertainty-aware approach will enable the distinctive signature of grasshopper habitat to be identified while concurrently quantifying grasshopper density variation at the local scale to improve pest outbreak forecasts. Modernizing Dietary Assessment: Adapting Deep Learning to Predict Ingredients from Food Photos   PI and Co-PIs: Danielle Lemay, Hamed Pirsiavash   Amount of award: $99,157   Abstract: Current dietary assessment methods are extremely burdensome for participants and contain substantial errors and biases. Artificial intelligence methods can be used to identify food from photo-based food diaries, which would enable convenient and real-time dietary data capture. Specifically, identification of individual foods and ingredients in photos are necessary for accurate dietary analysis and mapping to food and nutrient databases. Existing deep learning algorithms that predict recipes and ingredients from photos are promising but are trained on data that do not reflect eating patterns and food consumed “in the wild.” These methods need to be evaluated and adapted to predict ingredients from food photo diaries for dietary assessment. We propose to evaluate, compare, and adapt existing deep learning algorithms for prediction of ingredients from real-world food photos from our SNAPMe human nutrition study, which contains food photos paired with traditional food recalls (text-based) collected from healthy human participants. To improve prediction of food diaries from human studies, we will then fine-tune the most promising model on data with a variety of food types and food representations, including multi-cultural foods, core (single ingredient) foods, and food photos collected from human study participant smartphone cameras. This project will provide an algorithm for ingredient-level prediction from food photo diaries used for dietary assessment, and an evaluation of that algorithm on real-world benchmark data. Microbial Water Quality Determinations using Machine Learning: Application and Algorithm Comparison   PI and Co-PIs: Yakov Pachepsky, Matthew Stocker   Amount of award: $100,000   Abstract: During irrigation events, dangerous microorganisms can be transported to crops and soils with contaminated waters. To assess the potential presence of pathogenic microorganisms in irrigation waters, the concentrations of the fecal indicator bacteria, Escherichia coli (E. coli), are routinely measured and are compared to regulatory standards. However, methods of enumerating E. coli are labor-intensive and costly and the long analysis time means results may not be representative of current conditions. For these reasons, researchers often elect to develop predictive models for E. coli concentrations based on the levels of physicochemical, hydrological, and meteorological variables, which can be rapidly and easily measured. However, due to the complexity of aquatic ecosystems and nonlinearity in relationships between predictor variables and E. coli in surface waters, traditional statistical approaches often struggle to effectively characterize the dependency of E. coli concentrations on the measured predictor variables. Recently, Machine Learning (ML) algorithms have been shown to provide timely and accurate predictions of the microbial quality of recreational and drinking water sources. Still, little work has been done to apply ML to irrigation water quality. The objectives set in this proposal are to 1) comprehensively evaluate over 100 ML algorithms for the prediction of E. coli in irrigation waters and 2) to determine the most important environmental variables governing the presence of E. coli in irrigation waters. We propose analyzing one of the largest microbial water quality databases in existence which was generated in the USDA-ARS Environmental Microbial and Food Safety Laboratory and contains over 95,000 unique measurements of E. coli and environmental variables collected at multiple field sites from 2016 to 2021. This research is expected to create powerful ML models which will be used to expedite and improve microbial water quality determinations and ultimately food safety in the United States and abroad. Putting AI technology into the hands of farmers: Developing an app to make intelligent decisions using deep learning   PI and Co-PIs: Zhanyou Xu, Jo Heuschele, Zhou Zhang   Amount of award: $100,000   Abstract: Alfalfa is the third most valuable field crop after corn and soybean in the U.S., valued at about $10 billion annually. Climate, soil, and genetics affect alfalfa’s digestibility and biomass yield differently, with the paradox of high yield but low digestibility or high digestibility but low yield. Finding the optimal harvest window for desired yield and digestibility is one of the most critical decisions alfalfa farmers must make four to seven times every year. Low digestibility in alfalfa limits dry matter intake and energy availability in ruminant (dairy and beef) production systems, while low biomass yield reduces farmers’ profitability. Measuring forage digestibility is labor and resource-intensive, often with long turnaround times to obtain data. Computer vision-based deep learning analysis provides the opportunity to quickly predict digestibility and yield to help farmers and researchers make intelligent decisions. The goal of this proposal is to develop a mobile application decision tool (a cell phone app) using three convolutional neural networks (CNNs), Google Net, VGG-Block, and Reset with Google deep learning system “TensorFlow.” The app will have the capacity to 1) estimate alfalfa digestibility and yield in real-time; 2) measure the stems and plants per square foot to forecast yield potential and nitrogen credits to support decisions on stand termination; and 3) predict the harvest window for yield and quality goals taking into the account historical weather patterns and weather forecasts. Ultimately, the app will help farmers make AI-based decisions for increased economic returns for alfalfa production. The expected product of this proposal, a beta testing version 0.5, will be available for ARS internal testing in 2022. The farmer version 1.0 of the app will be launched as a free download for use by the end of 2023 in the Apple Store and 2024 in the Google Play Store. "
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "ARS AI Innovation Fund (FY23)",
      "category" : "Opportunities",
      "subcategory" : "",
      "set" : "",
      "url" : "/opportunities/ai-innovation/",
      "description" : "Internal USDA-ARS grants for artificial intelligence and machine learning projects",
      "date" : "",
      "content" : "OverviewThe goal of the ARS Artificial Intelligence Center of Excellence (AI-COE) is to enable ARS science by promoting the adoption and use of AI and machine learning (ML) tools and methods in agricultural research. For FY23, the AI-COE expects to fund 4 to 6 proposals at up to $100,000 each for activities to encourage and promote AI-related research in agriculture.  For examples of successful proposal topics, please see the abstracts of the AI Innovation Fund proposals funded in FY2022 and in FY2021.Proposals must be submitted using the online submission form and are due by close of business on Friday, December 23, 2022. All submitted proposals must be approved by a relevant RL or supervisor. We expect that funds will be available for use some time in early 2023 and will need to be spent or placed in collaborative agreements by the end of FY2023. We expect to fund additional proposals each of the subsequent years. For questions, contact Dr. Brian Stucky, Computational Biologist in the SCINet Office and Acting ARS CSIO.Proposal guidelinesScopeProjects of high priority for funding are those that:   Develop or adapt an AI/ML method that empowers ARS scientists to answer a specific question/problem or test a hypothesis of agricultural importance.   Develop or adapt AI/ML technologies to create a prototype digital product that solves a need for producers or agricultural researchers. Proposals should be primarily focused on developing, adapting, or applying methods that fall into the category of AI or ML (see definitions below).  Please refer to the abstracts of the AI Innovation Fund proposals funded in FY2022 and in FY2021 for examples of successful proposal topics.Researchers developing a method or digital product are encouraged to define a minimum viable product as a deliverable.Successful projects should:   Address real-world model concerns, such as data shift.   Have AI/ML development or modification as a primary focus.   Demonstrate that the project has a high probability of completion with impacts on an agricultural research question/problem.   Utilize SCINet computing resources, including our high-performance computing (HPC) clusters, Ceres and Atlas. These HPC systems are equipped with standard CPU nodes, graphical processing units (GPUs), and high-memory nodes. SCINet’s clusters support a wide range of modern AI/ML software and workflows, including containerization technologies. Topics that will not be considered for fundingTraining, workshop, and working group activities are not supported by this call. Please see the SCINet web site for ways to get involved in these activities or contact the Acting Chief Science Information Officer, Brian Stucky, with questions (ARS-CSIO@usda.gov).Project FundingWe expect to fund 4 to 6 proposals up to $100,000 each. Funds must be spent in fiscal year 2023, which may require an agreement with a university partner or the Oak Ridge Institute for Science and Education (ORISE).Proposal format and submissionAll proposals must be submitted using the online submission form.  The PI’s RL or supervisor must approve the proposal prior to submission (approval will be indicated on the submission form).  A complete proposal will include:   A proposal abstract of 300 words or less.   Proposal text (project description) of up to 2 pages in length that clearly lays out a specific challenge or question, proposes a method or tool to be developed or applied to solve the challenge or to answer the question, and demonstrates that the project team has the capability to complete the project.  Deliverables for the project should be defined.   A detailed project budget provided as an Excel spreadsheet. Please use REE budget form 455.   A budget explanation of no more than 1 page. The proposal text and budget justification should be submitted as PDF documents with margins of no less than 1 inch and font size of no less than 11.  References are not included in the 2-page limit for the proposal text.  Only one proposal as the lead investigator responsible for project completion can be submitted by a scientist, although a scientist can be a member of multiple proposals. We encourage teams of investigators collaborating on a problem.Deadline for proposal submission: Close of business on Friday, December 23, 2022.Eligibility: ARS Category 1, 4, or 6 scientists with RL or supervisor approval.  Please note that FY22 AI Innovation Fund awardees will not be eligible for an FY23 award.Submission form: All proposals must be submitted online at https://forms.office.com/g/c6s4AR7BnG).Definition of AI/ML technologies(These are examples and not inclusive of all possible methods and tools.)AI methods involve automated decision-making or inference from data, and use methods in the subfields:   machine learning (including deep learning)   mathematical optimization (integer programming and operations research)   machine reasoning and logic programming   knowledge representation   recommender systems Machine learning involves training a model with data and then making decisions or answering questions using that model. ML methods include:   tasks like classification, regression, dimensionality reduction, and clustering;   domain areas like natural language processing, computer vision, and time-series analyses;   statistical methods like Gaussian processes, neural networks, Bayesian networks, and support vector machines. "
  },
  
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Announcements",
      "category" : "News",
      "subcategory" : "",
      "set" : "",
      "url" : "/news/announcements",
      "description" : "",
      "date" : "",
      "content" : "Various announcements from the Virtual Research Support Core to SCINet users. See also the SCINet Forum Announcements page (must have a SCINet account to access).For system downtime information see the System Downtime page for planned downtime and SCINet Forum Announcements page (must have a SCINet account to access) about unplanned outages/system status."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Arthropod Genomics Research (AGR) Working Group",
      "category" : "Research",
      "subcategory" : "Working Groups",
      "set" : "",
      "url" : "/research/working-groups/arthropods",
      "description" : "summary of the working group",
      "date" : "",
      "content" : "The AGR working group focuses on outreach within USDA-ARS aimed at enhancing the genomic analyse knowledge and skill sets of members and collaborators. This is delivered through monthly teleconferences and webinars by internal and external scientists focused on genome assembly, gene and protein expression, population genomics, and reverse genetic (RNAi and CRISPR) approaches. The AGR was initiated following two organizational meetings in 2014 (AGR Workshop I) and 2016 (AGR Workshop II), at which participants communicated needs and devised solutions for challenging analytical pipelines. The AGR facilitates internal and external collaborations by merging those with complementary knowledge, skills, and capabilities within a given research area. Collaborations among AGR members have also resulted in two published papers reviewing ARS arthropod genomics research1 and developing RNAi and CRISPR technologies2.  Work within the AGR includes the Ag100Pest Initiative, which aims to provide reference quality genome sequence assemblies for arthropod pests of U.S. agricultural commodities.The AGR is open to anyone within ARS, and those interested in joining can contact Brad Coates.        Coates BS, Poelchau M, Childers C, Evans JD, Handler A, Guerrero F, Skoda S, Hopper K, Wintermantel WM, Ling KS, Hunter WB, Oppert B, Perez de Leon AA, Hackett K, Shoemaker D. 2015. Arthropod genomics research in the United States Department of Agriculture-Agricultural Research Service:  Current impacts and future prospects.  Trends in Entomology 11:1-27. (opens pdf in this browser window)           Gundersen-Rindal D, Adrianos S, Allen M, Becnel JJ, Chen YP, Choi MY, Estep A, Evans JD, Garczynski S, Geib SM, Ghosh SKB, Handler AM, Hasegawa DK, Heerman M, Jull J, Hunter W, Kaur N, Li J, Li W, Ling KS, Nayduch D, Oppert B, Perera OP, Perkin L, Sanscrainte N, Sim S, Sparks M, Temeyer K, Vander Meer R, Wintermantel WM, James R, Hackett K, Coates BS. 2017. Arthropod genomics research in the United States Department of Agriculture-Agricultural Research Service: Applications of RNA interference and gene editing in pest control. Trends in Entomology 13: 109-137. (opens pdf in this browser window)    SymposiaArthropod Genomics Symposium X (AGSx) Virtual Spring Symposium 2022Online: http://i5k.github.io/agsx2022  Webinar postings: https://www.youtube.com/c/i5k_community/videosThe ARS AGR working group along with the i5K Community, an international consortium committed to the sequencing of 5000 arthropod genomes, hosted the AGSx Virtual Symposium Spring 2022 from February through May, 2022. This was the second annual AGSx event. The 2022 AGSx was comprised of four sessions co-organized and moderated by ARS AGR members and researchers from other domestic and international research institutions. Each session highlighted presentations from subject matter experts, including four from ARS researchers, and facilitated open discussions and technology transfer.   Insect genomic Technologies to Improve Food Applications, by Dr. Brenda Oppert, USDA-ARS   Honeybee Workshop, by Dr. Alain Vignal, and Dr. Sonia Eynard, INRAE, France   Arthropod Genomics and Genome Engineering, by Dr. Lindsey Perkin, USDA-ARS   Application of New Genomics Tools and Techniques in Arthropods by Dr. Marcé Lorenzen, North Carolina State University There were 321 participations across all four AGSx 2022 sessions. The majority of attendees were from the United States, but AGSx reached a global audience inclusive of scientists in Asia, Europe, South America, and Africa.               Location       Number of Participants                       North America       234                 Europe       57                 Asia       15                 South America       8                        Location       Number of Participants                       Argentina       3                 Belgium       1                 Brazil       3                 Canada       6                 China       1                 Columbia       5                 Czech Republic       1                 Finland       2                 France       14                 Germany       7                 Greece       5                 India       2                 Indonesia       1                 Israel       1                 Kenya       1                 Netherlands       1                 Nigeria       1                 Pakistan       5                 Portugal       4                 Saudi Arabia       1                 Singapore       2                 Slovenia       1                 Spain       3                 Sudan       1                 Switzerland       10                 Thaiwan       2                 Turkey       1                 United Kingdom       8                 United States of America       228         Webinars from each session were recorded and made available on the i5K Community YouTube channel. Posting of related materials were made to the i5k Arthropod Genomics Community Slack Workspace.The AGSx was made possible by the Organizing Committee and Production Team   Brenda Oppert, USDA-ARS, Manhattan, KS, USA   Alain Vignal, INRAE, Castanet Tolosan, France   Sonia Eynard, INRAE, Castanet Tolosan, France   Lindsey Perkin, USDA-ARS, College Station, TX, USA   Marcé D. Lorenzen, North Carolina State University, Raleigh, NC, USA   Glenn Hanes, USDA-ARS, Beltsville, MD, USA   Anna Childers, USDA-ARS, Beltsville, MD, USA   Robert Waterhouse, University of Lausanne, Switzerland   Brad Coates, USDA-ARS, Ames, IA, USA Arthropod Genomics Symposium X (AGSx) Virtual Spring Symposium 2021Online: http://i5k.github.io/agsx2021  Webinar postings: https://www.youtube.com/c/i5k_community/videosThe ARS AGR working group along with the i5K Community, an international consortium committed to the sequencing of 5000 arthropod genomes, helped the AGSx Virtual Symposium Spring 2021 from February through May, 2021. There were four sessions co-organized and moderated by ARS AGR members and researchers from other domestic and international research institutions. Session highlighted presentations from subject matter experts in aspects of arthropod genomics, use of insects as food, genome engineering, now tools and techniques, and a session focusing of honeybees.   Insect genomic Technologies to Improve Food Applications, by Dr. Brenda Oppert, USDA-ARS   Arthropod Genomics and Genome Engineering, by Dr. Lindsey Perkin, USDA-ARS   Application of New Genomics Tools and Techniques in Arthropods by Dr. Marcé Lorenzen, North Carolina State University   Honeybee Workshop, by Dr. Alain Vignal, and Dr. Sonia Eynard, INRAE, France There were 628 participations across four AGSx 2021 sessions. Although the majority of attendees were from the United States, the AGSx reached a global audience inclusive of many scientists in Asia, Europe and South America.               Location       Number of Participants                       North America       336                 Europe       124                 Asia       76                 South America       40                 New Zealand       2                 Africa       6                        Location       Number of Participants                       Algeria       9                 Argentina       5                 Bangladesh       1                 Belgium       1                 Brazil       8                 Canada       17                 Chile       1                 China       13                 Columbia       1                 Czech Republic       1                 Denmark       2                 France       32                 Germany       21                 Greece       3                 India       2                 Ireland       1                 Israel       2                 Italy       9                 Japan       2                 Malaysia       3                 Mexico       3                 Netherlands       27                 New Zealand       3                 Nigeria       2                 Pakistan       61                 Panama       1                 Poland       2                 Portugal       14                 Saudi Arabia       1                 Slovenia       2                 South Africa       3                 Spain       4                 Sweden       1                 Switzerland       6                 Thailand       1                 United Arab Emirates       1                 United Kingdom       29                 United States of America       321         Webinars from each session were recorded and made available on the i5K Community YouTube channel. Posting of related materials were made to the i5k Arthropod Genomics Community Slack Workspace."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Carpentries Training",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/carpentries",
      "description" : "Carpentries courses",
      "date" : "",
      "content" : "SCINet is excited to sponsor Data Carpentry and Software Carpentry courses.The Carpentries teaches foundational coding and data science skills to researchers worldwide. These courses are offered to ARS staff looking to advance their computational skills sets in support of their research. ARS staff and skilled Carpentries instructors will be available during these hands-on trainings to facilitate your learning.Sign up to be notified about future training opportunities at https://forms.office.com/g/tVtE8wEgAtIf you are interested in becoming a Carpentries Instructor, please email SCINet-training@usda.gov.   Upcoming SCINet Trainings "
  },
  
  
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet High-Performance Computer Systems",
      "category" : "About",
      "subcategory" : "",
      "set" : "",
      "url" : "/about/compute",
      "description" : "SCINet High-Performance Computer Systems",
      "date" : "",
      "content" : "Ceres HPC Cluster | Ames, IACeres is an ARS-owned high-performance computing (HPC) cluster connected to SCINet and located in Ames, IA. The original cluster build included 72 regular compute nodes, 5 high memory nodes, and a two Petabyte file system for a range of scientific applications. The cluster has been extended multiple times and currently has 196 regular compute nodes, 26 high-memory nodes and one GPU node. A small subset of nodes, called “priority” nodes, has been funded by Research Units. “Priority” nodes are available to all ARS users when not in use by their funders. The ‘Technical Overview’ in the Ceres User Manual describes the number of logical cores and available memory for each type of node.All nodes run on Linux Centos and compute jobs are managed by the SLURM scheduler. The system is configured with a login node, which users access to submit compute jobs to the SLURM scheduler.Compute jobs are run on functional groups of nodes called partitions or queues. Each partition has different capabilities (e.g. regular memory versus high memory nodes) and resource restrictions (e.g. time limits on jobs). There are 10+ different partitions on Ceres to which users can submit compute jobs. For details on the current partitions/queues, see ‘Partitions or Queues’ in the User Guides.Members of the SCINet Virtual Research Support Core (VRSC) in Ames, IA are the operational administrators of the Ceres HPC cluster. The VRSC maintains the system hardware, software, and provides user support. To learn more about the VRSC, visit the VRSC page. For a current list of command-line software managed by the VRSC through the module system, see the Preinstalled Software List.All Ceres users (ARS and formal collaborators) receive 5GB of storage in their /home directory and can request additional storage in a /project directory.Additional technical information about the Ceres HPC cluster can be found in the Ceres User Manual.New users may also want to reference the Quick Start Guide.Many additional user guides are available under the “User Guides” dropdown list.Other helpful links:  Apply for a SCINet Account to access Ceres  SCINet/Ceres FAQs  Request Project Storage  Request Software Atlas HPC Cluster | Starkville, MSARS researchers also have access to a Cray system in Starkville, MS through an agreement with the Mississippi State University (MSU) to supplement Agency scientific computing capacity. Same SCINet credentials are used to connect to Atlas as to other SCINet HPC clusters.Atlas cluster consists of 228 standard compute nodes, 8 high memory compute nodes and 4 GPU nodes. All nodes run on Linux Centos and compute jobs are managed by the SLURM scheduler. The system is configured with a login node, which users access to submit compute jobs to the SLURM scheduler. More information about Atlas cluster can be found in the Atlas User Guide. Also, ARS has an agreement with MSU that supports the Advancing Agricultural Research through High-Performance Computing project.SCINet Cloud Computing ServicesIn addition to the ARS in-house compute infrastructure, cloud computing services are available to augment the HPC capability of Ceres with cloud resources.  Request SCINet Cloud Computing"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Contact Information",
      "category" : "About",
      "subcategory" : "",
      "set" : "",
      "url" : "/about/contact",
      "description" : "contact information for website, newsletter, policy/development, and technical assistance",
      "date" : "",
      "content" : "Email scinet_vrsc@USDA.GOV for:   questions or feedback about this website   questions about the SCINet newsletter or to contribute content   technical assistance with your SCINet account All inquires for help from the SCINet Office should be sent to SCINet-training@usda.gov or the CSIO.To contact SCINet users, post on the SCINet forum (you must have a SCINet account to access)"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Contact the VRSC",
      "category" : "Support",
      "subcategory" : "",
      "set" : "",
      "url" : "/support/contact",
      "description" : "Responsive support of SCINet is provided by a VSRC",
      "date" : "",
      "content" : "The Virtual Research Support Core (VRSC) is a team of Iowa State University and ARS personnel who manage the maintenance and operation of the Ceres HPC system and provide user support. See the Virtual Research Support Core page for more details.How and When to Contact the VRSCDiscussion that is relevant to other SCINet users such as questions about the best practices for research computations should be posted on the SCINet Forum message board. Access to the Forum is provided during the SCINet account application process (Apply for an Account).If your question is related to SCINet policy or development, contact the acting SCINet Program Manager, Stan Kosecki.Single user operational questions should be emailed to the VRSC at Ceres or Atlas. Support requests for login issues on either Ceres or Atlas should be sent to scinet_vrsc@usda.gov.  Examples of the kinds of questions/requests that should be sent to the support email addresses include:   I can’t login   I’m having problems with a batch script   I’m have a problem with storage   How do I use software package X?   Please update software application X on Ceres.   Please add user X to project Y.   I need help with script/program optimization or making a program run faster Again, please note that support requests for login issues on either Ceres or Atlas should be sent to scinet_vrsc@usda.gov.For new software installation requests, see Request Software Installation on SCINet.For new SCINet AWS project requests, see Request SCINet AWS."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Coursera Training",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/coursera",
      "description" : "Coursera courses and specializations with ARS provisioned licenses",
      "date" : "",
      "content" : "SCINet and the AI Center of Excellence are excited to provide training opportunities through Coursera. While many Coursera courses can be audited for free, ARS scientists and support staff can gain free access to a wider range of courses by applying for a 3-month SCINet-sponsored license. Successful completion of courses and specializations may result in widely recognized certificates and credentials.  Programs of StudyThe ARS-Coursera portal includes Programs of Study related to scientific computing and artificial intelligence from top-tier institutions. The ARS Programs of Study are actively growing catalogues, and Coursera continues to add classes on a regular basis. ARS employees are encouraged to request additional courses/specializations to be added to our portal by sending an email to scinet-training@usda.gov. Note that you should expect to spend 2-3 hours per week on many of these classes, especially for the more advanced courses.SCINet Program of Study: The courses and specializations included in this program of study focus on general computational and data-science skills. Additional course collections cover topics such as nutrition and climate science.AI Center of Excellence Program of Study: The courses and specializations in this program of study are varied, ranging from high-level introductory to technical classical machine learning and deep-learning.Application ProcessClick the button below to access and submit the application form. Even though there is no cost to the learner, you must receive supervisor approval to apply for this opportunity prior to completing the form. Licenses are allocated on a rolling basis and will be active for three months.ARS Coursera License ApplicationAdditonal questions can be sent to scinet-training@usda.gov."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Downtime Archive",
      "category" : "News",
      "subcategory" : "System Downtime",
      "set" : "",
      "url" : "/news/downtime/archive",
      "description" : "",
      "date" : "",
      "content" : "SCINet Past Scheduled OutagesThe table below lists information about past SCINet outages. See SCINet Forum Announcements page (must have a SCINet account to access) for communications about emergency outages."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Downtime",
      "category" : "News",
      "subcategory" : "",
      "set" : "",
      "url" : "/news/downtime/",
      "description" : "",
      "date" : "",
      "content" : "SCINet Future Scheduled OutagesThe table below lists information about planned SCINet outages. See SCINet Forum Announcements page (must have a SCINet account to access) for communications about emergency outages.  Visit the Downtime Archive for information about past outages."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Events Archive",
      "category" : "Trainings and Events",
      "subcategory" : "Upcoming Events",
      "set" : "",
      "url" : "/training/events/archive",
      "description" : "",
      "date" : "",
      "content" : "Below is a list of past trainings and events. You might want to take a look at upcoming trainings and events, too. If you are interested in a course that is not being offered at this time, add your name to the waitlist. "
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "USDA ARS Postdoctoral Fellowship Program in Big Data Science and AI Research",
      "category" : "Opportunities",
      "subcategory" : "",
      "set" : "",
      "url" : "/opportunities/fellowships",
      "description" : "current listing of SCINet-funded postdocs, administered through ORISE and partner universities",
      "date" : "",
      "content" : "Other Opportunities:Mississippi State University’s Geosystems Research Institute Summer Graduate Research Experience: As part of the MSU/USDA Advancing Agricultural Research through High-Performance Computing project, MSU will be hosting a multi-disciplinary program designed for graduate students with interest in agriculture productivity, environmental ecology, geospatial analysis, AI/ML, epidemiology, and bioinformatics. Selected students will spend 9-weeks this summer working at Mississippi State University side-by-side with leading faculty conducting research in a high-performance computing environment. Review of applicants will begin March 15, 2022, and those accepted will be notified on or before April 15, 2022. Learn more and apply today!"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Fungal Bioinformatics Working Group",
      "category" : "Research",
      "subcategory" : "Working Groups",
      "set" : "",
      "url" : "/research/working-groups/fungalbioinformatics",
      "description" : "summary of the working group",
      "date" : "",
      "content" : "As technology advances in the field of molecular biology, so does the need for accurate and robust bioinformatics data analyses. The Fungal Bioinformatics Working Group was formed in December 2022 with the goal of developing centralized resources and a networking community for pathologists analyzing fungal and oomycete genomes. These resources are to include links to genome databases, optimized workflows and pipelines, and computing environments designed for fungal genome analyses. The group will meet once a month to discuss research, projects, and resource development.Additionally, the working group hosts a bi-weekly journal club where participants take turns choosing research papers to read and discuss as a group. We also hope to host a Fungal Bioinformatics Workshop virtually in Spring 2023.If you are interested in joining this group, please contact Kayla Pennerman (kayla.pennerman@usda.gov) or Olive Stanley (olive.stanley@usda.gov.               Working Group Leadership Team       Title                       Kayle Pennerman       Postdoctoral Fellow                 Olive Stanley       Postdoctoral Fellow         "
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet for genomics research",
      "category" : "Research",
      "subcategory" : "Use Cases",
      "set" : "",
      "url" : "/research/uses/genomics",
      "description" : "SCINet has a number of tools available for genomics research",
      "date" : "",
      "content" : "  By: Erin ScullyUse CaseARS scientists in several locations throughout the United States are combining forces to provide high quality genome assemblies, gene models, and functional annotations for the 100 most dangerous and damaging arthropod pests of US agriculture in an effort called the Ag100Pest Initiative. Insect genomes are surprisingly complex and variable, ranging from slightly over 100 Mb to over 2 Gb in size. Furthermore, their structural complexities vary as many arthropod genomes contain multicopy gene families arranged in large tandem arrays and large repetitive regions. Using new methods for generating long-read sequencing libraries from low DNA inputs typically obtained from single small-bodied insects, genome assemblies of several arthropods have been generated using PacBio Sequel II instrument and assembled using tools deployed on the SCINet Ceres HPC system, including the spotted lanternfly (Lycorma delicatula) quarantine pest (see Kingan et al. 2019).As part of this initiative, a functional annotation pipeline was developed to facilitate the addition of Gene Ontology (GO) terms, Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway annotations, and IntePproScan results to gene models generated for de novo genome and transcriptome projects. Unlike other GO tools, this pipeline allows the user to customize criteria for adding GO and KEGG terms to gene models, such as the degree of functional validation, minimum bit scores, raw scores, e-values, and percent identities. This tool is now deployed on the SCINet Ceres HPC system.The National Agricultural Library (NAL) maintains all Ag100Pest genome assemblies and annotations at the i5k workspace. Integration with WebApollo facilitates manual curation and sharing of gene sets of interest with collaborators.Tools and SoftwareARS is among the world’s leading producer and user of genomic, transcriptomic, proteomic, and other -omics data for non-model organisms of agricultural importance, including pathogens, insects, animals, and plants. On-going projects in the agency related to these efforts include de novo genome and transcriptome assembly and annotation, whole genome resequencing for population, strain, and biotype analyses, and comparative genomics and gene family evolution.SCINet hosts the following tools to facilitate these studies (see the Preinstalled Software List for a full list of software currently available on Ceres):GOanna Gene Ontology tool (see instructions for using on Ceres)Genome assemblersfor long-reads, short-reads, and hybrid approaches and tools for generating Hi-C maps:  ALLPATHS, Canu, DISCOVAR, FALCON, Juicer, MECAT, SALSA, SOAPdenovo, SPAdes, and SupernovaTranscriptome assemblersABySS, Trinity, and VelvetGene prediction and annotation toolsAugustus, BUSCO, GeneMark, Glimmer, InterProScan, MAKER, Prokka, RepeatMasker , RepeatModeler, TransDecoder, Trinotate, and tRNAScanRNA-Seq analysisBallgown, Bowtie, Cufflinks, HISAT2, kallisto, PASA, Salmon, and StringtieComparative GenomicsBLAST, Circos, DIAMOND, MCScanX, and OrthoFinderPhylogeneticsGARLI, PhyML, and RAxMLGUI tools for working with genomic dataFloating licenses for Geneious Prime GUI (graphic user interface) tools are available through SCINet.Additionally, GUI server licenses for CLC Workbench are also available.A Galaxy instance is also available on Ceres, allowing users to run popular command line tools through a GUI.    SCINet users can also install other freely-available software to suit their research needs or request system-wide installation by contacting the Virtual Research Support Core."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Geospatial Research Working Group",
      "category" : "Research",
      "subcategory" : "Working Groups",
      "set" : "",
      "url" : "/research/working-groups/geospatial",
      "description" : "summary of the working group",
      "date" : "",
      "content" : "This working group was formed as the result of a September 2019 SCINet-funded workshop hosted by Deb Peters, the acting ARS Chief Science Information Officer. The focus of the group is to provide continued input on the development of SCINet, to determine the computational needs of ARS geospatial researchers, and to develop collaborative research projects. The group will also learn from each other on topics related to high-performance computing. For example, how to use Ceres (the ARS high-performance computer), optimizing code for parallel processing, getting software onto Ceres, etc. Monthly working group teleconference calls and working sessions are beginning soon.If you would like to join the group, please contact Heather Savoy to be added to the group’s Teams.               Working Group Leadership Team       Title                       Heather Savoy       SCINet Computational Biologist (Data Scientist)                 Amy Hudson       Research Ecologist         Geospatial WorkbookThis working group contributes content to the  Geospatial Workbook, a site dedicated to providing practical geospatial tutorials for SCINet users. Tutorials from our previous workshops and other meetings are available there for general use."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet for geospatial research",
      "category" : "Research",
      "subcategory" : "Use Cases",
      "set" : "",
      "url" : "/research/uses/geospatial",
      "description" : "SCINet has a number of tools available for geospatial research",
      "date" : "",
      "content" : "  By: Pat Clark and Rowan GaffneyUse CasesMachine Learning   Classification   Clustering: Python Notebook Example   Regression Modeling   Process Based or Statistical Models Time Series Analysis   Estimating Productivity   Land Use / Land Change Geostatistics   Spatial Variance or Autocorrelation   Kriging / Interpolation Processing Data   UAS DN/Radiance to Reflectance When to Use SCINet?Setting up analyses to run on SCINet involves a non-trivial amount of overhead. Therefore, you should first evaluate if SCINet is an appropriate avenue for your research. Typically, analyses that are well-suited for SCINet are:   CPU intensive workloads   high memory workloads Additional considerations are:   Are my analyses already optimized?   Will I need to parallelize my analyses (typical for CPU intensive workloads)?   Will I require more than a single node of compute power (ie. distributed computing)?   Tools and SoftwareThe following tools/software are currently available on SCINet. (See the Preinstalled Software List for a full list of currently available software.)Geospatial Specific Software   ENVI (5.5): Image analysis software (1 license available)   ESMF: Earth System Modeling Framework (7.1): High-performance, flexible software infrastructure for building and coupling weather, climate, and related Earth science applications   Proj4 (4.9.3): Generic coordinate transformation software   GDAL/OGR: Geospatial Data Abstraction Library (1.11.4): Library for reading and writing raster and vector geospatial data formats Applicable General Software   H2O (3.2.0.3): Distributed in-memory machine learning platform with APIs in R and Python   Python (3.6.6): Interpreted, high-level, general-purpose programming language   R (3.5.2): Software environment for statistical computing and graphics   RStudio and RStudio Server: An integrated development environment (IDE) for R   JupyterLab: Web-based user interface for Project Jupyter Other   SCINet Remote Sensing Container Image: Python+R geospatial libraries and JupyterLab IDE (R, IDL, and Python kernels).            User Tutorial for JupyterLab+Dask Distrubuted using:                    container: /project/geospatial_tutorials/data_science_im_rs_latest.sif           sbatch script: /project/geospatial_tutorials/data_science_nb_dask.sbatch                       Optionally, pull the container from dockerhub to local folder with:         singularity pull docker://rowangaffney/data_science_im_rs                         "
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Guides List",
      "category" : "User Guides",
      "subcategory" : "",
      "set" : "",
      "url" : "/guides/",
      "description" : "A guide to the SCINet guides",
      "date" : "",
      "content" : "Use the navgation options or select one of the guides below"
  },
  
  
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet for hydrology research",
      "category" : "Research",
      "subcategory" : "Use Cases",
      "set" : "",
      "url" : "/research/uses/hydrology",
      "description" : "SCINet has a number of tools available for geospatial research",
      "date" : "",
      "content" : "By: Lindsey YasarerAs hydrologic modeling tools become more sophisticated and data about the natural environment becomes available at finer and finer resolutions, there is a need for more advanced computing capabilities to run watershed, erosion, and hydrodyanmic simulation models. Currently tools such as the Environmental Policy Integrated Climate (EPIC) model, the Kinematic Runoff and Erosion Model (KINEROS2), and the Soil and Water Assessment Tool (SWAT) are available on SCINet. Users also can upload other freely-available software to suite their research needs.Use CaseARS scientists in Boise, Idaho at the Northwest Watershed Research Center (NWRC) utilize SCINet and Amazon Web Service (AWS) cloud computing to run iSnobal, ARS’s physically based snow model, with frequent airborne lidar surveys. SCINet and AWS allow scientists to efficiently and effectively deliver model results in real time to water managers in California. Snowpack summary reports are uploaded to AWS where they can be displayed and downloaded on the NWRC website. The spatial snowpack results are publicly available through a GIS server where collaborators can download the data or visualize real time model results. This information is vital for water management for agriculture in the west, as well as decision making for flood mitigation, drought management, reservoir outflows, power generation, and domestic water supplies.For more information see: https://www.ars.usda.gov/pacific-west-area/boise-id/watershed-management-researchReports can be found at: https://www.ars.usda.gov/pacific-west-area/boise-id/watershed-management-research/docs/ars-snowpack-reports."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Learning Pathway",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/learningpath",
      "description" : "With the expansive list of free training available online, finding the right training to meet your learning needs can be daunting.",
      "date" : "",
      "content" : "With the expansive list of free training available online, finding the right training to meet your learning needs can be daunting. Take the first steps in getting started with your introductory learning path to help you get started with SCINet. Learn about SCINet, how to sign up for an account, and what is possible when supported by SCINet infrastructure. Then dive in with hands-on tutorials available across multiple searchable platforms to find the information you need for just-in-time learning. "
  },
  
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Microbiome Working Group",
      "category" : "Research",
      "subcategory" : "Working Groups",
      "set" : "",
      "url" : "/research/working-groups/microbiome",
      "description" : "summary of the working group",
      "date" : "",
      "content" : "information coming soon, contact Adam Rivers"
  },
  
  
  
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Quarterly Newsletter Archive",
      "category" : "News",
      "subcategory" : "",
      "set" : "",
      "url" : "/news/newsletter",
      "description" : "",
      "date" : "",
      "content" : "ARS Researchers: Want your research featured on the SCINet website and newsletter?Research highlights from researchers who use SCINet resources are added quarterly. Contact SCINet-Newsletter@usda.gov and use this short guide for authors (opens pdf in this browser window).USDA ARS SCINet Newsletter: January 2023USDA ARS SCINet Newsletter: October 2022USDA ARS SCINet Newsletter: August 2022USDA ARS SCINet Newsletter: April 2022USDA ARS SCINet Newsletter: January 2022USDA ARS SCINet Newsletter: October 2021USDA ARS SCINet Newsletter: July 2021USDA ARS SCINet Newsletter: April 2021USDA ARS SCINet Newsletter: January 2021USDA ARS SCINet Newsletter: October 2020USDA ARS SCINet Newsletter: July 2020USDA ARS SCINet Newsletter: April 2020USDA ARS SCINet Newsletter: January 2020"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Organization of the SCINet project",
      "category" : "About",
      "subcategory" : "",
      "set" : "",
      "url" : "/about/organization",
      "description" : "Organization and Management of the SCINet project",
      "date" : "",
      "content" : "About SCINetThe SCINet initiative is an effort by the USDA Agricultural Research Service to improve the USDA’s research capacity by providing scientists with access to high-performance computing (HPC) clusters, high-speed networking for data transfer, and training in scientific computing.SCINet supports a growing community of nearly 2,000 USDA research scientists and university partners to accelerate agricultural discovery through advanced computational infrastructure and scientific computing.Current uses of SCINet span multiple disciplines, including genomics, plant breeding, hydrology, crop production, plant and animal disease modeling, and natural resource management. SCINet users include ARS and other federal scientists, as well as prtners external to the federal government.SCINet supports a broad range of computational tools, including R (and RStudio), Python (and Jupyter notebooks), and software for bioinformatics, geospatial analyses, machine learning and deep learning, and image processing.  Training and Support    ​Computational skills​   Data management​   Scientific algorithms for HPC​   Computational workflow design   Software installation and support​   AI and machine learning    Computing Resources    2 High-Performance Computing Clusters            320 Standard Compute Nodes       41 High-Memory Compute Nodes       9 GPU Nodes (more on the way)           The ARS Scientific Computing Initiative is led by an Executive Committee comprised of ARS leadership including Dr. Steven Kappes – Associate Administrator for National Programs,  Dr. Brian Stucky – Acting Chief Scientific Information Officer, Rob Butler – Acting SCINet Project Manager, ARS scientists representing IT and security, and users of the HPC systems. SCINet committees report to the Executive Committee, including the Scientific Advisory Committee (SAC), an HPC Policy Committee, and an HPC Software Committee.The program runs two high performance computer clusters and a storage unit.  The Ceres cluster is housed at the National Animal Disease Center in Ames, IA and operated by Iowa State University. The Atlas cluster is located and ran by Mississippi State University in Starkville, MS, and the Juno storage unit is located at the National Agricultural Library in Beltsville, MD.User support at SCINet is provided by the  Virtual Support Research Core (VSRC) operated by Iowas State University for Ceres, and by Mississippi State University for Atlas.Senior Leadership            Person      Position                  Dr. Steven Kappes      Associate Administrator for National Programs              Dr. Brian Stucky      Acting Chief Scientific Information Officer, ARS      Executive CommitteeThe ARS Scientific Computing Initiative is led by an Executive Committee.            Person      Position                  Dr. Steven Kappes      Associate Administrator for National Programs              Dr. Brian Stucky      Acting Chief Scientific Information Officer, ARS              Ms. Lorna Drennen      Assistant Chief Information officer              Mr. Stan Kosecki      Deputy Assistant Chief Information Officer              Mr. Rob Butler      Acting SCINet Project Manager              Dr. Adam Rivers      Chair, SCINet Scientific Advisory Committee              Mr. Renaoti Chan      Policy and Information Assurance Branch Chief Information Security Officer              Mr. Ming Chan      Branch Chief, Scientific Data Engineering Branch, National Agricultural Library              Mr. Nathan Weeks      Information Technology Specialist              Mr. Jay Joiner      Project Consultant              Mr. Andy Liberman      Information Technology Specialist (Networking)              Dr. Brian Scheffler      Former Acting Chief Scientific Information Officer      SCINet Office            Person      Position                  Brian Stucky      SCINet Computational Biologist and acting CSIO              Heather Savoy      SCINet Computational Biologist              Haitao Huang      SCINet Computational Biologist              Ryan Lucas      SCINet Data Science Coordinator              Moe Richert      SCINet Web Developer      Scientific Advisory CommitteeThe ARS Scientific Computing Initiative is in the domain of ARS researchers, thus the program is in a constant state of gathering information to meet users’ needs. The SAC is an effort to assure that users have input on the ARS HPC-storage system. Scientific Advisory Committee (SAC) members represent a broad range of scientific research at ARS, and membership includes two researchers from each of the five ARS geographic areas, a statistician, and an “at-large” member. The SAC divides its work over various subcommittees: communications, education, planning, and membership. In recent years, the SAC has assisted with a quarterly SCINet newsletter, designed a user needs survey, created the scinet.usda.gov website, and held multiple computational workshops and trainings. SAC progress and issues are elevated to the Executive Committee for review and approval. For more information, including how to participate (non-members welcome), please contact Adam Rivers (Chair).                    Person        Area        Role        Sub-subcommittees        Term expires                            Adam Rivers        Southeast        Chair        Communications        2023                    Steve Schroeder        Northeast        Vice-Chair        Planning, Software        2024                    Erin Scully        Plains        Secretary        Education, Membership        2025                    Jeremy Edwards        Southeast        Representative        Communications        2023                    Feng Guo        Northeast        Representative        Planning        2024                    Hye-Seon Kim        Midwest        Representative                 2025                    Margaret Woodhouse        Midwest        Representative        Communications        2023                    Jason Fiedler        Plains        Representative        Planning        2023                    Pat Clark        Pacific West        Representative        Education        2025                    Alison Thompson        Pacific West        Representative        Education        2024                    Kathy Yeater        Statistician        Representative        Membership        2024                    Chris Owen        At-Large        Representative        TBD        2025                    Jack Okamoro        ONP        Ex Officio        -        -                    Cyndy Parr        NAL        Ex Officio        -        -                    Bryan Kaphammer        Area Directors        Ex Officio        -        -                    Brian Stucky        Acting CSIO        Ex Officio        -        -                    Brian Scheffler        Former CSIO        Ex Officio        -        -                    Rob Butler        Acting SCINetExecutive Director        Ex Officio        -        -            Scientific Points of ContactThe Scientific Point of Contact (SPOC) is the Scientific Computing Infrastructure Network (SCINet) champion for the location. Working with the SCINet Operations Team, the Scientific Advisory Committee (SAC) and the SAC subcommittees to ensure location personnel are familiar with SCINet and the services that are offered, the SPOC should be familiar with the scientific needs of the unit(s) at their location, especially with respect to complex data and computational needs. When the SCINet Operations Team needs information about the science requirements and needs at a location, they will contact the SPOC who is expected to interact with other scientists within the location to provide the information and recommendations to the requesting party.The list of SPOC can be found on the USDA Sharepoint Site.SCINet Policy CommitteeFormed in 2020, this committee makes recommendations to the Executive Committee on policies and procedures relevant to HPC operations. The committee is comprised of members from various technical backgrounds, and includes representation from different ARS geographic areas. Membership also includes representatives from Iowa State University and Mississippi State University (where two of the ARS HPC systems are housed) in order to get representation and perspective of the issues that need to be addressed. For more information, including how to participate, please contact Jonathan Shao (chair).            Person      Area                  Jonathan Shao (Chair)      Northeast Representative              Steven Schroeder      Northeast Representative              Dereck Bickhart      Midwest Representative              Corey Moffet      Plains Representative              Erin Scully      Plains Representative              Joshua Udall      Plains Representative              Loren Honaas      Pacific West Representative              Stan Kosecki      NAL Representative              Curtis Brooks      GWCC Representative              Jim Coyle      ISU Representative              Vincent Sanders      MSU-HPCC Representative              Angela Pompey      Information Security              Chris Lowe      Information Security              Brian Stucky      Acting Chief Scientific Information Officer, ARS              Rob Butler      Acting SCINet Project Manager      SCINet Software CommitteeThe software committee evaluates requests by users to add or delete software to/from one of the HPCs. This committee is composed of scientific experts in the use of HPCs from across agricultural disciplines, and that represent our two HPCs. Users can make software requests by completing the Software Request Form."
  },
  
  
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet for plant breeding research",
      "category" : "Research",
      "subcategory" : "Use Cases",
      "set" : "",
      "url" : "/research/uses/plant-breeding",
      "description" : "SCINet plant breeding research use case",
      "date" : "",
      "content" : "By: Justin VaughnThe USDA’s National Plant Germplasm System maintains the world’s foremost collection of crop diversity. In order the make that diversity useful to breeders and stakeholders, the USDA seeks to characterize the agronomic and, more recently, genetic properties of the plants in these collections. Genome sequencing is the ultimate genetic characterization of an individual. To that end, the USDA is engaged in sequencing key individuals to represent each crop. This information, when combined with trait data, can reveal the genetic basis of plant traits.When a reference genome of a species is available, sequencing an individual usually involves getting millions of short sequence reads from random parts of the genome and then aligning those reads to the reference. Once aligned, additional algorithms are used to identity differences relative to the reference, which requires extensive computation. Though it is possible to process a few whole genomes on local computers, USDA’s crop collections range from hundreds to thousands of individuals. With its multi-threaded architecture and large static and dynamic memory capacity, SCINet’s computational cluster, Ceres, makes work on this scale possible.Use CaseARS scientists are currently characterizing numerous crop collections at the whole genome level. Two examples, spearheaded by the Genomics and Bioinformatics Research Unit (GBRU), are focused on US rice and cotton. GBRU in collaboration with the Dale Bumper National Rice Research Center has sequenced 166 rice varieties and produced a gold-standard set of genetic variants in rice. This effort required months of processing time on Ceres as well as software developed in human genetics (HaplotypeCaller). These variants will be released to RiceBase (ricebase.org) and can be explored in an interactive app called HaploStrata (github.com/USDA-ARS-GBRU/HaploStrata). Cotton, because of its complex history and polyploid structure, is still in the process of being analyzed on Ceres. When finished, the results will be coupled with phenotypic data to give new insight into the regions of the genome that can be targeted to further enhance the US fiber industry.See also this Ceres data processing tutorial for plant breeding scientists: Quantitative Trait Locus (QTL) Analysis for Breeding"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Policy",
      "category" : "About",
      "subcategory" : "",
      "set" : "",
      "url" : "/about/policies",
      "description" : "SCINet policies and procedures",
      "date" : "",
      "content" : "AccountsAccount Request::  To obtain a SCINet account, a SCINet Account Request must be submitted.  The approval process depends on the affiliation of the requester.  ARS: SCINet Account Request  Other: Non-ARS SCINet Account RequestTerms and Conditions::  All SCINet Users must agree to the Terms and Conditions.Password Expiration::  SCINet passwords do not expire.Multi-factor authentication (MFA):  SCINet users are required to use second factor when authenticating to SCINet resources.Security Awareness Training::  Each Non-USDA SCINet account user will atttest to taking Annual Security Awareness Training.Account Deactivation::  SCINet accounts are subject to deactivation.  Collaborator (Non-ARS) accounts expire.Re-establishing account::  SCINet users whose account has been deactivated for any reason will need to submit a new account request.StorageHome Directories::  Home directory quota on all HPC clusters is automatically granted to all SCINet user accounts.  You have space by default as a user, but you can request Project Allocation.  Project Allocation Request: A SCINet Project Allocation Request form must be submitted by the project’s Principal Investigator (PI).Job QueuesJob queue policies are described in the User Guide for each of the clusters.:  Queue policies are subject to periodic change.  Ceres User Guide  Atlas User GuideSoftwareSCINet is a community resource.:  Thus, all SCINet users have a responsibility to the other users on the system. There are restrictions on what software you can install.Data ManagementUsers are responsible for securing their data.:  Certain public datasets are made available to all SCINet users.  A Galaxy web-based graphical workflow management system is available to all SCINet users. This is accessible via SCINet login credentials at https://galaxy.scinet.usda.gov.  Public-facing websites are currently not available from SCINet.Data Retention: Upon SCINet account deactivation (see Accounts section), the following data retention policies go into effect for files owned by the deactivated SCINet user:  Home directory: supervisor (non-ARS: ARS sponsor) will be given access by the VRSC and required to take action within 90 days to preserve any files  Project directories:          If there are no other active users on the project, the supervisor will be required to take action within 90 days      If there are active users on the project, and the deactivated user was the project PI (requestor), then the supervisor must select another PI      If there are active users on the project, and the deactivated user was not the project PI, then no further action need be taken      "
  },
  
  
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Pollinator Working Group",
      "category" : "Research",
      "subcategory" : "Working Groups",
      "set" : "",
      "url" : "/research/working-groups/pollinator",
      "description" : "summary of the working group",
      "date" : "",
      "content" : "The SCINet Pollinator working group was initiated in February 2021 with a goal of promoting coordinated, computational research on managed and native pollinators. The group focuses on 1) illustrating the utility of large datasets for understanding and mitigating pollinator stressors and declines 2) facilitating interdisciplinary communication on quantitative methods applicable to pollinator research and 3) connecting USDA-ARS researchers and collaborators to high-performance computing and human resources available through SCINet.If you would like to participate in the group, please contact Melanie Kammerer to be added to the group’s Microsoft Teams site.Working Group Leadership TeamMelanie Kammerer, SCINet Postdoc, University Park, PAWorkshopsAugust 26-27, 2021: Virtual workshop ‘Leveraging advances in data science to manage and conserve pollinators’Planning committee: Melanie Kammerer, Amy Hudson, William Meikle, Michael Branstetter, Vanessa Corby-Harris, Karl Roeder, Jay EvansApproximately 65 participants attended this this two-day, virtual workshop addressing applications of data science methods to pollinator research. The workshop was made up of four thematic sessions: 1) monitoring wild-bee communities, 2) using remote sensing to quantify floral resources, 3) genomics and molecular methods in pollinator research, and 4) digital infrastructure for long-term monitoring of honey bees. Each session featured research presentations followed by participant discussion on the implications of the presented information for better data collection, storage, and integration, new research, and potential applications for machine learning and artificial intelligence, including barriers to using these resources.Workshop AgendaWorkshop Summary and SynthesisRecordings of workshop presentations (coming soon!)"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Protein Function and Phenotype Prediction Working Group",
      "category" : "Research",
      "subcategory" : "Working Groups",
      "set" : "",
      "url" : "/research/working-groups/proteinfunction",
      "description" : "summary of the working group",
      "date" : "",
      "content" : "This working group was formed as the result of a December 2021 SCINet-sponsored symposium, AI at the Frontiers of Protein Science for Agriculture. The focus of the group is to facilitate community and collaboration for use of SCINet resources to advance research activity in protein function, structure, and phenotype prediction.  The group is comprised of scientists already using SCINet resources and scientists interested in utilizing SCINet for their research. This group will provide collaborate on workflows, present results, and create training and workbook resources for ARS scientists.If you would like to join the group, please contact Hye-Seon Kim (hyeseon.kim@usda.gov) or Carson Andorf (carson.andorf@usda.gov).            Working Group Leadership Team      Title                  Hye-Seon Kim      Computational Biologist              Cason Andorf      Computational Biologist      Presentation recordings and slides from previous meetings are available to USDA employees in the table below.            Meeting      Title      Author      Recording      Slides                  January 2023 Quarterly Meeting      Protein structure and folding prediction on SCINet.      Dr. Carson Andorf      recording      slides              September 2022 Quarterly Meeting      Integrating AlphaFold protein structures in a model organism database.      Dr. Carson Andorf      recording      slides              September 2022 Quarterly Meeting      Genome wide identification of Fusarium effector proteins and prediction of 3D protein structures with Alphafold      Dr. Hye-Seon Kim      recording      slides      "
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Request a SCINet-funded Self-led or SCINet-led Workshop or Training",
      "category" : "Opportunities",
      "subcategory" : "",
      "set" : "",
      "url" : "/opportunities/request-workshop",
      "description" : "Apply to SCINet to develop a training meeting",
      "date" : "",
      "content" : "Each year, the SCINet initiative accepts two types of requests:1) proposals for SCINet-funded workshops or training led by the applicant(s), and2) suggestions for workshops/training that you would like to see SCINet conduct.Proposals for Self-led SCINet-funded Workshops or TrainingThe application period for self-led SCINet-funded workshops closed this year on Feb 1, 2020.The FY21 application process will open later this year and a link to the application will be posted to this page in fall/winter 2020.Suggestions for SCINet-led Workshops or TrainingSuggestions for workshops or training that you see a need for and that SCINet should lead are accepted year-round at scinet_vrsc@USDA.GOV.For full consideration of your suggestion, please include the following information in your email:  Describe the proposed workshop or training. Include information such as the topic area(s), ideal event style(s) (hands-on, interactive, working session, conference), and any other pertinent information.  What should the learning goals or outcomes of the event be?  What pre-requisite skills/knowledge, if any, should be required to attend?  What does the ideal audience (attendees) look like? (i.e. administration level, RLs, scientists, statisticians, postdocs, data managers, technicians, students, collaborators)  Cite/link to a documented need for the proposed event. State whether the need is broad-based (spans multiple disciplines or research units), confined to a specific research unit/location, or limited to the needs of a single individual.  List any ARS employees who are knowledgeable of the topic area / suggest people who could be involved in leading the event."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Request Resources",
      "category" : "Support",
      "subcategory" : "",
      "set" : "",
      "url" : "/support/request",
      "description" : "request scinet resources",
      "date" : "",
      "content" : "Below are instructions for how to make a request for Software, Storage, or AWS resources.Software RequestBefore sending VRSC a software request do the following:      Make sure the software isn’t already available as a module by checking the lists of module software on the Preinstalled Software List page.    If you only need a different version of a software package that is already installed on CERES please email scinet_vrsc@USDA.GOV.        Consider whether you should install the software yourself in your home or project directory - see the Preinstalled Software List for more details.    You may want to use Conda for package, dependency, and environment management- see this guide about how to use conda on SCINet.    If you’re not sure whether you should install software yourself or if you need help, email scinet_vrsc@USDA.GOV. Software that will be useful to many SCINet users should be installed as a module by the VRSC.  How to send a software requestIf you’ve done the above but need new software installed as a module, use the software request form (eAuthentication required, non-ARS users should contact their sponsor):Software Request FormNote: doing this requires an agency-level security review and takes a few weeks.Storage RequestWhen a user applies for a SCINet account, they are allocated space on Ceres in a /home directory. Each user is allowed 10GB of data in the home directory.Users are advised to request additional space in the /project directory. Project directory storage is large, fast, not backed up, and can be requested up to 1TB or larger if justified. Project directory storage is good for fast I/O to large data files from compute nodes. The new project requests and request for changes must be submitted by the ARS project’s Principal Investigator (PI).To Modify an Existing SCINet Project AllocationThe PI should send an email that includes:  the name of the project directory (/project/&lt;projectname&gt;/) and  the requested changesto the SCINet Virtual Research Support Core (VRSC) scinet_vrsc@USDA.GOV.To Request a Quota Increase for an Existing SCINet Project AllocationSCINet users have access to a large short-term storage /90daydata which has no quotas. When requesting a project quota increase on Ceres or Atlas the PI will need to justify the request and explain why using /90daydata is not sufficient.To request project quota increase for storage on Ceres, Atlas and/or Juno fill out the following application form:Request Project Quota IncreasePlease Note: Only the project manager or the project PI can request a quota increase. Any requests sent by other individuals will be declined.To Request a New Project AllocationWhen requesting additional data storage you will be asked for:  a project summary,  project end date,  and to detail your long-term plan for data in your project directory after your project end date.To request a new project directory, fill out an application form (eAuthentication required, non-ARS users should contact their sponsor):Request Project StorageAWS RequestSCINet provides AWS at no cost to SCINet users with suitable workloads. A limited amount of funding is available annually. Interested SCINet users must submit a SCINet AWS Project Request to be considered for AWS funding.To Add/Modify AWS Resources for an Existing SCINet ProjectSend an email that includes:  the name of the project directory (/project/&lt;projectname&gt;/) and  the requested changesto the SCINet Virtual Research Support Core scinet_vrsc@USDA.GOV.This includes requests for additional funding.To Request a New SCINet-funded AWS ProjectFill out an application form (eAuthentication required, non-ARS users should contact their sponsor)Request AWS ProjectRequests will be reviewed and projects funded based on criteria such as suitability for AWS versus other SCINet computing resources (e.g., Ceres, Atlas), resource requirements estimated from the project description, and availability of SCINet funding."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "ARS SCINet and AI Center of Excellence Postdoctoral Fellows Program (FY23)",
      "category" : "Opportunities",
      "subcategory" : "",
      "set" : "",
      "url" : "/opportunities/scinet-aicoe-fellowships",
      "description" : "Internal USDA-ARS funding for SCINet and AI-COE postdoctoral fellowships.",
      "date" : "",
      "content" : "OverviewThe SCINet Program, in collaboration with the ARS Artificial Intelligence Center of Excellence (AI-COE), is calling for proposals for funding to support postdoctoral fellows to be mentored by ARS scientists. The goal of the fellowship program is to develop the next generation of ARS scientists with expertise in conducting and leading individual and collaborative research using computationally intensive approaches.Each fellow should: (1) be involved in individual and collaborative, multi-unit research, (2) have training and leadership opportunities, and (3) contribute to the overall success of SCNet or the AI-COE and the Fellows Program. In addition, each fellow will have the opportunity to take advantage of training courses that build computational literacy, such as in data science, AI, bioinformatics, and geospatial analyses on SCINet’s high-performance computing clusters (Ceres, Atlas).The call for proposals is now open. The deadline for proposal submission is Friday, December 23, 2022. Please visit https://forms.office.com/g/ULtMeH9Kp9 for detailed instructions. Please note that FY22 awardees (PIs) will not be eligible for an FY23 award."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Corner",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/corner",
      "description" : "SCINet Corner is a space for people to meet and discuss SCINet related items.",
      "date" : "",
      "content" : "The SCINet Corner is a recurrent virtual gathering to provide a space for people to meet and discuss SCINet related items. The main idea is that SCINet users interested in similar things can get help from each other. This virtual meeting will provide space for, and facilitate these interactions on the third Thursday of the month. Meeting times may change.Register Here"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet-X",
      "category" : "News",
      "subcategory" : "",
      "set" : "",
      "url" : "/news/scinet-x",
      "description" : "Overview of the SCINet-X initiative",
      "date" : "",
      "content" : "Expanding SCINet for the Future of ScienceOverview of the SCINet-X ProjectThe ARS SCINet Team developed a SCINet network solution that provides two levels of high-bandwidth services to meet the different needs of research locations.  These solutions will accelerate the expansion of high-bandwidth SCINet connectivity to all ARS locations as fast as possible while balancing economic, technological, and local constraints.  The SCINet-X project involves the SCINet Team working with each individual location in ARS to directly connect the location to SCINet.The following locations are actively working on establishing high-bandwidth SCINet connectivity.            Kimberly, ID      Parlier, WA      Pendleton, OR      Salinas, CA              Wenatchee, WA      Akron, CO      Bushland, TX      College Station, TX              Fort Keogh, MT      Las Cruces, NM      Lincoln, NE      Lubbock, TX              Mandan, ND      Manhattan, KS      Peoria, IL      Urbana, IL              West Lafayette, IN      Burlington, VT      Leetown, WV      Orono, ME              Auburn, AL      Baton Rouge, LA      Booneville, AR      Byron, GA              Canal Point, FL      Charleston, SC      Dawson, GA      Fayetteville, AR              Gainesville, FL      Houma, LA      Jonesboro, AR      Mayaguez, PR              Miami, FL      Oxford, MS      Poplarville, AR      Raleigh, NC      The following locations have completed implementation of their high-bandwidth SCINet connectivity.Pacific West Area            Hilo, HI      Logan, UT      Maricopa, AZ      Burns, OR      Plains Area            Fargo, ND      Grand Forks, ND      Kerrville, TX      Midwest Area            Columbia, MO      Madison, WI      St. Paul, MN      Northeast Area            Boston, MA      Wyndmoor, PA      Kearneysville, WV      Southeast Area            Athens, GA      Florence, SC      New Orleans, LA      Tifton, GA      Starkville, MS      We are eager to work with the remaining ARS locations to ensure they are ready when network equipment is available for installation. Please see the steps (milestones) below to be followed in getting a location on our list. The SCINet Team is available to answer questions at any step along the way.Key Milestones for Achieving a high-bandwidth SCINet Connection  Location team identifies local network provider, which often involves an agreement with a university or working with a local internet provider  Location team completes online location survey. This form is only accessible to those who have a USDA email account.  Location team works with SCINet team to finalize installation details  Location team finalizes network connection and agreement with university or local internet provider  Location team initiates network service and SCINet installs network hardware  SCINet team establishes location’s SCINet-X connection and begins operational support and monitoringFor more information about the project or questions for the SCINet Team, please send an email to: SCINet@usda.gov."
  },
  
  
  
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Sign up for a SCINet account",
      "category" : "About",
      "subcategory" : "",
      "set" : "",
      "url" : "/about/signup",
      "description" : "SCINet account sign up for ARS scientists and ARS collaborators",
      "date" : "",
      "content" : "ARS EmployeesARS employees can request a SCINet account by filling out an online form (eAuthentication required). Anyone with a usda.gov email address working for ARS should use this form. It may take up to ten business days to create an account after the request is approved by the supervisor.Note: if you have a usda.gov email address but are not an ARS employee (e.g., if you are an ORISE postdoctoral fellow or a summer intern), then your sponsor at ARS will need to complete the non-ARS Employee form below.Request ARS employee accountNon-ARS EmployeesNon-ARS Employees require an ARS employee sponsor to obtain a SCINet account.  Please contact your sponsor and request that they complete the online form on your behalf (eAuthentication required). It may take up to ten business days to create an account after the request is approved by the sponsor’s supervisor.Request non-ARS accountThe ARS sponsor will need your:  Full name  Email  Institution  Position  Phone number  Office address  Affiliation(s) with ARS. They will need to enter agreement type(s) (e.g., RSA, NARA), agreement number(s), and agreement title(s) or grant, if applicable.  If other, please indicate any formal relationship between the sponsor and sponsored party e.g. advisory committee."
  },
  
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "A Quick Guide to getting started with SCINet",
      "category" : "User Guides",
      "subcategory" : "",
      "set" : "",
      "url" : "/guides/start",
      "description" : "A Quick Guide to getting started with SCINet",
      "date" : "",
      "content" : "No account? Signup here.What is SCINet?SCINet is the USDA-ARS’s initiative for scientific computing. It consists of:  High performance computer clusters for running command-line and graphical programs. There are currently two clusters: Ceres cluster in Ames IA and Atlas cluster in Starkville MS. SCINet also offers AWS cloud computing. See SCINet HPC Systems for more detail.  Network improvements across ARS.  Support for computing through the Virtual Research Support Core (VRSC). See VRSC Support for more detail.  Training and workshop opportunities in multiple areas of scientific computing. See our event calendar for more information.Users who are new to the HPC environment may benefit from the SCINet/Ceres onboarding video which covers most of the material contained in this guide. Note that /KEEP storage discussed in the video at 16:20 is no longer available. Instead data that cannot be easily reproduced should be manually backed up to Juno. The instructional video at https://www.youtube.com/watch?v=I3lnsCAfx3Q demonstrates how to transfer files between local computer, Ceres, Atlas and Juno using Globus.User GuidesUse the navgation options or select one of the guides below to get started with SCINet"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "User Stories",
      "category" : "News",
      "subcategory" : "Stories",
      "set" : "",
      "url" : "/news/stories",
      "description" : "",
      "date" : "",
      "content" : "ARS Researchers: Want your research featured on the SCINet website and newsletter?Research highlights from researchers who use SCINet resources are added quarterly. Contact SCINet-Newsletter@USDA.GOV and use this short guide for authors (opens pdf in this browser window)."
  },
  
  
  
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Overview of SCINet Training Resources",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/resources",
      "description" : "One of SCINet's missions is training scientists in computational methods to empower agricultural research.",
      "date" : "",
      "content" : "One of SCINet’s main objectives is training scientists in computational methods to empower agricultural research. We are doing this in several ways. We offer online and in person trainings, support topic-specific working groups, and provide help, training, and tutorials for scientists."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Free Online Computational Training Resources",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/free-online-training",
      "description" : "course links for common training needs",
      "date" : "",
      "content" : "A list of free trainings available to SCINet users sorted by topic.  This is not an exhaustive list, but may help you get started.We also have instructions on how to access the various learning platforms."
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "Upcoming Events",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/events/",
      "description" : "Advertise upcoming events",
      "date" : "",
      "content" : "SCINet, the AI-COE, and other providers regularly host a variety of events and trainings. Information on how to attend these events will be posted on this page closer to the event date.  You might want to take a look at events we’ve hosted in the past, too.If you are interested in a course that is not beign offered at this time, add your name to the waitlist.Keep an eye on this page for more info about upcoming events!"
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Use Cases",
      "category" : "Research",
      "subcategory" : "",
      "set" : "",
      "url" : "/research/uses/",
      "description" : "summary of current use cases",
      "date" : "",
      "content" : ""
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "The SCINet Virtual Research Support Core",
      "category" : "About",
      "subcategory" : "",
      "set" : "",
      "url" : "/about/vrsc",
      "description" : "Responsive support of SCINet is provided by a VSRC",
      "date" : "",
      "content" : "The Virtual Research Support Core (VRSC) is comprised of ARS personnel and contract personnel at Iowa State University and Mississippi State University. The VRSC manages the day-to-day operations and maintenance of the Ceres and Atlas HPC systems and is also involved with the networking aspects of SCINet. The VRSC is staffed by talented IT administrators and engineers with expertise in high-performance computing system and network engineering, operations, maintenance, and administration.Additionally, the VRSC provides research IT support, computational expertise in various research domains, and support for SCINet-funded workshops/training for SCINet users.Research IT Support      Linux/Unix        Advanced scripting/parallelization        HPC operational support        Software development, software installation, assistance using software, assistance building containers        Data transfer        Data management        Cloud computing  Computational Research Domain Expertise      Bioinformatics and Sequence Data Analysis        Image Processing        Modeling  Support for SCINet-Funded Workshops/TrainingThe SCINet initiative funds various workshops and training events annually (Apply to Hold a SCINet-Funded Workshop/Training) to which the VRSC can provide the following types of support:      general SCINet information presentation with Q&amp;A        hands-on SCINet training assistance        data management assistance / help designing computational workflows  How and When to Contact the VRSCSee the Contact Us page for more information"
  },
  
  
  
  
  
    
    
  
  
  
  
    
    
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Workbooks",
      "category" : "Trainings and Events",
      "subcategory" : "Learning Resources",
      "set" : "",
      "url" : "/training/workbooks",
      "description" : "workbook links and descriptions",
      "date" : "",
      "content" : ""
  },
  
  
  
  
  
    
    
    
    
    
    
  {
      "title" : "SCINet Working Groups Overview",
      "category" : "Research",
      "subcategory" : "",
      "set" : "",
      "url" : "/research/working-groups/",
      "description" : "summary of current working groups",
      "date" : "",
      "content" : "The SCINet initiative contributes funding to various working groups that use SCINet computational resources for research. The funding is generally used to convene in-person group workshops. There are also two working groups that develop SCINet learning/training materials.The current working groups are:  Ag100Pest Initiative (subgroup of AGR)  Arthropod Genomics Research (AGR) Working Group  Fungal Bioinformatics Working Group  Geospatial Research Working Group  Microbiome Working Group  Pollinator Working Group  SCINet-Longterm Agroecosystem Research (LTAR) Phenology Working Group  Protein Function and Phenotype Prediction Working Group"
  },
  
  
  
  
  
  
  
  
  
  
    
      
      
      
      
      
    
{
  "title" : "Directory Available",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2020-08-24-directory",
  "description" : "",
  "date" : "Aug 24, 2020",
  "content" : "We have a directory available for medium term usage which allows researchers to run multiple jobs against data in this directory over a 90 day period.This should be faster than using /project , since not everyone will be using it , and it will be kept performant.To keep this space available for all researchers, and performing well, it needs to be kept at a low level of space.  To accomplish this, files will only be allowed on this space for a period of 90 days.  Any file older than 90 days since last access is subject to DELETION.  This is permanent and the files cannot be recovered.  Make sure that you move files out of this directory well before the time that they will be deleted, as this is automated.  If you forget and go on vacation, or get sick, the files will still be deleted.Just like /project there is no backup for this space. If you need to keep something, use /KEEP/PROJECTNAMEAdditional warning:  If you download archived files, they may contain files with an access date from long ago. This date will still trigger deletion, so make sure that the files have a new access date. For example, when you untar a .tar or .tgz file, use the -m flag. If you use rsync to the space, do not use the -a flag, as that preserves date stamps.The name of this space is /90daydata, and it will go live Thursday, 8/27/20 at noon EDT.The VRSC will create a directory /90daydata/PROJECTNAME for each /project/PROJECTNAME  directory, so that there is no confusion among different groups.There will also be a space /90daydata/shared open to all for the passing of data between project groups. This will be readable by everyone on the system, so only put data in there that is appropriate."
},

    
      
      
      
      
      
    
{
  "title" : "Atlas Available",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2020-09-30-atlas",
  "description" : "",
  "date" : "Sep 30, 2020",
  "content" : "The Atlas Cluster at Mississippi State is now availible to all SCINet Users!  Documentation on logging in has been added to the Quick Start Guide.  Full documention on Altas is coming soon."
},

    
      
      
      
      
      
    
{
  "title" : "galaxy.scinet.science is set to retire in Feb 2021",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2021-02-05-galaxy-retire",
  "description" : "",
  "date" : "Feb 05, 2021",
  "content" : "galaxy.scinet.science is set to retire Feb 15, 2021Due to major refactoring, updates and policy changes we could not continue using the old database and are starting with a clean slate on galaxy.scinet.usda.govgalaxy.scinet.science is currently in “read-only” mode where users have access but cannot launch new jobs. Users have until 7AM CST Feb 15, 2021 to access galaxy.scinet.science and download the existing files.If you have any issues or concerns, email scinet_vrsc@usda.gov"
},

    
      
      
      
      
      
    
{
  "title" : "SAC Northeast Area Representitive",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2021-04-15-sac-representative",
  "description" : "",
  "date" : "Apr 15, 2021",
  "content" : "The SCINet Scientific Advisory Committee (SAC) is seeking a representative from the Northeast Area for a 3-year term.The SAC meets monthly to provide input on planning, educational and communication activities of the SCINet Initiative. There is also opportunity for input on planning and policy initiatives. For example, SAC members created a quarterly SCINet newsletter, facilitated a user-needs virtual meeting with scientific points-of-contact across ARS research units, launched the new SCINet website, and hold multiple computation workshops and trainings annually.This is a 3-year term which begins immediately and will end March 2024. Please reply to Kathleen Yeater (Kathleen.yeater@usda.gov) with nomination(s) by next Friday April 23, 2021. Self-nominations are accepted. Please provide e-mail contact information of nominees, and the SAC Membership committee will continue with the application process for the NEA representative to SAC."
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Discussion Forum",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2021-04-22-scinet-forum",
  "description" : "",
  "date" : "Apr 22, 2021",
  "content" : "SCINet has launched a new discussion forum that replaces Basecamp:SCINet ForumSCINet projects on Basecamp will no longer be available after April 30, 2021.From now on all announcements will be posted on the new SCINet Forum and published in this section of the SCINet website .SCINet users will NOT receive email notifications from SCINet Forum UNTIL they login to the Forum for the first time. To login to SCINet Forum, click here.  Then select “Log In” and use your SCINet credentials (what you use to log in to Ceres or Atlas). Make sure to append the GA 6-digit code at the end of the SCINet password.If you wish to save any data from Basecamp to your computer, please do it before April 30, 2021."
},

    
      
      
      
      
      
    
{
  "title" : "Ceres Down",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2021-07-06-scinet-down",
  "description" : "",
  "date" : "Jul 06, 2021",
  "content" : "07-July-2021Update: Site Service at Ames is available. Internet2 circuit vendor Lumen has determined a fiber cut from a local construction company to be the cause of the outage. Fiber splicing has been completed, and fiber crews are hands off. No further impact is expected.Start Time(s) []: 2021-07-06 15:47:54 - 2021-07-07 12:33:45 [0d 20h 45m 51s]06-July-2021Connection to Ceres is currently down. The issue is being investigated.Update: Site service at Ames is currently unavailable. ARS SCINet circuit vendor Internet2 has advised that the provider is reviewing a circuit issue that may be contributing to this outage low light on the path. No ETR is currently available. Updates will be provided as they are received.Update: Site Service at Ames is unavailable. Internet2 circuit vendor Lumen is investigating a low signal power issue in their network that may be impacting services. Testing is underway to determine the source of the loss. Vendor ticket has been escalated to expedite the incident resolution. No ETR is currently available. Updates will be provided as they are received."
},

    
      
      
      
      
      
    
{
  "title" : "No SCINet priority node purchases this year",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2021-07-27-no-node-purchase",
  "description" : "",
  "date" : "Jul 27, 2021",
  "content" : "No SCINet priority node purchases this yearWe will not be making priority node purchases through the SCINet Program this year. We added a large number of nodes to Ceres in the past several years that either replaced older, less reliable nodes or were new nodes that increased our overall capacity. In addition, we brought the Atlas Cray computer online at Mississippi State University, and we will be adding a backup storage system at the National Ag Library in the next month.These actions should provide sufficient computational power and storage for scientists to conduct their high-performance computing research without a unit needing to purchase its own nodes.We will re-assess the situation as scientific needs continue to change."
},

    
      
      
      
      
      
    
{
  "title" : "Planned connectivity outages",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2021-09-15-connectivity-outage",
  "description" : "",
  "date" : "Sep 15, 2021",
  "content" : "Planned connectivity outages at various SCINet locationsGNOC plans to upgrade the OS on the SCINet gear at the 6 locations. This will result in connectivity interruptions during the upgrade. The upgrade schedule is the following:Albany - 9/16 8AM PTClay Center - 9/16 4PM CTAmes - 9/17 8AM CTStoneville - 9/20 8AM CTNAL - 9/20 3PM CTCSU - 9/21 9AM CT"
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Network Hardware OS Updates",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2021-09-21-scinet-hardware",
  "description" : "",
  "date" : "Sep 21, 2021",
  "content" : "More SCINet Network Hardware OS Updates9/22/2021Stoneville: Upgrade OS on hardware starting at 1300 UTC (O800 CT)Ames: Complete upgrade on router at 1530 (1030 CT)Clay Center: Upgrade OS on hardware starting at 2000 UTC (1500 CT)9/23/2021NAL: Upgrade OS on hardware starting at 1900 UTC (1500 ET)9/24/2021 Dependent on Wednesday and Thursday’s progressFort Collins: Upgrade OS on hardware. TBDAlbany: Finish up the upgrade on the router only. TBD"
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Policy Changes",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2021-09-29-scinet-policy",
  "description" : "",
  "date" : "Sep 29, 2021",
  "content" : "The SCINet Policies document in your home directory on Ceres provides an updated SCINet policy that addresses user accounts, storage space, job and quality of service queues, software, data management, networking, and Galaxy access. Please take the time to read over the document to make sure that you are compliant with all SCINet policies and regulations.  A few examples of the changes in SCINet policy include:1) Accounts will be deactivated if there is one year of account inactivity.2) Passwords reset after 60 days.3) Collaborator (Non-ARS) accounts expire after one year.4) Non-ARS SCINet users must confirm that they have taken Annual Security Awareness Training and at the beginning of each year. The VRSC will send out emails to confirm account status eligibility.5) To clarify the policy on hardware purchases - when a node is purchased, it belongs to the priority pool and the purchaser has access to the priority queue for the period of the node hardware warranty. The purchaser does not own the physical node, and the property sticker belongs to the SCINet.6) SCINet is a community resource. Thus, all SCINet users have a responsibility to the other users on the system. There are restrictions on what software you can install.7) There is a /90daydata folder where you can perform analysis to save resources in your project directory. The data is then automatically deleted after 90 days with no back-up.These policy changes were approved by the SCINet Policy Committee for the safety of SCINet and to encourage a positive SCINet experience for all users. The Committee is made up of Scientists, Bioinformaticians and IT professionals across ARS. If you have a question or need clarification, please contact the chair of the SCINet policy committee. Details of the SCINet policy can be found on the following SharePoint page."
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Request Forms Down",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2022-11-16-forms",
  "description" : "",
  "date" : "Nov 16, 2022",
  "content" : "This issue was resolved on 11/16/2022Due to a Sharepoint issue, all SCINet resource request forms (for new accounts, new project allocations, and software installations) are currently unavailable. USDA IT technicians are working to restore service as soon as possible. We sincerely apologize for this inconvenience.Once the problem has been fixed, the SCINet website forms will be reactivated and we will send an update to all.  Please be aware that we are unable to see and process requests submitted after Nov 4th at this time."
},

    
      
      
      
      
      
    
{
  "title" : "Ceres Experiencing Delays",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2023-01-20-Ceres",
  "description" : "",
  "date" : "Jan 20, 2023",
  "content" : "Compute jobs on Ceres might be delayed; please see the SCINet Forum for updates."
},

    
      
      
      
      
      
    
{
  "title" : "Ceres Maintenance - February 20, 2023",
  "category" : "News",
  "subcategory" : "Announcements",
  "set" : "",
  "url" : "/announcements/2023-01-24-Ceres",
  "description" : "",
  "date" : "Jan 24, 2023",
  "content" : "Ceres cluster maintenance is scheduled for the week of February 20, 2023. This will be a major upgrade, as Ceres is being upgraded from EL 7 to EL 9.  We will rebuild the software modules to run on the new OS. If you have installed software on your own, you may need to rebuild it under the new OS as well.  During the maintenance we will also upgrade File system which should resolve the issue with fasterq-dump locking up nodes.  Some network changes will be performed during the maintenance.  Users with no projects will be moved from scinet account to sandbox account. It’s expected that SCINet users have access to a project to perform meaningful computations on Ceres and/or Atlas. To request a project, go to our request resources page.A lot of preparation work will be done prior to the maintenance, but we still may encounter problems during the update process.  We will send a follow up message when Ceres is available again.Note that queued jobs will not start if they cannot complete by 6AM February 20. In the output of the squeue command the reason for those jobs will state (ReqNodeNotAvail, Reserved for maintenance). The jobs will start after the scheduled outage completes.If you anticipate the need to access HPC resources during the week of February 20, 2023, we advise moving to Atlas cluster which will be available during the Ceres maintenance. Make sure to copy data from Ceres to Atlas prior to the maintenance if needed.Please submit any questions you may have via email to scinet_vrsc@usda.gov."
},


  
    
      
      
      
      
      
    
{
  "title" : "Arthropod Genomics Research Workshop 1",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2014-09-17-Arthropod-Genomic-Research-Workshop",
  "description" : "",
  "date" : "Sep 17, 2014",
  "content" : "Workshop Goals/Deliverables:   Build tighter linkages and communication among ARS scientists involved in arthropod genomics research (AGR).   Forge new collaborations within ARS by explicitly highlighting current and planned AGR within ARS.   Define critical research gaps or strengths of AGR within ARS.   Provide a clearer picture of current strengths and weaknesses within ARS relative to cyberinfrastructure, data storage, and analytical resources needed to accomplish ARS-wide goals with respect to AGR.   Develop a better understanding of available bioinformatics resources (infrastructure and personnel) within ARS.   Train participating ARS scientists in publicly available analysis tools (iPlant) that will facilitate research on high-throughput genomic data.   Provide guidance and input for future bioinformatics training of ARS scientists.   Define role for NAL in AGR in coordinating data standardization and database integration.   Build roadmap to accomplish ARS-wide goals with respect to AGR.   Plan movement towards integrative, multidisciplinary, and translational genomics. 09/17/2014 - 09/19/2014   ·   Beltsville, MD   ·   lead: Brad CoatesWorkshop Agenda (opens pdf in this browser window)Participant List (opens pdf in this browser window)Breakout Session Questions (opens pdf in this browser window)AGR Impact on Agriculture Statement (opens pdf in this browser window)"
},

    
      
      
      
      
      
    
{
  "title" : "Arthropod Genomics Research Workshop 2",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2016-07-26-Arthropod-Genomics-Workshop",
  "description" : "",
  "date" : "Jul 26, 2016",
  "content" : "Thirty three members of the Arthropod Genomics Research (AGR) community representing all five Areas of the Agricultural Research Service (ARS) convened a workshop charged with building collaborative research communities and improving communication channels in the Agency. Participants assessed how the current knowledge, skills and abilities of the AGR scientists impact the effectiveness of genomic data analyses, and affect the capacity of the Agency to address arthropod-specific challenge areas; 1) Controlling Vectored Diseases, 2) Managing Herbivorous Insects, and 3) Implementing Biocontrol, as well as meet the Grand Challenge of reducing agricultural inputs and environmental impact by 25% while increasing output by 25% by 2025. Participants were charged with forming long-term scientific teams that will support the exchange of scientific knowledge and expertise. The workshop was structured to evaluate gaps in knowledge and communication that currently exist, and to subsequently propose solutions that would increase the capacity of ARS to pursue genomics-based scientific inquiry. This task is aligned with the ARS Big Data Vision aimed to develop a well-trained scientific and technical staff and enhance knowledge, skills and abilities within the ARS workforce.07/26/2016 - 07/28/2016   ·   Beltsville, MD   ·   lead: Brad CoatesWorkshop Agenda (opens pdf in this browser window)Participant List (opens pdf in this browser window)Breakout Session I Exercise: Gap Analysis (opens pdf in this browser window)Breakout Session II Exercise: Devising solutions within and among Grand Challenge Areas (opens pdf in this browser window)Workshop Summary Document (opens pdf in this browser window) Related PublicationsGundersen-Rindal D, Adrianos S, Allen M, Becnel JJ, Chen YP, Choi MY, Estep A, Evans JD, Garczynski S, Geib SM, Ghosh SKB, Handler AM, Hasegawa DK, Heerman M, Jull J, Hunter W, Kaur N, Li J, Li W, Ling KS, Nayduch D, Oppert B, Perera OP, Perkin L, Sanscrainte N, Sim S, Sparks M, Temeyer K, Vander Meer R, Wintermantel WM, James R, Hackett K, Coates BS. 2017. Arthropod genomics research in the United States Department of Agriculture-Agricultural Research Service: Applications of RNA interference and gene editing in pest control. Trends in Entomology 13: 109-137. (opens pdf in this browser window)Poelchau M, Coates BS, Childers CP, Perez de Leon AA, Evans JD, Hackett K, Shoemaker D. 2016. Agricultural applications of insect ecological genomics. Current Opinions in Insect Science 13:61-69.Coates BS, Poelchau M, Childers C, Evans JD, Handler A, Guerrero F, Skoda S, Hopper K, Wintermantel WM, Ling KS, Hunter WB, Oppert B, Perez de Leon AA, Hackett K, Shoemaker D. 2015. Arthropod genomics research in the United States Department of Agriculture-Agricultural Research Service:  Current impacts and future prospects.  Trends in Entomology 11:1-27. (opens pdf in this browser window)"
},

    
      
      
      
      
      
    
{
  "title" : "Artificial Intelligence and Machine Learning Training",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2019-08-27-Gainesville-FL",
  "description" : "",
  "date" : "Aug 27, 2019",
  "content" : "A group of 30 ARS researchers and 20 researchers from the University of Florida (UF) met in Gainesville, FL in August 2019 to gain experience with various machine learning research techniques through interactive training modules developed by ARS and UF training instructors and modeled after the two day Google Machine Learning Crash course. The live coding lessons and in-class exercises used Jupyter Notebooks, Python, and the Scikit-Learn machine learning library. The training enabled attendees to 1) identify the types of problems that machine learning methods can be applied to, 2) have an understanding of basic concepts like the bias-variance tradeoff, and test/train/validate, 3) have experience applying a few types of classification and regression methods, 4) have a general sense of the methods and software available, and 5) know where to go to continue learning about machine learning. Training materials, including all the lessons can be found on the USDA-ARS / UF Machine Learning Training 2019 website. Contact Adam Rivers for more information.08/27/2019 - 08/28/2019   ·   Gainesville, FL   ·   lead: Adam Rivers"
},

    
      
      
      
      
      
    
{
  "title" : "Training to develop computational links from ground to satellite: LTAR phenology network case study",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2019-08-27-Las-Cruces-NM",
  "description" : "",
  "date" : "Aug 27, 2019",
  "content" : "A group of 12 researchers including ecologists and data scientists representing ARS and collaborating University partners met at the ARS Jornada Rangeland Research Unit in Las Cruces, NM in August 2019 to learn how the use of SCINet and CERES could expand the scope and impact of an on-going collaborative research project. Participants devised workflows to assemble multiple diverse data streams representing growing season dynamics and ecosystem productivity for agro-ecosystems nationwide at multiple spatial scales. SCINet and CERES will facilitate the development and optimization of models to characterize seasonal carbon dynamics in agroecosystems in the LTAR network. The first report of this work will be presented at the 2019 Fall meeting of the American Geophysical Union in December. Contact Dawn Browning for more information on this research and collaborative opportunities.08/27/2019 - 08/28/2019   ·   Las Cruces, NM   ·   lead: Dawn Browning"
},

    
      
      
      
      
      
    
{
  "title" : "2019 Geospatial Workshop",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2019-09-10-Geospatial-Workshop",
  "description" : "",
  "date" : "Sep 10, 2019",
  "content" : "A group of over 36 geospatial scientists, post-docs, research leaders, and data managers gathered at the ARS Jornada Rangeland Research Unit in Las Cruces, NM on September 10-11, 2019 to discuss high-performance computing (HPC) issues and artificial intelligence research methods applied to geospatial problems. The group identified computational issues with accessing and using SCINet and the ARS HPC system for geospatial research and also gained exposure to relevant machine learning and deep learning research methods. The workshop resulted in the creation of a SCINet Geospatial Research Working Group to continue identifying and addressing complex computational problems as well as to collaborate on new geospatial research projects.Workshop Agenda (opens pdf in this browser window)Participant List with Poster/Presentation Titles (opens pdf in this browser window)Workshop Notes (downloads Microsoft Word document)Visit the SCINet-funded Workshops Basecamp for posters, presentation slides, and recordings of the presentations. ARS employees can contact Kerrie Geil for access to the Basecamp project."
},

    
      
      
      
      
      
    
{
  "title" : "AI and Machine Learning SCINet Conference: Current Uses and Potential to Solve Complex Problems in Agriculture",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2019-09-19-Beltsville-MD",
  "description" : "",
  "date" : "Sep 19, 2019",
  "content" : "In September 2019, approximately 40 ARS research leaders, administrators, and scientists met at the George Washington Carver Center in Beltsville, MD for a SCINet-funded conference focused on the use of artificial intelligence (AI) in agriculture. The purpose of the event was to learn how AI is currently being used at ARS and to envision the harnessing of AI to transform agricultural research. Presentations included examples of how ARS researchers are using machine learning, deep learning, and AI recommendation systems to improve predictive analytics, decision making, and advance agricultural research. Attendees were also updated on SCINet resources available to ARS scientists. As a result of the conference, ARS researchers are hoping to publish a special issue in IT Professional magazine that highlights current and future use of AI methods in ARS research and the benefits to our stakeholders.09/19/2019 - 09/20/2019   ·   Beltsville, MD   ·   lead: Deb PetersConference Agenda (opens pdf in this browser window)Participant List (opens pdf in this browser window)Poster List (opens pdf in this browser window)Conference Notes (downloads Microsoft Word document)Visit the SCINet-funded Workshops Basecamp for posters, presentation slides, and recordings of the presentations. ARS employees can contact Kerrie Geil for access to the Basecamp project."
},

    
      
      
      
      
      
    
{
  "title" : "Introduction to SCINet",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2020-04-28-Manhattan-KS",
  "description" : "",
  "date" : "Apr 28, 2020",
  "content" : "04/28/2019  ·   remotely for Manhattan, KS and Lincoln, NE   ·   lead: Andrew Severin, Erin Scully, Kathy Yeater, Kerrie GeilAndrew Severin of the SCINet VRSC held a 3-hour training session covering what SCINet is, a walk through of scinet.usda.gov and bioinformaticsworkbook.org, and an intro to the SCINet HPC Ceres cluster, followed by a Q&amp;A session with participants. The training included 48 participants from Manhattan, KS and Lincoln, NE. This remote training is available for other ARS locations upon request. Please contact Andrew Severin at scinet_vrsc@usda.gov or one of the other organizers of this training for more information."
},

    
      
      
      
      
      
    
{
  "title" : "Unix Tutorial Follow Along",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2020-04-29-Manhattan-KS",
  "description" : "",
  "date" : "Apr 29, 2020",
  "content" : "remotely for Manhattan, KS and Lincoln, NE   ·   lead: Andrew Severin, Erin Scully, Kathy Yeater, Kerrie GeilAndrew Severin of the SCINet VRSC held a 4-hour hands-on introduction to the command line, helping participants login to the SCINet Ceres high performance computer and execute common Unix commands. The training included 41 participants from Manhattan, KS and Lincoln, NE. This remote training is available for other ARS locations upon request. Please contact Andrew Severin at scinet_vrsc@usda.gov or one of the other organizers of this training for more information.To complete this training on your own, visit the tutorial at https://bioinformaticsworkbook.org/Appendix/Unix/unix-basics-1.html#gsc.tab=0"
},

    
      
      
      
      
      
    
{
  "title" : "Software Carpentry Workshop on Shell, Git, and Python for USDA",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2020-07-14-remote-carpentries-python",
  "description" : "",
  "date" : "Jul 14, 2020",
  "content" : "remotely on Zoom   &amp;middotInstructors from The Carpentries with the help of ARS scientists Kathy Yeater, Anna Childers, and Steve Schroeder held a 2-day Software Carpentry workshop covering Shell, Git, and Python for 20 USDA participants.See https://lachlandeer.github.io/2020-07-14-usda-online/ for the workshop agenda and details."
},

    
      
      
      
      
      
    
{
  "title" : "Software Carpentry Workshop on Shell, Git, and R for USDA",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2020-07-22-remote-carpentries-R",
  "description" : "",
  "date" : "Jul 22, 2020",
  "content" : "07/22/2020 - 7/23/2020  ·   remotely on Zoom   ·   lead: Jonathan Shao, Amanda Hulse-KempARS scientists Jonathan Shao and Amanda Hulse-Kemp, along with two other non-ARS instructors (Jacob Deppen, Preethy Nair), held a 2-day Software Carpentry workshop covering Shell, Git, and R for 20 USDA participants.To complete this training on your own, visit the online materials at https://nairps.github.io/2020-07-22-usda-online/."
},

    
      
      
      
      
      
    
{
  "title" : "2020 Geospatial Workshop",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2020-08-25-Geospatial-Workshop",
  "description" : "",
  "date" : "Aug 25, 2020",
  "content" : "The working group held their annual workshop over 6 separate Zoom sessions which were attended by over 60 scientists, post-docs, research leaders, and data managers. The sessions included an annual meeting of the working group, four tutorials on high-performance computing including 1 tutorial on the use of machine learning, and a symposium on the use of AI techniques in agricultural research. Detailed information on all the sessions, including recordings (to be posted soon), can be found on the session pages of the workshop website.Visit the workshop website for more informationSession 1: Annual Meeting of the SCINet Geospatial Research Working GroupSession 2 Tutorial: Intro to the Ceres HPC System Environment (SSH, JupyterHub, basic linux, SLURM batch script)Session 3 Tutorial: Intro to Distributed Computing on the Ceres HPC System Using Python and DaskSession 4: Tutorial: Computational Reproducibility Tools (Git/Github, Conda, Docker/Singularity containersSession 5 Tutorial: Distributed Machine Learning: Using Gradient Boosting to Predict NDVI DynamicsSession 6 Symposium: Challenges and opportunities in leveraging machine learning techniques to further sustainable and intensified agriculture"
},

    
      
      
      
      
      
    
{
  "title" : "UAS Data Wrangling Workshop",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-03-23-Virtual-UAS-data",
  "description" : "",
  "date" : "Mar 23, 2021",
  "content" : "On March 23, 2021, 50 ARS researchers and university collaborators participated in a UAS Data Wrangling workshop hosted by Mississippi State University Geosystems Research Institute (GRI). The workshop, led by Lee Hathcock with GRI, outlined the various common unmanned aerial system (UAS) payloads such as RGB and multispectral imagers, hyperspectral sensors, and LIDAR.  A sample processing flow was demonstrated in two of the popular commercial photogrammetry suites, Agisoft Metashape and Pix4D Pix4Dmapper, producing a reflectance-corrected and georeferenced orthomosaic from each software suite.The training is set to be offered again virtually in late spring (date TBD). Those interested in registering for the upcoming training can contact Dixie Cartwright,  dixie at gri dot msstate dot edu to be added to the waitlist.VirtualWorkshop Slides (284Mb)"
},

    
      
      
      
      
      
    
{
  "title" : "NMSU-USDA-ARS AI Workshop",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-07-23-NMSU-USDA-ARS-AI-Workshop",
  "description" : "",
  "date" : "Jul 23, 2021",
  "content" : "remotely on ZoomSCINet hosted a 4-day AI workshop presented by Laura Boucheron from New Mexico State University. Twenty-three USDA participants attended.To review the materials and recording, please visit https://kerriegeil.github.io/NMSU-USDA-ARS-AI-Workshops/"
},

    
      
      
      
      
      
    
{
  "title" : "Introduction to SCINet",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-10-28-Intro-to-SCINet",
  "description" : "",
  "date" : "Oct 28, 2021",
  "content" : "Remotely/ISUExplore how you can best utilize SCINet resources. Hear a brief overview of SCINet, learn how others incorporate HPCs into their research, and understand the next steps to integrating HPCs into your own research.For questions, please email scinet-training@usda.govTo learn more or register visit https://forms.office.com/g/aU9gGCpUaX"
},

    
      
      
      
      
      
    
{
  "title" : "Logging Into SCINet",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-10-29-Logging-in-to-SCINet",
  "description" : "",
  "date" : "Oct 29, 2021",
  "content" : "Remotely/ISULogging into SCINet: Get hands-on support to help walk you through the process of logging into SCINet . For questions, please email scinet-training@usda.govTo learn more or register visit https://forms.office.com/g/hciFTp1LUJ"
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Onboarding",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-10-29-SCINet-Onboarding",
  "description" : "",
  "date" : "Oct 29, 2021",
  "content" : "Remotely Through ZoomThis one hour course will cover SCINet Onboarding. It will cover logging into Ceres, project space, transferring data, and submitting jobs.To learn more or register visit https://forms.office.com/g/dJHn10d2dA"
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Onboarding",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-11-04-SCINet-Onboarding",
  "description" : "",
  "date" : "Nov 04, 2021",
  "content" : "Remotely Through ZoomThis one hour course covered logging into Ceres, project space, transferring data, and submitting jobs.Watch this event  (You must have a USDA email account to view)"
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Onboarding",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-11-05-SCINet-Onboarding",
  "description" : "",
  "date" : "Nov 05, 2021",
  "content" : "Remotely Through ZoomThis one hour course covers SCINet Onboarding inlcuding logging into Ceres, project space, transferring data, and submitting jobs.Watch this event (You must have a USDA email account to view)"
},

    
      
      
      
      
      
    
{
  "title" : "SCINet and AI COE Fellows Conference 2021",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-11-09-Fellows-conference",
  "description" : "",
  "date" : "Nov 09, 2021",
  "content" : "The SCINet and AI COE Fellows Conference 2021 was held on November 9-10. This conference featured research presentations from the Fellows as well as Fellow-moderated panel conversations on promoting collaboration opportunities.Postdoctoral Research Fellows funded by SCINet or the AI Center of Excellence through the ORISE Program bring a fresh perspective to the ARS scientist community, while allowing these Fellows to learn through hands-on research under our top scientists. Fellows are funded for two-year terms with the goal to support unit and collaborative research using high performance computing and computational science skills.  The SCINet and AI COE Fellows Conference was held on November 9-10, 2021. This conference featured research presentations from the Fellows as well as Fellow-moderated panel conversations on promoting collaboration opportunities.View the AgendaLearn more about the Fellows and view their talks.               Photo       Fellow Bio                              Michael Alcorn Ph.D. in Computer Science and Software Engineering (Minor in Mathematics), Auburn University, CV AI COE Mentors: Deb Peters &amp; Brian Stucky  Conference Presentation: Deep learning approaches for modeling spatiotemporal dynamics of a livestock disease  Keywords: Machine learning and artificial intelligence algorithms   Research Presentation Recording                        Jennifer Chang   Ph.D. in Bioinformatics and Computational Biology minor in Statistics, Iowa State University, CV  SCINet Mentor: Andrew Severin  Conference Presentation: TBA   Keywords: Bioinformatic workflow developer  Research Presentation Recording                        Keo Corak”&gt;[Keo Corak]  Ph.D. Plant Breeding and Plant Genetics, University of Wisconsin-Madison, CV  SCINet Mentor: Amanda Hulse-Kemp  Conference Presentation: Breeding Insight OnRamp: Lessons from the first five months   Keywords: Breeding informatics  Research Presentation Recording                        Alicia Foxx  Ph.D. in Plant Biology and Conservation, Northwestern University, CV  SCINet Mentor: Adam Rivers  Conference Presentation: Variability accounting methods in metagenomic and amplicon meta-analyses  Keywords: Plant microbiome  Research Presentation Recording                        Kerrie Geil  Ph.D. in Atmospheric Sciences, University of Arizona, CV  SCINet Mentor: Deb Peters  Presentation: Selecting climate projections for your research based on climate model performance metrics  Keywords: Climate modelling  Research Presentation Recording                        Lucas Heintzman  Ph.D. in Biology, Texas Tech University, CV  SCINet Mentor: Deb Peters  Conference Presentation: Examining land cover dynamics of Chihuahuan Desert rangelands using imagery  Keywords: Ecological connectivity modelling and spatial statistics  Research Presentation Recording                        Amy Hudson  Ph.D. in Natural Resources, University of Arizona, CV  SCINet Mentor: Deb Peters  Conference Presentation: Drivers of continental climate and resulting dynamics in agro-ecosystems and vector-borne diseases  Keywords: Climate science, phenology, disease ecology  Research Presentation Recording                        Melanie Kammerer  Ph.D. in Ecology, Pennsylvania State University, CV  SCINet Mentor: Sarah Goslee  Conference Presentation: Putting flowers on the map: quantifying landscape floral resources for bees  Keywords: Pollinator and spatial ecology  Research Presentation Recording                        Andrew Oliver  Ph.D. Molecular Biology and Biochemistry, University of California Irvine, CV  SCINet Mentor: Danielle Lemay  Conference Presentation: Diet, lifestyle, and microbiome predictors of antimicrobial resistance in a healthy US cohort  Keywords: Nutrition and the human microbiome   Research Presentation Recording                        Shawn Taylor   Ph.D. in Ecology, University of Florida, CV  SCINet Mentor: Dawn Browning  Conference Presentation: Deep learning models for identifying crop and field attributes from near surface cameras   Keywords: Phenology and image processing  Research Presentation Recording         "
},

    
      
      
      
      
      
    
{
  "title" : "Command-line Skills for Scientific Computing",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-11-17-command-line-skills",
  "description" : "",
  "date" : "Nov 17, 2021",
  "content" : "Command-line skills are invaluable for a wide range of modern data science and analysis tasks, from data acquisition and processing to high-performance computing.  Participants attended this 3-hour workshop to learn how to use the Unix command line with hands-on practice in a supportive environment."
},

    
      
      
      
      
      
    
{
  "title" : "Symposium: AI at the Frontiers of Protein Science for Agriculture",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-12-01-protein-science-symposium",
  "description" : "",
  "date" : "Dec 01, 2021",
  "content" : "Computational methods for investigating protein structure and function have advanced rapidly in recent years, including the development of new methods and software tools based on artificial intelligence (AI) techniques. These methodological advances will open doors to new research opportunities in plant breeding, functional genomics, insecticide/antimicrobial molecule discovery, and a host of other topics that will significantly impact US Agriculture.At this two-day symposium and conference, we learned about recent computational advances in protein science and potential applications of these new methods to agricultural research.  We also learned about protein-related research already underway at ARS, and we discussed new opportunities for AI-powered, protein-related science at ARS and strategies for pursuing those opportunities.Recordings of all presentations from the symposium may be viewed online:   Keynote presentation – Dr. John Moult   First lightning talk session   Keynote presentation – Dr. Darrell Hurt   Second lightning talk session "
},

    
      
      
      
      
      
    
{
  "title" : "Command-line Skills for Scientific Computing",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2021-12-08-command-line-skills",
  "description" : "",
  "date" : "Dec 08, 2021",
  "content" : "Command-line skills are invaluable for a wide range of modern data science and analysis tasks, from data acquisition and processing to high-performance computing.  Participants attended this 3-hour workshop to learn how to use the Unix command line with hands-on practice in a supportive environment.  Recordings of this workshop are available at the links below (a usda.gov account is required to view the videos).   Part 1   Part 2   Part 3 "
},

    
      
      
      
      
      
    
{
  "title" : "Introduction to Image Processing and Classical Machine Learning",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-01-10-Intro-To-ML",
  "description" : "",
  "date" : "Jan 10, 2022",
  "content" : "Remote   ·    MSUThis 2 day course covered an introduction to machine learning. To be added to the waitlist for future offerings, please submit a request at https://forms.office.com/g/tVtE8wEgAt"
},

    
      
      
      
      
      
    
{
  "title" : "Advanced Topics in Deep Learning",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-01-12-Advanced-DL",
  "description" : "",
  "date" : "Jan 12, 2022",
  "content" : "Remote   ·   MSUThis 3 day course covered advanced topics in deep learning. To be added to the waitlist for future offerings, please submit a request at https://forms.office.com/g/tVtE8wEgAt"
},

    
      
      
      
      
      
    
{
  "title" : "Data Carpentries",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-02-16-Carpentries",
  "description" : "",
  "date" : "Feb 16, 2022",
  "content" : "RemoteSCINet sponsored a Data Carpentry workshop covering Python, SQL and OpenRefine supporting 20 USDA participants.To complete this training on your own, visit the online materials at https://jas58.github.io/2022-02-16-usda-online/."
},

    
      
      
      
      
      
    
{
  "title" : "Data Carpentries",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-03-07-Carpentries",
  "description" : "",
  "date" : "Mar 07, 2022",
  "content" : "*RemoteSCINet sponsored a Data Carpentry workshop covering R, SQL and OpenRefine supporting ~20 USDA participants.To complete this training on your own, visit the online materials at https://kyeater.github.io/2022-03-07-usda-online/.Sign up to be notified when this course is offered gain."
},

    
      
      
      
      
      
    
{
  "title" : "Developing a UAS Program",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-03-07-UAS",
  "description" : "",
  "date" : "Mar 07, 2022",
  "content" : "RemoteThis SCINet-sponsored UAS workshop provided information to researchers about developing a robust UAS program to ensure their organization can take to the skies quickly, safely, and cost effectively.Sign up to be notified when this course is offered gain."
},

    
      
      
      
      
      
    
{
  "title" : "Software Carpentries",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-03-08-Carpentries",
  "description" : "",
  "date" : "Mar 08, 2022",
  "content" : "RemoteSCINet sponsored a Software Carpentry workshop covering  UNIX, GIT, and Python supporting ~20 USDA participants.To complete this training on your own, visit the online materials at https://steven-schroeder.github.io/2022-03-08-usda-online/.Sign up to be notified when this course is offered gain."
},

    
      
      
      
      
      
    
{
  "title" : "Data Wrangling Workshop",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-03-22-Data-Wrangling",
  "description" : "",
  "date" : "Mar 22, 2022",
  "content" : "Remote   ·   MSUThis workshop briefly touched on various common unmanned aerial system (UAS) payloads such as RGB and multispectral imagers, hyperspectral sensors, and LIDAR. It then provided an overview of how photogrammetry works with RGB/multispectral imagery. A sample processing flow was demonstrated in two of the popular commercial photogrammetry suites, Agisoft Metashape and Pix4D Pix4Dmapper, producing a reflectance-corrected and georeferenced orthomosaic from each software suite.Sign up to be notified when this course is offered again."
},

    
      
      
      
      
      
    
{
  "title" : "Software Carpentries",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-03-24-Carpentries",
  "description" : "",
  "date" : "Mar 24, 2022",
  "content" : "RemoteSCINet sponsored a Software Carpentry workshop covering the Unix shell, version control with Git, and plotting and programming with Python.The course website can be used to complete the exercises independently, or sign up to be notified when this course is offered again."
},

    
      
      
      
      
      
    
{
  "title" : "2022 Geospatial Workshop",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-08-25-Geospatial-Workshop",
  "description" : "",
  "date" : "Aug 25, 2022",
  "content" : " Workshop GoalsThe 2022 SCINet Geospatial Workshop continues the efforts outlined from the 2020 virtual workshop and the 2019 workshop held in Las Cruces, NM. The two overarching goals of this workshop are to:   Provide hands-on tutorials on workflows to access the SCINet high-performance computing (HPC) systems and conduct geospatial research at scale.   Foster research efforts that had previously been un-attainable due to computational limitations or technical bottlenecks. Organizing Committee   Heather Savoy, SCINet Computational Biologist, Las Cruces, NM   John Humphreys, Research Ecologist, Sidney, MT   Pat Clark, Rangeland Scientist, Boise, ID   Amy Hudson, SCINet Postdoc, Las Cruces, NM   Ryan Lucas, SCINet Data Science Coordinator, Ft Collins, CO   Brian Stucky, SCINet Computational Biologist and acting CSIO, Gainesville, FL How to Participate Working group members can register here for all workshop events. We also welcome anyone interested in learning about the working group, SCINet, or geospatial research to attend Session 1.The workshop is split over 11 separate Zoom sessions (as well as a pre-meeting assistance session) that will include:   Lightning presentations of geospatial workflow to address agricultural issues   Lectures on fundamental material related to geospatial data and parallel processing   Hands-on tutorials to assist researchers in utilizing the SCINet HPC systems   Collaborative Bring-Your-Own-Problem sessions to discuss your research needs To follow along with the tutorials you need to already have or apply for a SCINet account and be able to successfully login to your account.  We recommend applying for an account by 8/22/2022 at the latest, as the process can take time for final approval. Please note, if you need help accessing your SCINet account you should plan on attending the pre-meeting login assistance session on 8/25/2022 (Session 0), but make sure you have applied for an account in advance of this session.Please register for the sessions so we can have an idea of how many people will be present at each event. Note, each session will have a separate Zoom link and password so you must register for each session you would like to attend. Calendar events with zoom links will be sent out based on registrations.Lastly, review the pre-meeting checklist and background information on the Pre-Meeting page to ensure you are prepared for the workshop sessions. "
},

    
      
      
      
      
      
    
{
  "title" : "Digital Tools to Move from Precision to Decision Agrictulture",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-11-11-digital-tools",
  "description" : "",
  "date" : "Nov 11, 2022",
  "content" : "Digital technologies are valuable tools that may help farmers improve efficiency and make better decisions. Remote sensing has been a key source of data from the early days of digital agriculture and has been linked to crop mapping, biophysical variables, yield forecasting, and prescriptive analytics required to monitor crop growth and agricultural output from the field scale to the global scale. The remote sensing sector has never been more capable of helping deliver on the promises of digital agriculture, thanks to recent developments in machine learning and artificial intelligence. But there are some problems and limitations that need to be fixed before these technologies can be used effectively and agriculture is digitally transformed on a large scale. These include accuracy, interoperability, data storage, computational capacity, and farmers’ reluctance to use them. The seminar will focus on two case studies: retrieval of crop biophysical parameters from UAV hyperspectral imagery and fruit detection using high-resolution UAV RGB imagery."
},

    
      
      
      
      
      
    
{
  "title" : "Geospatial Workshop",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2022-11-29-Carpentries",
  "description" : "",
  "date" : "Nov 29, 2022",
  "content" : "The SCINet Office is collaborating with The Carpentries to provide a Geospatial Workshop. This workshop will provide instruction for working with geospsatial data using R.Fill out the registration form if you are interested in taking this course."
},

    
      
      
      
      
      
    
{
  "title" : "Software Carpentry with Python",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2023-02-23-carpentries",
  "description" : "",
  "date" : "Feb 23, 2023",
  "content" : "RemoteWorkshop dates: Feb. 23-24, Mar. 2-3Sign up for this course by filling out this form"
},

    
      
      
      
      
      
    
{
  "title" : "AI Training Images Workshop",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2023-02-28-AI",
  "description" : "",
  "date" : "Feb 28, 2023",
  "content" : "Are you interested in AI-based image analysis methods for scientific research?  If so, please join us for an AI training images workshop at the end of February!   What are “AI training images”?  AI-based image analysis techniques typically require relatively large image datasets – “training images” – to help the AI system improve its performance on a given analysis task.  Obtaining suitable training images is often one of the most expensive and time-consuming aspects of applying AI image analyses in scientific research.On February 28, 2023, from 12-4 pm EST, ARS’s AI Center of Excellence will host a virtual workshop to help lay the foundations for building shared AI training image resources for ARS research.  We will:   Help attendees gain a better understanding of AI image analysis methods, how they are used in ARS, and why we need agency-wide training image resources.   Identify training image requirements and use cases.   Begin to develop an inventory of extant training image datasets. At the beginning of the workshop, we will host a lightning talk session for attendees to share their work using AI-based image analyses.  If you would like to attend the workshop, please fill out this form."
},

    
      
      
      
      
      
    
{
  "title" : "Software Carpentry with R",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2023-03-27-carpentries",
  "description" : "",
  "date" : "Mar 27, 2023",
  "content" : "RemoteWorkshop dates: Mar. 27-28, Apr. 3-4Sign up for this course by filling out this form"
},

    
      
      
      
      
      
    
{
  "title" : "Software Carpentry with Python",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/2023-04-11-carpentries",
  "description" : "",
  "date" : "Apr 11, 2023",
  "content" : "RemoteWorkshop dates: Apr. 11-12, 18-19Sign up for this course by filling out this form"
},


  
    
      
      
      
      
      
    
{
  "title" : "Introduction to Atlas",
  "category" : "Trainings and Events",
  "subcategory" : "Events",
  "set" : "",
  "url" : "/events/intro-to-atlas",
  "description" : "",
  "date" : "Mar 06, 2023",
  "content" : "Remotely/MSUThis 60-minute information session is for anyone interested in learning more about USDA’s new state-of-the art supercomputer Atlas. Atlas is a high-performance computer with 101 terabytes of total RAM designed to help power research advances in biocomputing, epidemiology, geospatial technology and more. This introduction will cover a range of topics that include everything from a general overview of high-performance computing and why you might want to take advantage of its capabilities in your research to basic protocols associated with using Atlas (how to gain access to Atlas, how to log-in to your account, how to transfer data, etc.). This introduction session will be led by Dr. Adam Thrash, MSU Research Associate and Computer Specialist with MSU’s Institute of Genomics, Biocomputing, and Biotechnology."
},


  
    
      
      
      
      
      
    
{
  "title" : "Accessing SCINet",
  "category" : "User Guides",
  "subcategory" : "Access",
  "set" : "",
  "url" : "/guides/access",
  "description" : "How to access SCINet",
  "date" : "Mar 06, 2023",
  "content" : "If you have recieved your login credentials in an email, this guide will help you get connected to SCINet.  Otherwise, please email the Virtual Research Support Core at scinet_vrsc@USDA.GOV for assistance."
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Analysis",
  "category" : "User Guides",
  "subcategory" : "Analysis",
  "set" : "",
  "url" : "/guides/analysis",
  "description" : "Guide to runnning Analyses on SCINet",
  "date" : "Mar 06, 2023",
  "content" : ""
},

    
      
      
      
      
      
    
{
  "title" : "AWS Resources",
  "category" : "User Guides",
  "subcategory" : "AWS",
  "set" : "",
  "url" : "/guides/aws",
  "description" : "AWS Product Resources",
  "date" : "Mar 06, 2023",
  "content" : ""
},

    
      
      
      
      
      
    
{
  "title" : "Storage and Data Management",
  "category" : "User Guides",
  "subcategory" : "Data",
  "set" : "",
  "url" : "/guides/data",
  "description" : "Guide to storage options on SCINet HPC clusters",
  "date" : "Mar 06, 2023",
  "content" : ""
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Computing Resources",
  "category" : "User Guides",
  "subcategory" : "Resources",
  "set" : "",
  "url" : "/guides/resources",
  "description" : "Guide to SCINet HPC",
  "date" : "Mar 06, 2023",
  "content" : "HPC Clusters on SCINet                         Cluster name         Location         Login Nodes         Transfer Nodes                                 Ceres         Ames, IA         ceres.scinet.usda.gov         ceres-dtn.scinet.usda.gov                       Atlas         Starkville, MS         atlas-login.hpc.msstate.edu         atlas-dtn.hpc.msstate.edu                       TBD         Beltsville, MD         TBD         TBD                 "
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Software",
  "category" : "User Guides",
  "subcategory" : "Software",
  "set" : "",
  "url" : "/guides/software",
  "description" : "Guide to installing Software on SCINet",
  "date" : "Mar 06, 2023",
  "content" : "The login node provides access to a wide variety of scientific software tools that users can access and use via the module system. These software tools were compiled and optimized for use on SCINet by members of the Virtual Research Support Core (VRSC) team. Most users will find the software tools they need for their research among the provided packages and thus will not need to compile their own software packages.The popular R, Perl, and Python languages have many packages/modules available. Some of the packages are installed on Ceres and are available with the r/perl/python_2/python_3 modules. To see the list of installed packages, visit the Preinstalled Software List page or use  module help &lt;module_name&gt;  command. If users need packages that are not available, they can either request VRSC to add packages, or they can download and install packages in their home/project directories. We recommend installing packages in the project directories since collaborators on the same project most probably would need the same packages. In addition, home quotas are much lower than project directories quotas. See the Guide to Installing R, Python, and Perl Packages for instructions and examples on how to add packages/modules for these languages.If users would like to compile their own software with GNU compilers, they will need to load the gcc module. It is recommended to compile on compute nodes and not on the login node. However, before embarking on compiling their own software packages we strongly encourage users to contact the VRSC team to ensure that their required tool(s) might not be better distributed as a shared package within the official software modules tree. All new software needs to be approved by SOC committee before being centrally installed on the system. To request a new software package to be installed, visit the Request Software page."
},


  
    
      
      
      
      
      
    
{
  "title" : "Differences between Ceres and Atlas",
  "category" : "User Guides",
  "subcategory" : "Resources",
  "set" : "",
  "url" : "/guides/resources/ceresatlasdifferences",
  "description" : "A guide to the differences between the Ceres cluster and the Atlas cluster",
  "date" : "Mar 06, 2023",
  "content" : "This guide lists differences between the Atlas and Ceres clusters to ease transition from one cluster to another.QuotasAll project directories exist on both clusters, however they may have different quotas. Data in the project (and home) directories is not automatically synced between the clusters. The default project directory quota on Atlas is 1TB.On Ceres usage and quota information for home and project directories that user belongs to is displayed at login (to speed-up logins, the usage data is calculated once a day). To get the current usage information, users can issue the “my_quotas” command. On Atlas “quota -s” command reports usage and quota for the home directory and “/apps/bin/reportFSUsage -p proj1,proj2,proj3” provides that information for specific projects.SoftwareNot all software installed on Ceres is available on Atlas. However software packages provided as Singularity container image files in the Ceres Container Repository are synced to Atlas daily.Using ContainersOn both clusters some modules are installed as singularity containers. On Ceres, all modules, including container based, are displayed in the output of the “module avail” command. On Atlas one needs to load a singularity module (“module load singularity” or “module load singularity/&lt;version&gt;”) to make the “singularity” command accessible and to see container based modules:module load singularity module avail Furthermore, on Atlas one needs to explicitly issue singularity command to use a container based software:singularity exec &lt;container image&gt; On Ceres, loading a container-based module will also load a singularity module, and one does not need to type the “singularity exec ...” command since it’s already built in the module. One can use “module help &lt;module_name&gt;” command to get information on how to use the software package provided by the module.Seeing All Environment ModulesSimilar to containers, some modules on Atlas are not reported by the “module avail” command and can not be loaded without loading an appropriate gcc module:module load gcc/10.2.0 module avail module load canu/2.2 To see all available modules on Atlas, one can use “module spider” command instead of the “module avail” command. This command will also work on Ceres, but it’s not necessary to use it.Atlas-login-1[7] marina.kraeva$ module spider canu  --------------------------------------------------------------------------------------------------------------------   canu: --------------------------------------------------------------------------------------------------------------------      Versions:         canu/2.1.beta         canu/2.1         canu/2.2  --------------------------------------------------------------------------------------------------------------------   For detailed information about a specific canu package (including how to load the modules) use the module's full name.   Note that names that have a trailing (E) are extensions provided by other modules.   For example:       $ module spider canu/2.2 -------------------------------------------------------------------------------------------------------------------- CondaTo build a Conda environment on Ceres, one can load miniconda module (“module load miniconda”). On Atlas one needs first to install miniconda in their home directory:wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Since home directories on both clusters have 10GB quota, it’s recommended to move the installed miniconda to a project directory and create a symbolic link to the new location in the home directory:mv ~/miniconda3 /project/project_folder/software/. ln -s /project/project_folder/software/miniconda3 ~/. Submitting a JobJob scripts from one cluster may not work on the other cluster. See below what needs to be changed in the script to make it work. This also applies to the salloc/srun commands.Slurm accountTo run jobs on compute nodes of either cluster, the jobs need to be associated with a slurm account. For users that have access to one or more project directories, their slurm accounts have same names as the project directories. The slurm account for users without project directories is called sandbox. On Ceres cluster all users have a default slurm account, and thus when submitting a job, they don’t need to specify a slurm account for the job unless they want to associate the job with a non-default slurm account (this concerns only users with multiple slurm accounts; see the SCINet Ceres User Manual “Slurm Accounts” section for how to list and change the default Slurm account). On Atlas one needs to specify a slurm account when submitting a job. To specify slurm account, either use “-A &lt;slurm_account_name&gt;” on the slurm command (srun, salloc, sbatch) or add the following line to your job script:#SBATCH -A &lt;slurm_account_name&gt; PartitionsOne does not have to specify a partition when submitting a job to a default partition on either Ceres or Atlas. However scripts that have a partition specified will need to be updated when used on a different cluster. To see the list of available partitions on a cluster, either issue “sinfo” command or consult the appropriate user guide: Ceres or Atlas.NodesTwo clusters have different node types, that have different numbers of compute cores and amounts of memory. For most jobs the scripts already specify resources that are available on either cluster, so no changes need to be made. However if a job relies on the default amount of memory being large, it may fail on the other cluster that has smaller defaults. Some jobs may have high number of cores specified. On Ceres hyper-threading is turned on, meaning that each physical core on a node appears as two separate processors to the operating system and can run two threads. Because of that setting all nodes have at least 72 cores available. On Atlas, nodes have 48 cores. Thus if a script specifies more than 48 cores per node, it won’t be accepted on Atlas.Internet ConnectionOn Atlas compute nodes do not have access to internet. If your job requires internet access, either submit it to the service partition or modify the job prefetching data and using the previously downloaded data in the job. Conda installs will need to be performed either on the login or data transfer nodes.sallocOn Ceres issuing “salloc” command will allocate resources for an interactive job and log you into the node assigned to the job. On Atlas the “salloc -A &lt;slurm_account_name&gt;” command will only allocate resources, but one will still need to use srun command to utilize resources assigned to the job. To login to the node, issue “srun --pty --preserve-env bash”. Note that exiting from that session will not kill the job, and to release resources assigned to the job, you will need to issue “scancel &lt;job_number&gt;” command. Note that salloc command is not needed, and one can only use the “srun -A &lt;slurm_account_name&gt; --pty --preserve-env bash” command to use a node interactively."
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Atlas",
  "category" : "User Guides",
  "subcategory" : "Resources",
  "set" : "",
  "url" : "/guides/resources/atlas",
  "description" : "Guide to Atlas",
  "date" : "Mar 06, 2023",
  "content" : ""
},

    
      
      
      
      
      
    
{
  "title" : "AWS Product Documentation",
  "category" : "User Guides",
  "subcategory" : "AWS",
  "set" : "",
  "url" : "/guides/aws/aws",
  "description" : "AWS Product Documentation",
  "date" : "Mar 06, 2023",
  "content" : "How To Project Monthly Costs For AWS ProductsThe following instructions will demonstrate how to calculate the projected monthly cost of an AWS project. In this example, the cost of an EC2 Instance is calculated.Utilized Resources:   AWS Console via Oauth Authentication   AWS Simple Monthly Calculator    Navigate to https://auth.scinet.usda.gov/ and click the link for “AWS Login”     Authenticate using your SciNet credentials     Ensure you have the correct region selected “US East (N. Virginia)” in the top right dropdown menu of the AWS console:     Navigate to the “Services” menu, and select “Managment Tools” &gt; “Service Catalog”     From the Service Catelog menu select “Products list”     From here you can selelect the product you wish to calculate the monthly cost for, in this case we’ll choose “EC2 Instance”     Select the latest product version, in this case “0.2.0”. You will then see the Service Catalog template in YAML format. This template contains the product configuration options you’ll plug into the AWS Simple Monthly Calculator.     Open a new browser tab to the AWS Simple Monthly Calculator   Navigate to the applicable product tab in the left hand sidebar the the AWS Simple Monthly calculator            Uncheck the “FREE USAGE TIER: New Customers get free usage tier for first 12 months” checkbox in the top message banner            Click the “Add New Row” icon under the “Compute: Amazon EC2 Instances”.            Make mental note of the configurable parameters such as “Region”, and “Type”. For Billing Option you’ll always select “On-Demand” and “100” for Usage.            Switch back to the browser tab containing the YAML product template, identify the avaliable options that can be used in the AWS Simple Monthly Calculator. In this case, “InstanceType” and “AWSRegionImageType2AMI” an be used.     Select the desired Region and Instance Type from the list of allowed options. Let’s use t2.large and us-west-1 in this example.   Now switch back to the AWS Simple Monthly Calculator and plug in the values, always selecting the Region first since it will clear the calculator table values.       Now enter any additional values or estimations in the form fields below:     You will now see the projected monthly cost for the applicable product in the “Estimate of your Monthly Bill” tab   Example EC2 Projected Costs"
},

    
      
      
      
      
      
    
{
  "title" : "Create an AWS Resource",
  "category" : "User Guides",
  "subcategory" : "AWS",
  "set" : "",
  "url" : "/guides/aws/aws-create-resource",
  "description" : "How to create an AWS resource via the service catalog",
  "date" : "Mar 06, 2023",
  "content" : "Guide to creating an AWS resource via the service catalog.Log inIn your web browser, visit https://shibboleth.scinet.science/ and click the “Amazon Web Services” link. Log in with your SCInet username and password. If you are presented with a role selection screen, choose your desired project to proceed to the console.AWS Service Catalog ConsoleYou must be using the US East (N. Virginia) region for all AWS activities. Please check your currently selected region before following this document’s instructions.  To check and update your region, click on the dropdown in the upper right of the console window, just next to “Support.” Select US East.(N. Virginia)In the main Console page, click on “Service Catalog” within the Management Tools section, or type “ServiceCatalog” into the search bar.   You will see all of AWS’s service offerings listed on the main page, even though your project role likely restricts your access. Don’t get overwhelmed! If you get lost, click the AWS logo in the upper left to get back to the main console.  The Service Catalog lists the products that have been assigned to your project, as well as the products that have been provisioned (launched) provisioned by you or on your behalf.   If your Service Catalog console looks different than the one in thescreenshot, you are using the “old look”. You can switch to the “newlook” by clicking on the dropdown in the upper left of Service Catalog the console and choosing. Try the new look. The products assigned to your project may differ from those shown in the screenshot. Create a resource - Launch a ProductClick the three-dots button on the desired product card and choose Launch Product, or click on the product name and click Launch Product on thefollowing screen. Step 1: Name and VersionProvide a name for the product to be launched. This name will be shown in the list of provisioned products, and must contain only letters, numbers, dashes, underscores and periods. Also. select the desired version out of the available versions.Step 2: Product ParametersFill in the parameters as made available by the product template definition.   Some parameters may have limits or requirements. If you receive a warning message saying ‘abcde must equal the value of “Parameter”’, copy and paste the value on the left-hand side into the parameter field to proceed through the warning.  Step 3: TagOptionsYou may provide optional tags to help you locate and track your resources. For most users, the default tags are sufficient.Steps 4-5: Notifications and ReviewContinue through the Notifications screen by clicking Next. Finally, click Launch on the last page to launch the product Provisioned Product DetailsAfter initially launch, the product details page will show the product as “Under change” with status “In progress”. The page will automaticallyupdate (or you can refresh the page if desired). When complete, the product will change to “Available” with status “Succeeded” Follow the instructions in the AWS Product Documentation for more details on how to use the product you just launched.Delete a Resource - Terminate a ProductView the list of provisioned products (resources previously launched) by clicking “Provisioned products list” in the Service Catalog console menu(the three bars icon in the upper-left). From the three-dots menu, select “Terminate provisioned product”, or choose Terminate from the Actions menu within the Provisioned product details screen. Click “Terminate” to confirm termination of the product. The screen will show “Under change” to show that the product is in process of termination. "
},

    
      
      
      
      
      
    
{
  "title" : "AWS Resource FAQ",
  "category" : "User Guides",
  "subcategory" : "AWS",
  "set" : "",
  "url" : "/guides/aws/aws-faq",
  "description" : "AWS resourses availble to ARS scientists",
  "date" : "Mar 06, 2023",
  "content" : "FAQ about AWS resources available to ARS scientists.How do I get started?First, if you’ve never used AWS before or you’re unfamiliar with its current capabilities for SCInet, talk to someone at the VRSC (you can email scinet_vrsc@usda.gov) to discuss how it may apply to your problem. Before you start, you should have a reasonable idea of what you’d like to spin up and about how much it may cost.Once you’re ready to begin the process, please use the stand ARS project request form to request an AWS project.  If your group already has an existing AWS project you can just request to be added to it.  Click here for the AWS project request form.This will be a space with a managed budget and controlled set of resources that you can launch, and where you can log in using your SCInet credentials.How do I log in?Visit https://auth.scinet.usda.gov/  You will need to use your SCINet password immediately folowed by your 6 digit GA code.I’m logged in fine, but I don’t see the Service Catalog that I expect. Where did it go?Make sure that you have the right region selected! Your region is shown in a dropdown menu in the upper right of the screen. It should say “N. Virginia” -if it says something like “Ohio”, click the menu and select “US East (N. Virginia)”.I’m trying to do something in the AWS Console, but I’m getting an error message saying I don’t have enough permissions. What should I do?If you’re not in the Service Catalog, then you probably aren’t allowed to take that action directly. Most creation and deletion operations must be done through the Service Catalog. If you’re trying to do something that isn’t already in the Service Catalog, contact the VRSC for assistance.  If you are in the Service Catalog, you may see occasional harmless notices of a permission problem, which I believe is due to a bug. If you see errors that are preventing you from performing a task, contact the VRSC for troubleshootingI’d like to use a service that isn’t covered in the Service Catalog. Can I have it added?The VRSC will need to evaluate whether your desired service is appropriate for being included in the Service Catalog. Some services currently aren’t suitable for various reasons; unfortunately, if this is the case, it’s likely because we couldn’t control costs or permissions for that service sufficiently to prevent other users from interfering with you. If it is possible, make the request and be patient -it can take some time to write the proper template in asecure and reliable manner."
},

    
      
      
      
      
      
    
{
  "title" : "Using AWS SAML",
  "category" : "User Guides",
  "subcategory" : "AWS",
  "set" : "",
  "url" : "/guides/aws/aws-saml",
  "description" : "Retrieve AWS-SAML API keys",
  "date" : "Mar 06, 2023",
  "content" : "The aws-saml tool can be used in conjunction with the Shibboleth SAML identity provider to retrieve time-limited API keys suitable for commandline use. It interactively prompts you for your password, and if you have multiple roles available, you are prompted if not otherwise specified on the command line. The resulting credentials are normally stored in your standard AWS credential file, but a command line flag can be provided tohave the credentials output to standard output in Bash format for scripting. These credentials normally expire after one hour; by providing therefresh flag to the tool, it will fork into the background and keep the credentials refreshed as long as your login cookie remains valid.InstallationThe tool is currently stored at https://stash.scinet.science/projects/UAAI/repos/aws-saml-cli/browse. From here, the code can be cloned to your local machine, and installed with the command$ pip install from the directory containing setup.pyUsageOnce installed, basic usage is simple.$ aws-saml -u your.username On first usage, you will be prompted for an IDP Entry URL. For SCInet’s Shibboleth server, this is:  https://shibboleth.scinet.science/idp/profile/SAML2/Unsolicited/SSO?providerId=urn:amazon:webservices When prompted for your password, enter your SCInet password.  If prompted for a role, enter the number corresponding to the role you’d like to use for this session.Please choose the role you would like to assume: [ 0 ]:  arn:aws:iam::123456789012:role/SAML-proj-beta_testers [ 1 ]:  arn:aws:iam::123456789012:role/SAML-admin Selection: 1 Full usage documentationusage: aws-saml [-h] [--username USERNAME] [--prompt-role] [--role ROLE]                         [--credentials FILE] [--profile PROFILE] [--verbose]                         [--stdout] [--refresh] [--no-ssl-verify]  Retrieve AWS credentials via SAML.optional arguments:     -h, --help            show this help message and exit       --username USERNAME, -u USERNAME                             Login username       --prompt-role, -R     Prompt for role selection       --role ROLE, -r ROLE  Desired role name       --credentials FILE, -c FILE                             Destination file for AWS credentials       --profile PROFILE, -p PROFILE                             AWS credential file profile       --verbose, -v       --stdout, -s                                       Print credentials in bash format to standard output       --refresh             Fork into the background, keeping token fresh       --no-ssl-verify       Disable SSL certificate verification for IDP (allows self-signed certificates - INSECURE)  "
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Ceres",
  "category" : "User Guides",
  "subcategory" : "Resources",
  "set" : "",
  "url" : "/guides/resources/ceres",
  "description" : "Guide to Ceres",
  "date" : "Mar 06, 2023",
  "content" : "Onboarding VideosUsers who are new to the HPC environment may benefit from the following Ceres onboarding video which covers much of the material contained in this guide plus some Unix basics.Ceres Onboarding (Intro to SCINet Ceres HPC) (length 42:13)Note: /KEEP storage discussed in the video at 16:20 is no longer available. Instead data that cannot be easily reproduced should be manually backed up to Juno. The instructional video at https://www.youtube.com/watch?v=I3lnsCAfx3Q demonstrates how to transfer files between local computer, Ceres, Atlas and Juno using Globus.The video includes:   logging on to Ceres   changing your password   home and project directories   data transfer to/from SCINet clusters   basic SLURM job scheduler commands   computing in interactive mode with salloc   accessing Ceres software modules   computing in batch mode with a batch script Technical OverviewCeres is the dedicated high performance computing (HPC) infrastructure for ARS researchers on ARS SCINet. Ceres is designed to enable large-scale computing and large-scale storage. Currently, the following compute nodes are available on the Ceres cluster.120 regular compute nodes, each having:   72 logical cores on 2 x 18 core Intel Xeon Processors (6140 2.30GHz 25MB Cache or 6240 2.60GHz 25MB Cache) with hyper-threading turned ON   384GB DDR3 ECC Memory   250GB Intel DC S3500 Series 2.5” SATA 6.0Gb/s SSDs (used to host the OS and provide small local scratch storage)   1.5TB SSD used for temporary local storage   Mellanox ConnectX®­3 VPI FDR InfiniBand 76 regular compute nodes, each having:   96 logical cores on 2 x 24 core Intel Xeon Processors (6240R 2.40GHz 36MB Cache) with hyper-threading turned ON   384GB DDR3 ECC Memory   250GB Intel DC S3500 Series 2.5” SATA 6.0Gb/s SSDs (used to host the OS and provide small local scratch storage)   1.5TB SSD used for temporary local storage   Mellanox ConnectX®­3 VPI FDR InfiniBand 4 large memory nodes, each having:   80 logical cores on 2 x 20 core Intel Xeon Processors (6148 2.40GHz 27.5MB Cache or 6248 2.50GHz 27.5MB Cache) with hyper-threading turned ON   768GB DDR3 ECC Memory   250GB Intel DC S3500 Series 2.5” SATA 6.0Gb/s SSDs (used to host the OS and provide small local scratch storage)   1.5TB SSD used for temporary local storage   Mellanox ConnectX®­3 VPI FDR InfiniBand 11 large memory nodes, each having:   80 logical cores on 2 x 20 core Intel Xeon Processors (6148 2.40GHz 27.5MB Cache or 6248 2.50GHz 27.5MB Cache) with hyper-threading turned ON   1,536GB DDR3 ECC Memory   250GB Intel DC S3500 Series 2.5” SATA 6.0Gb/s SSDs (used to host the OS and provide small local scratch storage)   1.5TB SSD used for temporary local storage   Mellanox ConnectX®­3 VPI FDR InfiniBand 11 large memory nodes, each having:   96 logical cores on 2 x 24 core Intel Xeon Processors (6248R 3GHz 27.5MB Cache or 6248 2.50GHz 27.5MB Cache) with hyper-threading turned ON   1,536GB DDR3 ECC Memory   250GB Intel DC S3500 Series 2.5” SATA 6.0Gb/s SSDs (used to host the OS and provide small local scratch storage)   1.5TB SSD used for temporary local storage   Mellanox ConnectX®­3 VPI FDR InfiniBand 1 GPU node that has:   72 logical cores on 2 x 18 core Intel Xeon Processors (6140 2.30GHz 25MB Cache) with hyper-threading turned ON   2 Tesla V100   384GB DDR3 ECC Memory   250GB Intel DC S3500 Series 2.5” SATA 6.0Gb/s SSDs (used to host the OS and provide small local scratch storage)   1.5TB SSD used for temporary local storage   Mellanox ConnectX®­3 VPI FDR InfiniBand In addition there are a specialized data transfer node and several service nodes.In aggregate, there are more than 9000 compute cores (18000 logical cores) with 110 terabytes (TB) of total RAM, 500TB of total local storage, and 3.7 petabyte (PB) of shared storage.Shared storage consists of 2.3PB high-performance Lustre space, 1.4PB high-performance BeeGFS space and 300TB of backed-up ZFS space.System ConfigurationSince most HPC compute nodes are dedicated to running HPC cluster jobs, direct access to the nodes is discouraged. The established HPC best practice is to provide login nodes. Users access a login node to submit jobs to the cluster’s resource manager (SLURM), and access other cluster console functions. All nodes run on Linux CentOS 7.8.Software Environment               Domain       Software                       Operating System       CentOS                 Scheduler       SLURM                 Software       For the full list of installed scientific software refer to the Preinstalled Software List page or issue the  module spider  command on the Ceres login node.                 Modeling       BeoPEST, EPIC, KINEROS2, MED-FOES, SWAT, h2o                 Compilers       GNU (C, C++, Fortran), clang, llvm, Intel Parallel Studio                 Languages       Java 6, Java 7, Java 8, Python, Python 3, R, Perl 5, Julia, Node                 Tools and Libraries       tmux, Eigen, Boost, GDAL, HDF5, NetCDF, TBB, Metis, PROJ4, OpenBLAS, jemalloc                 MPI libraries       MPICH, OpenMPI                 Profiling and debugging       PAPI         For more information on available software and software installs refer to our guides on Modules, Singularity Containers and Installing R, Python, and Perl Packages.Additional Guides for Ceres:"
},

    
      
      
      
      
      
    
{
  "title" : "Cite SCINet",
  "category" : "User Guides",
  "subcategory" : "Resources",
  "set" : "",
  "url" : "/guides/resources/citation",
  "description" : "SCINet Citation/Acknowledgment in Publications",
  "date" : "Mar 06, 2023",
  "content" : "Add the following sentence as an acknowledgment for using CERES as a resource in your manuscripts meant for publication:“This research used resources provided by the SCINet project of the USDA Agricultural Research Service, ARS project number 0500-00093-001-00-D.” "
},

    
      
      
      
      
      
    
{
  "title" : "CLC Server",
  "category" : "User Guides",
  "subcategory" : "Analysis",
  "set" : "",
  "url" : "/guides/analysis/clc-workbench",
  "description" : "Using CLC Server",
  "date" : "Mar 06, 2023",
  "content" : "This document assumes that a licensed copy of CLC Genomics WorkBench 22 is installed locally and available to the user. Before You BeginEmail scinet_vrsc@USDA.GOV so that the admins can setup the import/export directories and permissions for access.We need the following information:   Path to your project directory.   Do you need access to the mem nodes for your CLC workflow? CLC Server Login   File -&gt; Connections -&gt; CLC Server Connection   Server Name and Port    If connecting via VPN/OCVPN     Server name: 10.1.5.210 Server port: 7777           If connecting via ARS Network     Server host: 205.237.112.197 Server port: 7777                 Username and Password(GA code not required)            Log in. After successful login, you should see a directory CLC-&lt;your project&gt; in the top left window.CLC Server vs GRIDCLC provides two ways to offload jobs - CLC Server and GRID. Both serve different purposes.CLC Server can only be used to perform Standard Import and Export. These tasks are performed on Ceres DTN node for faster transfer rate.Note that if users select “CLC Server” for any tasks other than those mentioned below, those tasks will not execute and will remain paused.GRID can be used to run compute tasks on Ceres nodes.The table below lists the tasks and option to use               Category       Utilities       Option                       Export               CLC Server                 Import               CLC Server                 Search for Reads in SRA               GRID                 Classical Sequence Analysis                                         Create Alignment       GRID                         K-mer Based Tree Construction       GRID                         Create Tree       GRID                         Model Testing       GRID                         Maximum Likelihood Phylogeny       GRID                         Extract Sequences       GRID                         Motif Search       GRID                         Translate to Protein       GRID                         Convert DNA to RNA       GRID                         Convert RNA to DNA       GRID                         Reverse Complement Sequence       GRID                         Find Open Reading Frames       GRID                         Download Pfam Database       GRID                         Pfam Domain Search       GRID                         Find and Model Structure       GRID                 Molecular Biology Tools                                         Trim Sequences       GRID                         Assemble Sequences       GRID                         Assemble Sequences to Reference       GRID                         Secondary Peak Calling       GRID                         Find Binding Sites and Create Fragments       GRID                         Add attB Sites       GRID                         Create Entry clone (BP)       GRID                         Create Expression clone (LR)       GRID                 BLAST                                         BLAST       GRID                         BLAST at NCBI       GRID                         Download BLAST Databases       GRID                         Create BLAST Database       GRID                 Prepare Sequencing Data                                         QC for Sequencing Reads       GRID                         Trim Reads       GRID                         Demultiplex Reads       GRID                 Quality Control                                         QC for Targeted Sequencing       GRID                         QC for Read Mapping       GRID                         Whole Genome Coverage Analysis       GRID                         Combine Reports       GRID                         Create Sample Report       GRID                 Resequencing Analysis                                         Map Reads to Reference       GRID                         Local Realignment       GRID                         Merge Read Mappings       GRID                         Remove Duplicate Mapped Reads       GRID                         Extract Consensus Sequence       GRID                         Basic Variant Detection       GRID                         Fixed Ploidy Variant Detection       GRID                         InDels and Structural Variants       GRID                         Identify Known Mutations from Mappings       GRID                         Copy Number Variant Detection (CNVs)       GRID                         Filter against Known Variants       GRID                         Remove Marginal Variants       GRID                         Remove Homozygous Reference Variants       GRID                         Remove Variants Present in Control Reads       GRID                         Annotate from Known Variants       GRID                         Remove Information from Variants       GRID                         Annotate with Conservation Scores       GRID                         Annotate with Exon Numbers       GRID                         Annotate with Flanking Sequences       GRID                         Annotate with Repeat and Homopolymer Information       GRID                         Identify Enriched Variants in Case vs Control Samples       GRID                         Identify Shared Variants       GRID                         Trio Analysis       GRID                         Create Variant Track Statistics Report       GRID                         Amino Acid Changes       GRID                         Predict Splice Site Effect       GRID                         GO Enrichment Analysis       GRID                         Download 3D Protein Structure Database       GRID                         Link Variants to 3D Protein Structure       GRID                 RNA-Seq and Small RNA Analysis                                         RNA-Seq Analysis       GRID                         PCA for RNA-Seq       GRID                         Differential Expression in Two Groups       GRID                         Differential Expression for RNA-Seq       GRID                         Create Heat Map for RNA-Seq       GRID                         Create Expression Browser       GRID                         Create Venn Diagram for RNA-Seq       GRID                         Gene Set Test       GRID                         Quantify miRNA       GRID                         Annotate with RNAcentral Accession Numbers       GRID                         Create Combined miRNA Report       GRID                         Extract IsomiR Counts       GRID                 Microarray Analysis                                         Create Box Plot       GRID                         Principal Component Analysis       GRID                         Proportion-based Statistical Analysis       GRID                         Gaussian Statistical Analysis       GRID                         Create MA Plot       GRID                         Create Scatter Plot       GRID                         Create Histogram       GRID                 Epigenomics Analysis                                         Histone ChiP-Seq       GRID                         Transcription Factor ChIP-Seq       GRID                         Annotate with Nearby Gene Information       GRID                         Map Bisulfite Reads to Reference       GRID                         Call Methylation Levels       GRID                         Create RRBS-fragment Track       GRID                         Learn Peak Shape Filter       GRID                         Apply Peak Shape Filter       GRID                         Score Regions       GRID                 De Novo Sequencing                                         De Novo Assembly       GRID                         Map Reads to Contigs       GRID                 Utility Tools                                         Extract Annotated Regions       GRID                         Merge Overlapping Pairs       GRID                         Extract Reads       GRID                         Merge Annotation Tracks       GRID                         Merge Variant Tracks       GRID                         Convert to Tracks       GRID                         Convert from Tracks       GRID                         Filter on Custom Criteria       GRID                         Annotate with Overlap Information       GRID                         Filter Annotations on Name       GRID                         Filter Based on Overlap       GRID                         Create GC Content Graph       GRID                         Create Mapping Graph       GRID                         Identify Graph Threshold Areas       GRID                         Update Sequence Attributes in Lists       GRID                         Split Sequence List       GRID                         Subsample Sequence List       GRID                         Rename Elements       GRID                         Rename Sequences in Lists       GRID                 Legacy Tools                                         Compare Sample Variant Tracks (legacy)       GRID                         Empirical Analysis of DGE (legacy)       GRID         "
},

    
      
      
      
      
      
    
{
  "title" : "Using Linux Command Line Interface",
  "category" : "User Guides",
  "subcategory" : "Access",
  "set" : "",
  "url" : "/guides/access/cli",
  "description" : "List of basic CLI commands",
  "date" : "Mar 06, 2023",
  "content" : "SCINet HPC resources can be used via GUI tools such as Geneious, CLC Workbench, SmartAnalysis, JupyterHub, SCINet Galaxy etc., however users are limited by what these tools provide. One can do much more ssh-ing to a cluster and using Command Line Interface (CLI). If you’re not familiar with CLI, check out the following resources:   Introduction to Unix   Unix Tutorials Below is a list of basic CLI commandsman command - Show manual for commandpwd - Output the current directory that you are inls - List the content of the current directory   ls -a - List all the content, including hidden files    ls -l - List the content and its information mkdir foldername – Create a new folder foldernamecd foldername – Change the working directory to foldername   cd - Return to $Home directory    cd .. - Go up a directory    cd - - Return to the previous directory my_quotas - reports storage usage and quota information for home and project directoriesemacs, nano, vi – Editorscp source destination – Copy source to destination   cp -r source destination – Copy a folder recursively from source to destination mv source destination - Move (or rename) a file from source to destinationrm file - Remove file   rm -r folder - Remove folder and its contents recursively cat file – Print contents of file on the screenless file - View and paginate filehead file - Show first 10 lines of filetail file - Show last 10 lines of filehistory – Show list of commands issued earlier!commandnumber – Reissue command number commandnumberwhich command – Shows the full path of commandmodule avail or module spider – Show all available software modules   module avail swname – Show all modules that have swname in the name module load modulename – Load module modulename in your environmentmodule unload modulename - Unload module modulename from your environmentmodule list – List all modules currently loaded in your environmentmodule help modulename – Show information about module modulenamemodule purge – Unload all modules from your environmentmodule use path – Add path where to search for available modulessalloc -p debug -N 1 -n 2 –mem=7G -t 2:00:00 – Request 2 cores and 7 GB of memory in debug partition for 2 hoursexit – Close session (should be used to close interactive session)sbatch jobscript – Submit jobscript into queuesqueue – Show all jobs currently running or waiting in the queuesinfo – Show available partitions (queues)scancel jobnumber – Cancel job jobnumberseff jobnumber – Get resource usage information for job jobnumber (use it for jobs that have already finished)"
},

    
      
      
      
      
      
    
{
  "title" : "User-Installed Software on Ceres with Conda",
  "category" : "User Guides",
  "subcategory" : "Software",
  "set" : "",
  "url" : "/guides/software/conda",
  "description" : "Using Conda",
  "date" : "Mar 06, 2023",
  "content" : "Conda is a software package manager for data science that allows unprivileged (non-administrative) Linux or MacOS users to search, fetch, install, upgrade, use, and manage supported open-source software packages and programming languages/libraries/environments (primarily Python and R, but also others such as Perl, Java, and Julia) in a directory they have write access to. Conda allows SCINet users to create reproducible scientific software environments (including outside of Ceres) without requiring the submission of a SCINet software request form for new software, or contacting the VRSC to upgrade existing software. Many open-source scientific software packages are available:   Browse/search all conda packages The Bioconda channel contains thousands of software packages that are useful for bioinformatics.   Browse/search available Bioconda software packages SetupBefore using conda or conda-installed software on Ceres, the miniconda environment module (which contains the conda software environment) must be loaded. To load the latest miniconda module available on Ceres:[user.name@ceres ~]$ module load miniconda You can see all available versions of miniconda on Ceres with:[user.name@ceres ~]$ module spider miniconda (Optional one-time setup for bioconda users) If you plan on installing software primarily from the bioconda channel, before using conda for the first time on Ceres, you may wish to configure conda per the bioconda documentation to search for software packages in the conda-forge, bioconda, and defaults channels (in that order):[user.name@ceres ~]$ conda config --add channels defaults [user.name@ceres ~]$ conda config --add channels bioconda [user.name@ceres ~]$ conda config --add channels conda-forge Otherwise, the conda-forge and then bioconda channels must be specified every time software is installed via conda install or conda create:conda install -c conda-forge -c bioconda SOFTWARE_PACKAGE1 SOFTWARE_PACKAGE2... Installing SoftwareSoftware can be installed into separate environments (directories) that are managed separately. At least one environment must be created before installing software using the Ceres miniconda environment module.On Ceres, suitable locations for conda environments housing conda packages include:   Home directory (default; subdirectory of $HOME/.conda/envs/) NOTE: some Conda packages (with dependencies) can take gigabytes of storage space. Use the Ceres command  my_quota  to check the available space in your home directory.   A user-specified directory within one’s project directory, e.g., /project/&lt;MY_PROJECT&gt;/&lt;MY_ENVIRONMENT_DIRECTORY&gt; This environment is then usable by others in the project.Best Practices   Use an interactive session on a compute node to install software with conda to avoid slowing down the login node for everyone, e.g,     [user.name@ceres ~]$ salloc [user.name@ceres14-compute-60 ~]$ module load miniconda [user.name@ceres14-compute-60 ~]$ source activate my_env (my_env) [user.name@ceres14-compute-60 ~]$ conda install &lt;package_name&gt; ...         Example 1: Installing Trinity into a home directoryLoad the latest miniconda module if you haven’t already and create an environment called “trinityenv”:[user.name@ceres ~]$ module load miniconda [user.name@ceres ~]$ conda create --name trinityenv Note that the  conda create command used above without the –prefix option will create the environment in your home directory ($HOME/.conda/envs/).To activate the environment (and update environment variables such as PATH that are required to use software installed into this environment):[user.name@ceres ~]$ source activate trinityenv (trinityenv) [user.name@ceres ~]$         Do not execute conda init            In conda &ge; 4.6, if you run conda activate, you will be prompted to run conda init to modify your shell interactive startup script (e.g, ~/.bashrc):        [user.name@ceres ~]$ conda activate  CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'. To initialize your shell, run  $ conda init &lt;SHELL_NAME&gt;              This will have the undesirable side effect of modifying your PATH to include that particular version of miniconda every time you log in (even without loading the miniconda environment variable).              If you accidentally run conda init, edit your shell intereractive startup file ($HOME/.bashrc for the bash shell) and remove all lines between               &gt;&gt;&gt; conda initialize &gt;&gt;&gt;               and               &lt;&lt;&lt; conda initialize &lt;&lt;&lt;                 conda deactivate may still be used safely.              We will continue to monitor the GitHub issue that describes the problem and other workarounds.                        conda activate after source activate                           If conda activate is needed for some advanced use cases like nested (stacked) environments, it can be used after source activate:            [user.name@ceres ~]$ ml miniconda [user.name@ceres ~]$ source activate (base) [user.name@ceres ~]$ conda activate samtools (samtools) [user.name@ceres ~]$                                     Now that you are inside the trinityenv environment, install software into this environment with:(trinityenv) [user.name@ceres ~]$ conda install &lt;package_name&gt; &lt;package_name&gt; &lt;package_name&gt; For example, install the Trinity transcriptome assembler and Kallisto RNA-Seq quantification application (an optional dependency that is not included with the default Trinity 2.8.4 installation). Note this step may take a few minutes:(trinityenv) [user.name@ceres ~]$ conda install trinity kallisto ... Proceed ([y]/n)? y ... Afterwards, the Trinity and Kallisto executables are in your PATH:(trinityenv) [user.name@ceres ~]$ type Trinity Trinity is hashed (/home/user.name/.conda/envs/trinityenv/bin/Trinity)  (trinityenv) [user.name@ceres ~]$ Trinity --version Trinity version: Trinity-v2.8.4-currently using the latest production release of Trinity.  (trinityenv) [user.name@ceres ~]$ type kallisto kallisto is hashed (/home/scinet.username/.conda/envs/trinityenv/bin/kallisto)  (trinityenv) [user.name@ceres ~]$ kallisto version kallisto, version 0.44. To exit the environment:(trinityenv) [user.name@ceres ~]$ conda deactivate [user.name@ceres ~]$  After deactivating the trinityenv environment, Trinity and kallisto are no longer in your PATH:[user.name@ceres ~]$ type Trinity -bash: type: Trinity: not found Example 2: Installing Tensorflow into a /project directoryLoad the latest miniconda module if you haven’t already and create an environment in your /project directory by using the option  --prefix:[user.name@ceres ~]$ module load miniconda [user.name@ceres ~]$ conda create --prefix /project/my_proj/tensorflow ... [user.name@ceres ~]$ source activate /project/my_proj/tensorflow (/project/my_proj/tensorflow) [user.name@ceres ~]$ conda install tensorflow ...  Note: conda first downloads packages into a package cache directory. By default, the package cache is in your home directory  ($HOME/.conda/pkgs). If installing a large amount of software that may cause home directory quota to be exceeded, you can configure another directory to be the package cache by adding a pkgs_dirs list to the $HOME/.condarc file (YAML); e.g.:pkgs_dirs:- /project/my_proj/my_pkg_cacheManaging EnvironmentsSee the official Conda documentation for managing environments for a complete list of commands.To list environments that have been created in your home directory:[user.name@ceres ~]$ conda env list ## conda environments: # trinityenv               /home/user.name/.conda/envs/trinityenv root                  *  /software/7/apps/miniconda/4.7.12 To list software packages in an environment:[user.name@ceres ~]$ conda list --name trinityenv ## packages in environment at /home/user.name/.conda/envs/trinityenv: # ... OR[user.name@ceres ~]$ conda list --prefix /project/my_proj/tensorflow ## packages in environment at /project/my_proj/tensorflow: ... Tip: For reproducibility, a list of all packages/versions in an environment can be exported to an environment file, which can be used to recreate the environment (e.g., by another user, or on another system) or archived with analysis results. This makes it easy for you or anyone else to re-run your analysis on any system and is also a record of the exact software environment you used for your analysis.To remove an environment in your home directory:[user.name@ceres ~]$ conda env remove --name trinityenv To remove an environment in your /project directory:rm -rf /project/my_proj/tensorflow To remove packages not used by any environment, as well as tarballs downloaded into the conda package cache ($HOME/.conda/pkgs):conda clean --all "
},

    
      
      
      
      
      
    
{
  "title" : "Data and Storage SOP",
  "category" : "User Guides",
  "subcategory" : "Data",
  "set" : "",
  "url" : "/guides/data/data-management",
  "description" : "Suggested best practices for managing data on SCINet infrastructure",
  "date" : "Mar 06, 2023",
  "content" : "This document describes recommended procedures (SOP) for managing data on ARS HPC and storage infrastructure. The key concept is to only use “Tier 1 storage” (that is, storage that is local to a given HPC cluster) as required for actively running compute jobs. Tier 1 storage is not backed up and should not be used for archival purposes. Juno, on the other hand, is periodically backed up to tape and can be used for long-term storage of data and results.  We recommend immediate adoption of the procedures described here for new projects. These procedures should also be followed for “old” projects after first moving extant project data in Tier 1 storage to Juno using one of the procedures described below.  If you would like more detailed information about the data storage options provided by SCINet and how to use them, please see the SCINet Storage Guide. Figure 1. Recommended procedures for managing data on ARS HPC infrastructure using Globus.   Move data to Juno   Copy data to target HPC (Ceres or Atlas)   Run compute tasks   Copy Results to Juno   Copy results to local machine, if desired Definitions   Juno: Large, multi-petabyte ARS storage device at the National Agricultural Library in Maryland, accessed by users; periodically backed up to tape device.  Includes periodic file system snapshots that allow users to recover accidentally deleted files.   Tape backup: Off-site backup of Juno, located at Mississippi State University, accessible by VRSC staff for disaster recovery following major system data loss.   Tier 1 Storage: Storage on either of the HPC clusters, local to computing resources at the National Centers for Animal Health in Ames, Iowa or Mississippi State University. These locations are for storing code, data, and intermediate results while performing a series of computational jobs. This storage is not backed up. Two storage locations are available on each HPC cluster, /project and /90daydata. Historically, space had routinely been allocated in /project, but going forward, space in /project will only be allocated on an as-needed basis. Most users should use /90daydata for routine Tier 1 storage. Detailed instructions, using Globus (preferred)Using Globus is recommended for file transfer performance and reliability. The recommended data management workflow using Globus is illustrated in Figure 1, above.See the SCINet Globus Data Transfer guide for instructions on how to use Globus.   Copy raw data and custom software (e.g., custom scripts, ideally also versioned in Git repositories; conda environments/environment.yml, Singularity image/definition files) from a local workstation or other data source to a project directory on Juno using Globus (search for Globus collection “NAL DTN 0”) (Figure 1.1).  On Juno, the data should be copied to /LTS/project/PROJNAME/.   If not already there, copy data to Tier 1 storage on the cluster you will run on, either Ceres or Atlas, using Globus.  For most projects, you should use /90dayadata for routine, short-term storage (Figure 1.2).  There are no storage quotas on /90daydata, so it is ideal for data-intensive work.  For projects that require longer-term use of Tier 1 storage, space can be requested in /project.  However, please note that no data on Tier 1 storage is backed up.   Use ssh to log in to your chosen HPC cluster (Ceres or Atlas; see Logging in to SCINet); (e.g.:     ssh FIRST.LAST@ceres.scinet.usda.gov          ) and run your compute job (Figure 1.3).      Copy any results you need to keep to Juno so that they are preserved (Figure 1.4).   If desired, you can also copy results to your local workstation (Figure 1.5).  Note that step 1 might not be necessary for all data used for a project. For example, suppose you need to use a large, publicly available satellite-derived dataset for a project. In this case, the data probably do not need to be stored on Juno and can simply be placed directly in /90daydata on Tier 1 storage and deleted when they are no longer needed. Alternative instructions, not using GlobusIt is also possible to move data on SCINet infrastructure using traditional command-line tools like scp and rsync.  See the SCINet File Transfer guide for instructions on how to use these commands to transfer data to/from Ceres and Atlas cluster.To move data to/from Juno, login to ceres-dtn and issue scp or rsync command to nal-dtn, e.g.:ssh ceres-dtn rsync -avz --no-p --no-g ttt nal-dtn.scinet.usda.gov:/LTS/project/&lt;project_name&gt;/ "
},

    
      
      
      
      
      
    
{
  "title" : "SCINet File Transfer",
  "category" : "User Guides",
  "subcategory" : "Data",
  "set" : "",
  "url" : "/guides/data/datatransfer",
  "description" : "Best practices for file transfer",
  "date" : "Mar 06, 2023",
  "content" : "Best Practices   Globus Online is the recommended method for transferring data to and from the Ceres cluster. It provides faster data transfer speeds compared to scp, has a graphical interface, and does not require a GA verification code for every file transfer.   Given the space and access limitations of a home directory, large amounts of data or data that will be used collaboratively should be transferred to a project directory. See the User Manual section Quotas on Home and Project Directories for more information on home and project directory quotas.   If you have to transfer very large amounts of data or if network speed at your location is slow, please submit a request to the Virtual Research Support Core (VRSC) to ingress data from a hard drive as described below (section Large Data Transfer by Shipping Hard Drives).   If you have issues with transferring data, please contact the VRSC at scinet_vrsc@USDA.GOV. Globus Data TransferGlobus Online is the recommended method for transferring data to and from the Ceres cluster. It provides faster data transfer speeds compared to scp, has a graphical interface, and does not require a GA verification code for every file transfer. To transfer data to/from a local computer, users will need to install Globus Connect Personal which does NOT require admin privileges.The instructional video at https://www.youtube.com/watch?v=I3lnsCAfx3Q demonstrates how to transfer files between local computer, Ceres, Atlas and Juno using Globus.Login   In a browser, navigate to https://www.globus.org/, click ‘Log In’ in the upper right corner.   There are several ways to login to the Globus Web App. Login using one of the available methods:            ORCiD iD login (recommended). You can use your ORCiD, a persistent digital identifier for individual researchers, to sign into Globus (all ARS researchers should have an ORCiD). Click on “Sign in with ORCiD iD” at the bottom of the page. If you don’t yet have an ORCiD, please visit https://orcid.org/ to obtain one.       Globus ID login. You can create an account on Globus ID and use it to log into Globus. See the link to Globus ID under the list of organizational logins.       Organizational login. Many organizations, mostly universities, have organizational logins. ARS currently does not provide an organizational login. If your organization provides an organizational login, select your organization from the list and login with your organization credentials at the familiar login page.       Google account login. Click on “Sign in with Google” at the bottom of the page if you want to use your google account.           You should now be on the Globus ‘File Manager’ page. You should see two panels. If you don’t, click on the 2-panels icon at the top right of the screen. You should now see two panels: Copying Data   The two panes represent the two systems that you want to copy data between. You will need to select a Collection (an Endpoint) for each pane and most likely, authenticate.   Click in the ‘Collection’ box on one of the panes. A selection window will appear. Recent collections will be listed under recent tab. Click on other tabs to see collections that you’ve bookmarked, created or got from other people. In the Recent tab you will also see “Install Globus Connect Personal” button. See below how to install Globus Connect Personal on your computer to transfer data to/from this computer.   If you see your desired collection, click it and continue. If you do not see your desired collection start typing the name in the box at the top. When your collection appears, select it.   If authentication is required for the selected collection, required fields or other instructions will appear.   Type or select “Ceres DTN” for Ceres DTN, “msuhpc2#Atlas-dtn” for Atlas DTN or “NAL DTN 0” for Juno. You will be prompted to login.   Username is your Ceres username. Your password is your Ceres password PLUS the six digits generated by Google Authenticator for access to the Ceres cluster. You would use ‘Password123456’ for the password field where ‘123456’ would be the current code from Google Authenticator and ‘Password’ would be your normal SCINet password.   To transfer data between Ceres and Atlas clusters, log into Ceres DTN in one pane and to Atlas DTN in the other pane. Similarly, to transfer data to/from Juno, log into NAL TN 0 in one of the panels.   Once you have logged in on both panes you will need to navigate to the data you want to copy in one pane (source pane) and navigate to the location you want it copied to on the other pane (destination pane). You can either click on directory names or type the full path in the Path box (under the Collection box).   Before you start your copy, look at the Transfer &amp; Sync Options at the bottom of the page (click on arrow to see available options). To see a short description of each option click on “i” next to the option.   To start the transfer click the blue box with the arrow at the bottom of the page pointing in the direction of the destination pane.   You may now click ‘Activity’ at the left to see a list of current and prior transfers. Click on “File Manager” at the top left of the page to get back to the main page. Globus Connect PersonalTo transfer files to your personal computer you may use Globus Connect Personal. A link “Install Globus Connect Personal” can be found in the Recent tab and in the “More Options” tab in the list of Collections. Click on “Collection” box to get to the list.The installation instructions for Globus Connect Personal are available here:   Windows Install Instructions   Mac Install Instructions   Linux Install Instructions By default, Globus Connect Personal prompts to be installed in C:/Program Files which requires administrator rights. However you don’t need Administrator rights to install Globus Connect Personal on your local machine. If you do not have Administrator rights browse to a place you have write access to (e.g. your Desktop folder) or contact your local IT staff for assistance.Small Data Transfer Using scp and rsyncscp is usually available on any Linux or MacOS machine, and on Microsoft Windows 10 (in PowerShell).  It’s best used when you need to transfer a single file.Below are examples of scp commands to be issued on your local machine. In these examples   &lt;local_path_to_file/&gt; can be omitted, in this case current directory on your local machine will be used   &lt;remote_path_to_file/&gt; can be omitted, in this case home directory on Ceres or Atlas will be used   dest.ext can be omitted, in this case the name of the file being transferred will be used. Transfer To Ceres:scp &lt;local_path_to_file/&gt;file.ext  &lt;SCINetID&gt;@ceres-dtn.scinet.usda.gov:&lt;remote_path_to_file/&gt;dest.ext Transfer To Atlas:scp &lt;local_path_to_file/&gt;file.ext &lt;SCINetID&gt;@atlas-dtn.hpc.msstate.edu:&lt;remote_path_to_file/&gt;dest.ext Transfer From Ceres:scp &lt;SCINetID&gt;@ceres-dtn.scinet.usda.gov:&lt;remote_path_to_file/&gt;file.ext  &lt;local_path_to_file/&gt;dest.ext Transfer From Atlas:scp &lt;SCINetID&gt;@atlas-dtn.hpc.msstate.edu:&lt;remote_path_to_file/&gt;file.ext  &lt;local_path_to_file/&gt;dest.ext It is not advised to use “scp -r” command to transfer directories to Ceres, since the setgid bit on directories at destination is not inherited.  This is not a problem if directories are copied to /home/$USER but is a problem when copying to /project area and usually results in quota exceeded errors.If you decide to use scp to transfer directories to /project, you will have to manually set a setgid bit on the directory and all subdirectories after the transfer using “chmod g+s &lt;dir_name&gt;” command. The following command will set ownership of the files in a directory in /project to the project group and set the setgid bit:find /project/&lt;project_name&gt;/&lt;dir&gt; -exec chgrp proj-&lt;project_name&gt; {} + -a -type d -exec chmod g+s {} +  To learn more about scp command and all available options issue “man scp”.Instead of scp one can use rsync command for bulk transfers. rsync synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transferred. It is installed by default on macOS and is available on many Linux hosts. The following command will recursively transfer all new and updated files in the directory &lt;dir_name&gt; on the local machine into directory /project/&lt;project_name&gt;/&lt;dir_name&gt; on Ceres:rsync -avz --no-p --no-g &lt;dir_name&gt; &lt;SCINetID&gt;@ceres-dtn.scinet.usda.gov:/project/&lt;project_name&gt; To learn more about rsync command and all available options issue “man rsync”.Large Data Transfer by Shipping Hard DrivesLarge data transfers will be facilitated by the VRSC and involves users shipping hard disk drives (not thumb drives) with their data on it to the VRSC in Ames, Iowa.  The VRSC will then upload the data directly and put it in a project directory specified by the user.You can send hard drives containing data to the VRSC if you have very large amounts of data (typically greater than 50GB) to transfer to Ceres or if the network speed at your location is slow. Please follow these instructions:        Submit an email request to the VRSC scinet_vrsc@USDA.GOV for a data transfer with the following information:             Amount of data       Target project directory.       Type of filesystem the data is coming from (Window, Mac, Linux)           If you don’t already have a project directory please request one first: Request Project Storage (eAuthentication required)           Copy the data onto a SATA hard drive or SSD             You will be responsible for purchasing your own drive(s)       Any type of hard drive (not a USB drive) is fine but SSDs will be more tolerant of the postal system       Disks must be EXT4, NTFS, HFS, XFS, or FAT formatted                Ship the disk to the following address and email the tracking information to scinet_vrsc@USDA.GOV. Include a print out of your email containing the data transfer request to VRSC in your package. Send to:      Nathan Humeston  74 Durham  613 Morrill Rd  Ames, IA 50011-2100           Once we receive the data we will copy it over to the appropriate project directory and notify you once it is complete.           Please include a prepaid return shipping label so that we can send the drive(s) back to you after the data transfer is complete. Otherwise the drive(s) will not be returned.    Other Ways to Transfer DataOther programs that have a GUI to transfer data and are suitable for smaller file transfers are:        Cyberduck - https://cyberduck.io/           FileZilla - https://filezilla-project.org/    Cyberduck supports multiple protocols (including Amazon S3, iRODS, and Google Drive) and is more secure than FileZilla.FileZillaFilezilla is being made available on SCINet cafe machines. FileZilla uses a dual pane approach to moving files with the left pane being the local system and the right pane being remote. Users will need to setup remote sites using the ‘Site Manager’ found under the File menu.Once in ‘Site Manager’, do the following:   click ‘New Site’   give the site a name   select SFTP protocol   enter a hostname for one of the DTNs   choose Interactive for Logon Type   enter your SCINet username(not an email)   click Connect to test your connection If successful, you should be able to drag and drop files and folders between systems, if permissions and quota allow. You may also right click a file or directory and select Upload or Download depending on which pane you are in.Note for Linux users, external drives are mounted under /run/media/firstname.lastname .Data Transfer to NCBITo transfer data to/from NCBI, ssh to a DTN node and use either ncftp commands (ncftpput, ncftpget) or Aspera command (ascp). Since ftp connections can be unstable we recommend using Aspera for large file transfers.To use Aspera, you will need a private key file that you can acquire by following instructions listed on Page 3 of the document at https://www.ncbi.nlm.nih.gov/books/NBK242625"
},

    
      
      
      
      
      
    
{
  "title" : "EC2 Instance and Pathway Tools",
  "category" : "User Guides",
  "subcategory" : "AWS",
  "set" : "",
  "url" : "/guides/aws/ec2",
  "description" : "AWS Tools",
  "date" : "Mar 06, 2023",
  "content" : "EC2 InstanceAWS ConsoleYou must be using the  US East (N. Virginia) region for all AWS activities. Please check your currently selected region before following this document’s instructions. To check and update your region, click on the dropdown in the upper right of the console window, just next to “Support”. Select US East.(N. Virginia)In the main Console page, click on “Services” in the top menu then “EC2” within the Compute section, or type “EC2” into the search bar. Step 1: Create Key PairClick on the “Key Pairs” link in the left menu or under Resources. Then click the Create Key Pair button. Click the Create button. Your browser will download the private key contents.Save your private key somewhere for future use. This key will enable you to log in to your EC2 instance. You only need to create a KeyPair once for multiple EC2 Instances.Step 2: Launch the EC2 Instance productUse the top menu to navigate to Services &gt; Service Catalog. Click the three-dots button on the “EC2 Instance” product card and choose Launch Product, or click on the product name and click Launch Product on the following screen. Enter a name for the product and select a version from the list of versions. Step 3: Product parametersFill in the parameters as made available by the product template definition. Click next.Some parameters may have limits or requirements. If you receive a warning message saying ‘abcde must equal the value of “Parameter”’, copy and paste the value on the left-hand side into the parameter field to proceed through the warningStep 4: TagOptionsYou may provide optional tags to help you locate and track your resources. For most users, the default tags are sufficient. Click Next.Steps 5-6: Notifications and ReviewContinue through the Notifications screen by clicking Next. Finally, click Launch on the last page to launch the product. Provisioned Product DetailsAfter initially launch, the product details page will show the product as “Under change” with status “In progress”. The page will automaticallyupdate (or you can refresh the page if desired). When complete, the product will change to “Available” with status “Succeeded”Connecting to an InstanceWhen the EC2 Instance product is “Available”. Copy the PublicDNS from the Outputs table. This is the hostname you will use to connect to yourinstance. You must provide the private key that you downloaded from EC2 in Step 1. The log in username is ec2-user.ssh -i /path/to/keypair.pem ec2-user@&amp;lt;PublicDNS&amp;gt; If you are using PuTTY on Windows, the private key (.pem) will have to be converted using PuTTYgen.Terminate an InstanceView the list of provisioned products (resources previously launched) by clicking “Provisioned products list” in the Service Catalog console menu(the three bars icon in the upper-left). Click “Terminate” to confirm termination of the product.Pathway ToolsThe product follows the same general process as the EC2 Instance. For full documentation on using Pathway Tools see the Pathway Tools Software documentation.Connecting to Pathway ToolsWhen the EC2 Instance product is “Available”. Click on the WebsiteURL in the Outputs table. This is the address you will use to connect to your instance.Starting Pathway ToolsTo start or restart the pathways tools service use systemd commands.sudo systemctl start pathway-tools.service sudo systemctl status pathway-tools.service "
},

    
      
      
      
      
      
    
{
  "title" : "External computing resources",
  "category" : "User Guides",
  "subcategory" : "Resources",
  "set" : "",
  "url" : "/guides/resources/external-computing-resources",
  "description" : "External computing resources",
  "date" : "Mar 06, 2023",
  "content" : "In addition to the Ceres and Atlas clusters, there are external computing resources available to the SCINet community, including Amazon Web Services, XSEDE, and the Open Science Grid. These resources may be of interest to SCINet users that require:   very large jobs (either numerous small jobs, or many nodes in parallel)   special computing hardware requirements (e.g., GPUs, Xeon Phi, extremely-large memory)   software that isn’t supported on Ceres (e.g., web apps, relational databases, VMs, Hadoop, Spark, certain commercial software)  A brief synopsis of some resources: Amazon Web Services   What: large ecosystem of Amazon cloud services: EC2 compute instances, S3 object storage, etc. (https://aws.amazon.com/)   How much: Budget (paid for by SCINet) determined at time of request   Support: SCINet VRSC; Basecamp group for SCINet AWS users   How to request: Submit a SCINet AWS Project Request   More Information: Find more information about AWS in our AWS Resources. XSEDE   What: HPC compute, storage, cloud, and support from several large academic computing centers in the United States. Most systems look &amp; operate similarly Ceres (SSH access, SLURM job scheduler, shared file systems, etc.)   How much: Allocation amount determined by XSEDE committee. No cost to researchers in the United States.   How to request:            Create an XSEDE account at https://portal.xsede.org/       Apply for one of the following allocations:                 Startup Allocation                    Some justification required; typically &lt; 1 week time to review/approval                       Research Allocation                    Larger allocation requiring more justification required; evaluated quarterly.                       Contact your institution’s XSEDE Campus Champion (ARS: Nathan Weeks) to be added to their Campus Champions allocation, which covers most XSEDE systems.                    No justification necessary; meant for trying XSEDE resources before applying for an allocation                            Transfer data between SCINet and XSEDE systems via Globus (or scp/sftp).                                                   Support: XSEDE Help Desk   See also: Bioinformatics Workbook Introduction to XSEDE Open Science Grid   What: Excess computing capacity aggregated from &gt; 100 U.S. institutional research computing systems for High-Throughput Computing jobs.   How much: Unlimited (subject to fair-share policy); no cost to researchers in U.S.   How to request:            Sign up via OSG Connect (http://osgconnect.net/) with your Globus ID (enter “USDAARS” for organization, if appropriate)                 Access typically granted within days       Can also/instead access OSG via XSEDE, subject to XSEDE allocation policies and limits     2. Request to be added to the Open Science Grid Users Basecamp project (ping / email Nathan Weeks, or respond in the comments).       You will also be added to a “SCINet” OSG project.           Transfer data between SCINet and OSG Connect via Globus (or scp/sftp).            If there is sufficient Open Science Grid interest / usage in the SCINet community, the VRSC may explore tighter integration, allowing direct “bursting” to OSG from Ceres, rather than logging into an OSG Connect submit host.         "
},

    
      
      
      
      
      
    
{
  "title" : "Galaxy on SCINet",
  "category" : "User Guides",
  "subcategory" : "Analysis",
  "set" : "",
  "url" : "/guides/analysis/galaxy",
  "description" : "Using Galaxy",
  "date" : "Mar 06, 2023",
  "content" : "IntroductionFor support contact: scinet_vrsc@usda.govFirst, a quick note on the terms “Ceres” and “SCINet”: “SCINet” is the network through which you access the high-performance computer named “Ceres”. Generally, we will refer to Ceres accounts in this tutorial because most people think of having an account on a computer, but the account is, more broadly, a network account on SCINet.There are a few compelling reasons why we think you’re going to like using Galaxy on Ceres:   It is a great bioinformatics alternative for those who prefer a graphical user interface.   You don’t have to worry with defining queues and number of nodes in a batch command file. We have tried hard to establish appropriate default parameters to take advantage of Ceres’s parallel processing power.   FTP transfer is tightly integrated in the Ceres Galaxy framework, so file transfer is intuitive, and you don’t run into data limit bottlenecks as quickly.   Galaxy makes it easy to share your analysis with bioinformatic support should the need arise. In addition, you can share with collaborators assuming that they have a Ceres account.   There is plenty of external documentation covering almost all conventional bioinformatic analyses. In fact, many workflows (see below) probably already exist that you can use directly on your data.   If you don’t see a tool in the current interface, it probably is in the Toolshed (see below). You can then request that an administrator install it. In order to use SCINet Galaxy, you need an account on Ceres. Sign up for a SCINet account to start that process. New accounts will automatically get a Galaxy user name and directory. The user name should match the email you used when registering for your Ceres account and the password will match your Ceres password.NOTE: All uploaded and output files are removed after 90 Days so it is recommended users save their data outside of galaxy.Logging OnCeres Galaxy is public facing, so to access it you just type or paste “https://galaxy.scinet.usda.gov” into the address bar of a web browser.  We recommend Firefox, Chrome, or Safari. MAKE SURE TO TYPE THE “https://” prefix and note the “s”.You will be presented with the following login screen, with boxes on the right to enter your username and password:Your user name should match the full email address you used when registering for your Ceres account and the password should match your Ceres password followed by the Google Authenticator code (Example: password123456). After logging in, the standard Galaxy home screen should appear:The “Tools” bar in the left window frame is where you can load, manipulate, and analyze data. The central window frame is where you will see options and parameters programs that you will be running. The right “History” frame shows all the imported files and programs you have run.  In order to begin, you need to upload data. Often data of general interest, such a genome sequences or annotations, are already available in Galaxy under the “Get Data” tab. More commonly, you will be using data that you have generated.You will import files into Galaxy by clicking on the “Tools” sub-heading “Get Data” and then “Upload File” under “Get Data”.  The following download/upload screen should appear:Galaxy offers a method to import data directly from your computer as “Choose local file” button. Feel free to experiment with this direct method, but we find it can be slower and fickler than FTP transfer. To that end, we will mainly focus on uploading files using a file transfer program, such as Globus or Filezilla. If you click on “Choose FTP file”, Galaxy will look in your FTP upload folder on Ceres (/galaxy/upload/ceres_email_ID) for files you have uploaded, so you must first upload your data to that folder.Import data to Ceres GalaxyYou need to move or copy your data files to the Galaxy folder in your home directory on Ceres to make them “visible” to Galaxy. Then, you will be able to upload/import the files using Galaxy. Remember, the default quota in SCINet galaxy is only 100 GB. Please contact us to increase your default quota.We recommend using Globus to move files to your Galaxy folder on Ceres. For instructions please see the Transferring Files Guide.NOTE: If you already have data on Ceres, simply copy files into your Galaxy folder on Ceres either via command-line (cp command) or by using a Filezilla-like tool. These files will appear as if you had uploaded them via FTP and can be imported using the approach described below.IMPORTANT NOTE: Make sure when transfering files to consider how the files are being transfered, i.e. using cp  or mv. Moving (mv) instead of copying(cp) will REMOVE  the original files so it is recommended to use cp  or review how you want your files to be handled before moving.Once your data files are in your Galaxy folder in your home directory on Ceres, go back to the “Upload File” screen and click “Choose FTP site”. You will see the download/upload screen with a list of your files:Select the file you want to import and hit “Start”.NOTE: Importing the data into galaxy will REMOVE  the file that you moved via FTP. NOTE: Uploading directly from disk is slower than the other methods of tranfering.You can close this window and the file will appear in the History pane on the right of your home screen:This is a generic way to import files and can be used regardless of file format.  You are now ready to process your data.Using GalaxyWe have tried to focus on aspects of using Galaxy that are specific to USDA-ARS’s Ceres installation. There are hundreds of tutorials and videos to introduce you to the Galaxy framework, which is essentially the same no matter where it is installed. We suggest that you start at https://galaxyproject.org/learn/. A nice interactive introduction is also available at Help &gt; Interactive Tours, or https://galaxy.scinet.usda.gov/tours.A common initial hang-up is getting your uploaded data in the right format. For sequencing data, you usually need to make clear what quality-scale you are using. For illumina reads, review the following link: https://galaxyproject.org/support/fastqsanger/. In addition, it is common for sample data from sequencing centers to be spread across multiple lanes. If you only have a few samples, you can concatenate these in Galaxy using Concatenate – Text Manipulation tool. If you have many samples, it will probably be best to concatenate those files prior to uploading using the commandline function cat.For example, for files ‘samp1_L1_R1.fq’, ‘samp1_L2_R1.fq’ use[jane.user@sn-cn-18-0 ~]$ cat *R1.fq &gt;sample1_R1.fq Also note, .gz compressed files can be concatenated using the same approach without having to uncompress them first.You will usually want to structure your data into Collections for batch processing and downstream analysis. See https://galaxyproject.org/tutorials/collections/ or, for a worked example, https://depot.galaxyproject.org/hub/attachments/documents/presentations/gcc2014/Chilton.pdf. Alternatively, most tools will allow you to run the same process on multiple datasets of the same format without combining them as Collections.Some links to common analyses are given below. Many of these analyses already exist as published workflows and can be used directly. See https://usegalaxy.org/workflow/list_published for a searchable list. To use, download the workflow of interest, click on the “Workflow” tab at the top of your main screen, and then import the workflow according to the instructions. An example of usage is available here: http://sepsis-omics.github.io/tutorials/modules/workflows/. For those wanting to develop their own worflows, a graphic editor is available, as described here: https://galaxyproject.org/tutorials/g101/#opening-workflow-editor.RNA-seq – general overview (https://galaxyproject.org/tutorials/rb_rnaseq/) and galaxy specific pipeline (https://galaxyproject.org/tutorials/nt_rnaseq). Also check out https://sites.google.com/site/princetonhtseq/tutorials/rna-seq.SNP-calling – https://galaxyproject.org/tutorials/var_dip/ for diploid genomes and https://galaxyproject.org/tutorials/var_hap/ for haploid genomes.IMPORTANT: If you receive the job error “This job was terminated because it ran longer than the maximum allowed job run time” it means that the tool has not been appropriately configured to run on Ceres using the scale of data that you have provided. Please contact scinet_vrsc@usda.gov and we can optimize these parameters for you and for future users.Sharing Your Data and AnalysisIf you need some consultation on your results or on parameter settings, it can be very useful to share your analysis with someone so that you can both be viewing the same thing. To that end, we will give this aspect special emphasis. You can see a short introduction to this option at https://galaxyproject.org/learn/share.Briefly, if you are in the appropriate history, follow the clicks on the History pane indicated in red (select Settings gear icon, then select “Share or Publish” on the settings menu):You will see the “Share or Publsish History Galaxy_Walkthru” screem:You can manually enter the user name you wish to share with in the drop-down space. Be careful that the user-name is exact and appropriate capitalization is observed.After you have gone through these steps, the person you have shared your history with will need to go to the same gear icon used above but instead click on “Histories Shared With Me”. The shared history should be present on the resultant interface.Can’t Find a Tool You Need?We have loaded a core subset of all tools that are available on the public Galaxy server (https://usegalaxy.org). If you do not see a tool you need and cannot use a good alternative, you should search in the Toolshed (https://toolshed.g2.bx.psu.edu). After identifying the relevant tool, email a Ceres Galaxy administrator: scinet_vrsc@usda.gov. If there are multiple options, we encourage the use of tools developed by either “devteam” or “iuc”. Depending on the nature of the tool you are requesting, we may ask you to supply a sample dataset for testing purposes, so anticipate having that data available.Additionally, there is an option for galaxy install as well on the application to request software installation by the VRSC. Select the Galaxy checkbox in the “Type of Application” section:"
},

    
      
      
      
      
      
    
{
  "title" : "Geneious Software Guide",
  "category" : "User Guides",
  "subcategory" : "Analysis",
  "set" : "",
  "url" : "/guides/analysis/geneious",
  "description" : "Geneious Software Use Guide",
  "date" : "Mar 06, 2023",
  "content" : "Ceres no longer has Geneious Server. The developer has discontinued that product.Ceres does provide floating licenses for Geneious Prime which is the desktop client.Caveats:   The floating license server will only work at USDA sites or via VPN due to firewall restrictions.   The Geneious Prime license server should work through both the regular USDA VPN and the SCINet ocvpn vpn servers.   If you do encounter license server issues let us know at scinet_vrsc@usda.gov First download the Geneious client from https://www.geneious.com/download/Install it as usual and start it up.Geneious will complain about not having a license. Click “Activate a License”On the “Enter Your License Details” screen,   select “Use floating license server”,   enter geneious.scinet.usda.gov in the “License Server” box, and   enter 27001 in the “Port” box. Geneious Prime can also be accessed via OpenOnDemand which will allow you to run Geneious Prime directly on a cluster node.  "
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Nomenclature",
  "category" : "User Guides",
  "subcategory" : "Access",
  "set" : "",
  "url" : "/guides/access/glossary",
  "description" : "A quick overview of some of the software, hardware, and confusing nomenclature",
  "date" : "Mar 06, 2023",
  "content" : "The software discussed and shown in these user guides is largely open source, can run on a desktop, HPC, or cloud environment, and can be installed with software management systems that support reproducibility (such as Conda, Singularity, and Docker). Below is a quick overview of some of the software, hardware, and confusing nomenclature that is used throughout this site.SCINet vs. Ceres vs. AtlasSCINet is the USDA ARS Scientific Computing INitiative that aims to improve access to high performance and cloud computing, improve networking to facilitate high speed data transfer, and facilitate scientific computational training. The Virtual Research Support Core (VRSC)Ceres is one of the HPC systems (located in Iowa) connected to the SCINet infrastucture. The system is largely maintained by staff at Iowa State University.Atlas is the more recently added HPC systemFor more information on the HPC systems that SCINet offers, see the Computer Systems page.Open OnDemandJupyter is an open-source, non-profit project to support interactive data science and scientific computing. Jupyter is language agnostic with support for &gt;130 different scientific programing language kernels. This workshop will use the following two applications from the Jupyter software stack:   Open OnDemand: A software to serve JupyterLab to multiple users (this is how we will launch an instance of JupyterLab on Ceres without having to to SSH into the cluster). Documention at: https://jupyterhub.readthedocs.io/en/stable/   JupyterLab: A web-based interactive development environment. Documentation at: https://jupyterlab.readthedocs.io/en/stable/ SLURMSLURM (Simple Linux Utility for Resource Management) is the workload manager used on the Ceres and Atlas HPC system to allocate computational resources. From the SLURM documentation, SLURM is “an open source… cluster management and job scheduling system for large and small Linux clusters. As a cluster workload manager, SLURM has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.”Scientific Coding Languages - Python and RPython and R appear to be the most common scientific programming languages used across ARS. However, many of these examples could also be run in other programming languages such as Julia, Go, IDL/ENVI, etc."
},

    
      
      
      
      
      
    
{
  "title" : "Logging in to SCINet",
  "category" : "User Guides",
  "subcategory" : "Access",
  "set" : "",
  "url" : "/guides/access/login",
  "description" : "How to access SCINet",
  "date" : "Mar 06, 2023",
  "content" : "No account? Signup here.All users should have received their login credentials in an email.  If you have not, please email the Virtual Research Support Core at scinet_vrsc@USDA.GOV.Before accessing various SCINet resources, new users need to ssh either to Ceres or Atlas cluster and change the temporary password. Note that home directories on Atlas are not created right away, so it is recommended to wait a day after receiving email with the credentials before logging to Atlas cluster.Using ssh to ConnectFrom Windows 10Windows 10 that is up to date has an ssh client in the Windows Power Shell. To use that client, click on the Start button and start typing “power”. Select Windows PowerShell from the list.In the PowerShell window, you can type one of two options:1) ssh user.name@ceres.scinet.usda.gov (for Ceres Connections) 2) ssh user.name@atlas-login.hpc.msstate.edu (for Atlas Connections) NOTE: Be sure to replace “user.name” in both options above with your assigned username.It is also recommended to create a config file on your computer. You may do so using Notepad.  The file you create must be titled “config” with no extension for this method to work properly (i.e. “config” not “config.txt”).  Note: Do not copy the code into the terminal itself, it must be in a separate file.Create a ~/.ssh/config file replacing USER.NAME with your actual username, all in lowercase.  To save this config file to your .ssh folder, you must save it to your user folder.  The path to the .ssh file is as follows: C&gt;Users&gt;(Your Account)&gt;.sshHost ceres-login HostName ceres.scinet.usda.gov User USER.NAME TCPKeepAlive yes ServerAliveInterval 20 ServerAliveCountMax 30  Host atlas-login HostName atlas-login.hpc.msstate.edu User USER.NAME TCPKeepAlive yes ServerAliveInterval 20 ServerAliveCountMax 30 If you don’t want to use the config file method above, add the following title to the ssh command replacing USER.NAME with your actual username, all in lowercase.ssh -o TCPKeepAlive=yes -o ServerAliveInterval=20 -o ServerAliveCountMax=30 USER.NAME@ceres.scinet.usda.gov andssh -o TCPKeepAlive=yes -o ServerAliveInterval=20 -o ServerAliveCountMax=30 USER.NAME@atlas-login.hpc.msstate.edu After you have followed either of the options above, enter the 6-digit “Verification Code” generated by the Google Authenticator app when prompted. Note that the code and your password will not be visible on the screen when typed. If the code is correct, the system will then prompt you for your password.Please Note: If you are connected to the SCINet VPN, you will not be prompted to enter the Google Authenticator Code. If you make a mistake entering the code, you will be prompted to enter the verification code once more, but you must wait for a new code to be generated.If your password has expired (new temporary passwords expire right away, and the passwords set by users expire after 60 days) you will be prompted to change your password. To paste from a clipboard into PowerShell, perform single right click with the mouse and then press Enter. Please see below for a detailed guide on changing your password.When Changing Your Temporary Password (Mandatory Prompt When First Connecting):   Enter either of the following:  A. $ ssh user.name@ceres.scinet.usda.gov  B. ssh user.name@atlas-login.hpc.msstate.edu   Enter the 6-digit verification code generated by the Google Authenticator app when prompted.   Enter your temporary password found in your welcome email when prompted for the password.   When prompted for your “Current Password”, re-enter your temporary password.   Enter your “new password” twice when prompted.   Note: The new password requirements are listed below the Mac and Linux instructions. After updating your password, you will be prompted to answer Yes or No to entering a cell phone. It is strongly recommended to add a cell phone just in case you need to reset your Google Authenticator in the future. Enter “y” and then enter your cellphone number with no dashes or parenthesis, as shown in the image below.From Older Windows VersionsTo begin, download Putty.exe.  Once downloaded, start PuTTY and use the following instructions:   In the left-hand menu select the ‘Session’ category, then on the right side type into the ‘Host Name’ either ceres.scinet.usda.gov (for Ceres connections) or atlas-login.hpc.msstate.edu (for Atlas connections).   In the left-hand menu select the ‘Connection’ category, then on the right side replace 0 with 60 for “Seconds between keepalives” and check the “Enable TCP keepalives”   In the left-hand menu select “Data” category under the ‘Connection’ category and type your username on the right side   To save these settings for later logins select the ‘Session’ category, and in the “Saved Sessions” type SCINet Ceres or SCINet Atlas, then click on “Save” button.   Hit “Open”   Enter the 6-digit verification code generated by the Google Authenticator app when prompted. Note that when you type the code or the password, nothing will be shown on the screen. If you made a mistake when typing 6-digit code, you will be prompted for Verification code once again, but you must wait for the new code to be generated before entering the code again.  If system accepts the code it will prompt you for current password. If you are logging in for the first time, enter your temporary password provided in your welcome email. If you are changing your password, you may enter your current password. To paste from a clipboard into Putty, perform single right click with the mouse and then press Enter. (However, this form of copy and paste may no longer be functional)  Note: The new password requirements are listed below the Mac and Linux instructions. From Mac and LinuxOpen a terminal window. We recommend setting up a config file to make logging in easier and use settings to provide a more stable connection.  This can be done by creating a config file u sing the following code and then saving it to your .ssh file.  The file you create must be titled “config” for this method to work properly.  Note: Do not copy the code into the terminal itself, it must be a separate file on your computer.Create a ~/.ssh/config entry similar to this, replacing USER.NAME with your actual username, all in lowercase:Host ceres-login HostName ceres.scinet.usda.gov User USER.NAME TCPKeepAlive yes ServerAliveInterval 20 ServerAliveCountMax 30  Host atlas-login HostName atlas-login.hpc.msstate.edu User USER.NAME TCPKeepAlive yes ServerAliveInterval 20 ServerAliveCountMax 30 Note: To save this config file to your .ssh folder, you must save it to your user folder. However, the .ssh file is currently hidden to you. To reveal the hidden files, you will press and hold CMD + SHIFT + . (Period Key) when choosing a location to save your file. The .ssh file will now be visible for you to save the config file.That will send a “keepalive” signal every 20 seconds and keep retrying for up to 30 failures. This also simplifies your login to just:ssh ceres-login or ssh atlas-loginIf you don’t want to use the config file method above, add the following title to the ssh command replacing USER.NAME with your actual username in lowercase.ssh -o TCPKeepAlive=yes -o ServerAliveInterval=20 -o ServerAliveCountMax=30 USER.NAME@ceres.scinet.usda.gov andssh -o TCPKeepAlive=yes -o ServerAliveInterval=20 -o ServerAliveCountMax=30 USER.NAME@atlas-login.hpc.msstate.edu After typing ssh command, enter the 6-digit code verification code generated by the Google Authenticator app when prompted for the Verification Code. Note that when you type the code or the password, nothing will be shown on the screen. If system accepts the code it will prompt you for password.Please Note: If you are connected to the SCINet VPN, you will not be prompted to enter the Google Authenticator Code. If you make a mistake entering the code, you will be prompted to enter the verification code once more, but you must wait for a new code to be generated.When Changing your Password (Mandatory Prompt when First Connecting):   Enter either of the following: $ ssh user.name@ceres.scinet.usda.gov OR ssh user.name@atlas-login.hpc.msstate.edu   Enter the 6-digit verification code generated by the Google Authenticator app when prompted.   Enter your temporary password found in your welcome email when prompted for the password.   When prompted for your “Current Password”, re-enter your temporary password.   Enter your “new password” twice when prompted.    Note: The new password requirements are listed below. After updating your password, you will be prompted to answer Yes or No to entering a cell phone. It is strongly recommended to add a cell phone just in case you need to reset your Google Authenticator in the future. Enter “y” and then enter your cellphone number with no dashes or parenthesis, as shown in the image below.Password Requirements   AT LEAST 14 characters long   Your last 24 passwords cannot be reused. Frequently Asked QuestionsIt didn’t ask me for the Verification Code from Google Authenticator. What do I do?  If you are connected to the VPN, the code will be waived, no need to worry about entering it.I can’t find the .ssh folder when saving my config file. Where is it located?   The .ssh file is usually hidden to you.  You must show all hidden files when saving your file before you will be able to save to the .ssh file.My password/Google Authenticator Verification Code isn’t showing up when I type it. What is wrong?   There is nothing wrong with your terminal, the password and GA Verification Code will never show when you type. That is why it is important to be careful when entering the information. For passwords, copy + paste are extremely helpful to avoid mistakes.After creating my account and changing my temporary password, I was prompted for my cell phone. Should I include this?   It is highly recommended to include your cell phone when setting up your account.  This will be used if you need to perform a self-reset on your Google Authenticator code.  Without the cell phone in your account, you may need to reapply for a SCINet account."
},

    
      
      
      
      
      
    
{
  "title" : "Multifactor Authentication",
  "category" : "User Guides",
  "subcategory" : "Access",
  "set" : "",
  "url" : "/guides/access/mfa",
  "description" : "A guide to setting up multifactor authentication for SCINet",
  "date" : "Mar 06, 2023",
  "content" : "What is Multifactor AuthenticationFederal computer systems are required to use multifactor authentication for site security. This means that a code is required in addition to a password to login to SCINet. If you’re ssh-ing to Ceres cluster from a connected site (where you already had to use second factor such as PIV/CAC), you probably won’t be prompted for a verification code. However the code is always required to ssh to the Atlas cluster and to use other SCINet resources, such as SCINet Galaxy or SCINet Forum. It’s also needed to connect to SCINet VPN.You will need to install an app on a mobile device or a small program on your computer that will generate your verification codes. The app is initialized with your SCINet Google Authentication (GA) account, that is normally created along with your SCINet account.To add your SCINet GA account to the app, you will need a key or a QR code, that you can find in your “Welcome to SCINet” email.Note: If your SCINet account was created before January 2020, and a GA account was never created, attempt to login to the Ceres DTN node:ssh &lt;your SCINet username&gt;@ceres-dtn-0.scinet.usda.gov At the first attempt a GA account will be created for you and you will receive an email containing a key/code that should be used to initialize GA on your mobile device.If you have already had GA installed and initialized but now need to either re-install GA or install it on a new device, and if you saved your SMS phone number in the system, you can reset GA account by going to https://ga.scinet.usda.gov/reset. After resetting your GA account you will receive an email containing a key/code that should be used to initialize GA on your mobile device.Follow the instructions below to set up multifactor authentication on either your mobile Android device (recommended), Apple device (recommended), or on your computer.Google Authenticator (GA) on AndroidRequirementsTo use Google Authenticator on your Android device, it must be running Android version 2.1 or later (other TOTP authenticators can be used if you prefer).Downloading the app   Visit Google Play.   Search for Google Authenticator.   Download and install the application. Setting up the appOpen the Google authenticator app on your mobile device.If this is the first time you have used Authenticator, click the Add an account button. If you already use the app and are adding a new account, choose “Add an account” from the app’s menu.There are two ways to link your device to your account. Using QR code is easier but requires a barcode scanner which you can install if you don’t have it yet (Google Authenticator app will prompt you to install it). An alternative would be to use a Google Authenticator key as mentioned on the above screenshot.Link phone using QR codeSelect Scan a barcode. If the Authenticator app cannot locate a barcode scanner app on your phone, you might be prompted to download and install one. If you want to install a barcode scanner app so you can complete the setup process, press Install then go through the installation process. Once the app is installed, reopen Google Authenticator, display QR code on your computer by clicking on the link in the “Welcome to SCINet” email and point your camera at the QR code.Link phone manually with a GA keySelect Enter a provided key, then Enter account name, which is &lt;your SCINet username&gt;@ceres. Next, enter the Google Authenticator key from the “Welcome to SCINet” email into the box under Enter Key. Make sure you’ve chosen to make the key time-based and press “Save.”VerifyTo verify the application is working correctly,ssh &lt;your SCINet username&gt;@ceres-dtn-0.scinet.usda.gov Enter the verification code currently displayed in Google Authenticator, then enter your SCINet password.If your code is correct, you will be connected to Ceres’ data transfer node. If your code is incorrect and you’re prompted for a verification code again, wait for a new code to be generated on your phone, then enter it on your computer. If you’re still having trouble, verify that the time on your phone is correct.After logging in the first time you will be prompted to save an SMS phone number in the system so an SMS message can be sent to you when a GA account reset is requested.Contact scinet_vrsc@usda.gov if you need assistance.Google Authenticator (GA) on Apple iOS iPhones and iPadsRequirementsTo use Google Authenticator on your iPhone, iPod Touch, or iPad, you must have iOS 5.0 or later. In addition, in order to set up the app on your iPhone using a QR code, you must have a 3G model or later.Downloading the app   Visit the App Store.   Search for Google Authenticator.   Download and install the application. Setting up the appOn your iOS device, open the Google Authenticator application. Tap the plus icon. Tap “Time Based” (label 1).There are two ways to link your device to your account. Using a QR code is easier but requires a camera. An alternative would be to use a Google Authenticator key in the Key field (label 2c in above screenshot).Link phone using QR codeTap “Scan Barcode” (label 2a above) and then point your camera at the QR code on your computer screen (click on the link in the “Welcome to SCINet” email to access QR code).Link phone manually with a GA keyIn the box next to Account (label 2b above), enter your account name, which is &lt;your SCINet username&gt;@ceres . Then, enter the Google Authenticator key from the “Welcome to SCINet” email into the box next to “Key” (label 2c above) and tap “Done” (label 2d above).VerifyTo verify the application is working correctly,ssh &lt;your SCINet username&gt;@ceres-dtn-0.scinet.usda.gov Enter the verification code currently displayed in google authenticator and then enter your password.If your code is correct, you will be connected to Ceres’ data transfer node. If your code is incorrect and you’re prompted for a verification code again, wait for a new code to be generated on your phone, then enter it on your computer. If you’re still having trouble, verify that the time on your phone is correct.After logging in the first time you will be prompted to save an SMS phone number in the system so an SMS message can be sent to you when a GA account reset is requested.Contact scinet_vrsc@usda.gov if you need assistance.Authentication on your computer using AuthyIf you are unable to use a smart device for multifactor authentication you may use Authy on your PC for a second factor.Authy can be downloaded from https://authy.com.Click the download button then scroll down to the “Desktop” section and download the binary for your platform. (Windows or macOS)Run the installer. This won’t require administrator privileges, but it will need to be run by each user of the machine.The installer will need to tie your login to a phone number and an email address. The phone number can be either a land line or a cell phone.The systems will want to send you a verification number. If you are not using a cell phone please select “voice call” (not SMS) for the verification.After making your choice a code will be shown and you should receive a call or a text which you should respond to with the code on your screen.You are now setup and can add your SCINet GA key to Authy.To do so click the red “+” in the Authy app and enter the key from the email you received titled “Welcome to SCINet”.You will need to enter the key from the email since Authy cannot use QR codes.Now name the account and pick an icon.Authy should now be displaying codes for use with Ceres. Each code is shown for 30 seconds and then a new code in generated automatically. Codes are only valid for a few seconds after they are displayed and cannot be reused.Contact scinet_vrsc@usda.gov if you need assistance.GA account resetUsers may need to reset GA accounts when switching or reinstalling their mobile devices that have GA application installed. To be able to perform GA reset by yourself, save an SMS phone number in the system by issuing phone-collect.sh command on Ceres. This is the same process that is run automatically the first time you login. After doing that you will be able to do a self-service reset of your GA at https://ga.scinet.usda.gov/reset/.If you are unable to or do not wish to save an SMS phone number, GA account reset request will have to go though your supervisor or sponsor as we have no way of directly verifying your identity."
},

    
      
      
      
      
      
    
{
  "title" : "Environment Modules",
  "category" : "User Guides",
  "subcategory" : "Software",
  "set" : "",
  "url" : "/guides/software/modules",
  "description" : "The Environment Modules package provides dynamic modification of your shell environment.",
  "date" : "Mar 06, 2023",
  "content" : "The Environment Modules package provides dynamic modification of your shell environment. This also allows a single system to accommodate multiple versions of the same software application and for the user to select the version they want to use. Module commands set, change, or delete environment variables, typically in support of a particular application.Useful Modules CommandsHere are some common module commands and their descriptions:               Command       Description                       module list       List modules currently loaded in your environment                 module avail / module spider       List available modules                 module unload &lt;module name&gt;       Remove &lt;module name&gt; from the environment                 module load &lt;module name&gt;       Load &lt;module name&gt; into the environment                 module help &lt;module name&gt;       Provide information about &lt;module name&gt;                 module swap &lt;module one&gt; &lt;module two&gt;       Replace &lt;module one&gt; with &lt;module two&gt; in the environment         For example to use NCBI-BLAST installed on Ceres, follow these steps:$ module load blast+ This will load latest version of NCBI-BLAST into your environment and you can use all commands that come with this installation. To see the path to the loaded software and the version type$ which blastp which should display something like:/software/7/apps/blast+/2.9.0/bin/blastp If you want to load legacy NCBI-BLAST on Ceres, follow the example below:$ module load blast $ which blastall should display something like/software/7/apps/blast/2.2.26/bin/blastall If you would like to find out more about a particular software module, you can use the  module help  command, e.g.$ module help blast will output basic information about the blast package, including an URL to the package website.Loading and Unloading ModulesYou must remove some modules before loading others, to switch versions or dependencies.For example, if you have already loaded a blast+ module using the “module load blast+” command to use latest version of NCBI-BLAST, but later you want to load a previous version of blast+ (2.2.30), then follow the steps below:$ module swap blast+ blast+/2.2.31 or:$ module unload blast+ $ module load blast+/2.2.31 $ which blastp The last command should display/software/7/apps/blast+/2.2.31/bin/blastp Another example. If you want to compile parallel C, C++, or Fortran code and wanted to use OpenMPI instead of MPICH which is currently loaded in your environment, you can use  module swap  or  module unload:$ module swap mpich openmpi or:$ module unload mpich $ module load openmpi Some modules depend on other modules, so additional modules may be loaded or unloaded with one module command. For example, BEAST requires a Java module, so loading the “beast” module automatically loads the correct Java version:$ module load beast $ which java should display something like:/software/7/apps/java/1.8.0_121/bin/java If you find yourself regularly using a set of module commands, you may want to add these to your configuration files (.bashrc for Bash users, .cshrc for C shell users).Module: command not foundThe error message module: command not found is sometimes encountered when switching from one shell to another or attempting to run the module command from within a shell script or batch job. The reason that the module command may not be inherited as expected is that it is defined as a function for your login shell. If you encounter this error execute the following from the command line (interactive shells) or add to your shell script:$ source /etc/profile.d/modules.sh "
},

    
      
      
      
      
      
    
{
  "title" : "Open OnDemand Interface Guide",
  "category" : "User Guides",
  "subcategory" : "Access",
  "set" : "",
  "url" : "/guides/access/open-ondemand",
  "description" : "Using Open OnDemand on Ceres",
  "date" : "Mar 06, 2023",
  "content" : "Open OnDemand is an intuitive, innovative, and interactive interface to remote computing resources. The key benefit for SCINet users is that they can use any web browser, including browsers on a mobile phone, to access Ceres. AccessTo access Open OnDemand on the Ceres cluster, go to Ceres OpenOndemand  The login page, titled SCINet Two Factor Login, prompts for your SCINet username and then your combined password and Google Authenticator code. If your password were password and your code were 123456, your entry for this field would be:password123456 Using OODShell AccessTo open a shell, select the Clusters dropdown menu and choose &gt;_Ceres Shell Access. This will open a new tab with an interactive console session on the login node.FilesOpen OnDemand includes a file manager. To open it, select the Files dropdown menu and choose the desired directory. Files can be uploaded, downloaded, viewed, and edited all from the web browser.Interactive AppsSeveral interactive apps are available on Open Ondemand, and more can be added later.To launch an interactive app, select the Interactive Apps dropdown menu and choose the desired app. It will open the My Interactive Sessions page where settings for app can be selected such as the partition or app version to run.   After selections have been made, pressing Launch will submit an interactive job. This job will be displayed as the topmost entry in the My Interactive Sessions page. The bottom of the intaractive app’s job information card will display text saying the job is starting or, if it has already started, “Connect to Interactive App Name.” Press the Connect button to launch the app in a new tab.  Logging OutTo log out of Ceres Open OnDemand, click the Log Out button."
},

    
      
      
      
      
      
    
{
  "title" : "Software preinstalled on Ceres",
  "category" : "User Guides",
  "subcategory" : "Software",
  "set" : "",
  "url" : "/guides/software/preinstalled",
  "description" : "Software preinstalled on Ceres",
  "date" : "Mar 06, 2023",
  "content" : "Graphical Software               Software       information                       Galaxy Server       SCINet Runs its own Galaxy Server.  Galaxy is an open source, web-based platform for data intensive bioinformatic research.                 CLC Server       SCINet has a license key to CLC Server                 Geneious       SCINet has a license key to Geneious                 Rstudio       SCINet runs a R studio Server                 Jupyter       Project Jupyter notebooks and Lab can be run on Ceres         Command-line Software on SCINet ClustersEach SCINet cluster has software preinstalled on it. Some general software is available in the global environment but most specialized scientific software is managed by the Module system. This software can be loaded with a command likemodule load bamtools or a specific version can be selected withmodule load bamtools/2.5.1 Available modules are listed below in the section Ceres Command-line Software Managed by the Module System.The command-line software I need is not on the list!No problem. You have two options: 1) you can install the software yourself or 2) you can request the software be installed for you.Option 1: Install the software yourselfThe easiest way to do this is to use a conda environment. Often times installing the software you need is as easy as typingconda create --name my_environment my_program Thousands of biological packages and their dependencies can be installed with a single command using the Bioconda repository for the Conda package manager. You can also install a package directly from Github or elsewhere and compile it yourself. For more on installing Conda see User-installed Software on Ceres with Conda.You can also install and or compile software manually in your  $HOME  or  $PROJECT  directories. This is the fastest and easiest way to get your software.Option 2: Request a new module be installedIf you need software that you think will be useful to many SCINet users you can request that the software be installed as a module. Doing this requires an agency-level security review and takes a few weeks. Modules can be requested with the Software request form (eAuthentication required, non-ARS users should contact their sponsor).Ceres Command-line Software Managed by the Module System"
},

    
      
      
      
      
      
    
{
  "title" : "Storage Quotas",
  "category" : "User Guides",
  "subcategory" : "Data",
  "set" : "",
  "url" : "/guides/data/quotas",
  "description" : "Quotas on Home and Project Directories",
  "date" : "Mar 06, 2023",
  "content" : "Each file on a Linux system is associated with one user and one group. On Ceres, files in a user’s home directory by default are associated with the user’s primary group, which has the same name as user’s SCINet account. Files in the project directories by default are associated with the project groups. Group quotas that control the amount of data stored are enabled on both home and project directories.At login, current usage and quotas are displayed for all groups that a user belongs to. The my_quotas command provides the same output:$ my_quotas If users need more storage than what is available in the home directory, they should visit the Request a Project Storage page. Several users may work on the same project and share the same project directory.Project directories are located in the 2.3PB Lustre space that is mounted on all nodes as /lustre/project and is also accessible as /project. Directories in /project are not backed up. It is not recommended to run jobs from a directory in /KEEP.Since on Ceres usage and quotas are based on groups, it’s important to have files in the home directories to be associated with the users’ primary groups, and files in the project directories to be associated with project groups. Sometimes it may happen that files that were originally located in a home directory, were later moved to a project directory with the group ownership preserved. In this case even though files will be located in a project directory, they still will count against home directory quota. To fix this, change the group ownership of these files to the project directory group. The following command will change group association of all files in the project directory in /project (it may take a while if there are too many files in the directory):$ chgrp -R proj-&lt;project_directory_name&gt; /project/&lt;project_directory_name&gt; To search for files owned by your primary group in a project directory, issue:$ find /project/&lt;project_directory_name&gt; -group &lt;SCINet UserID&gt; -type f For more information about storage options, refer to SCINet Storage Guide.Local Sharing of Files with Other UsersUsers who would like to share files with other users can use /90daydata/shared directory. Files older than 90 days will be automatically deleted.NOTE: Files in /90daydata/shared folder by default are accessible to everybody on the system. Thus, this mechanism for sharing should only be used for files of a non-confidential nature."
},

    
      
      
      
      
      
    
{
  "title" : "Guide to Installing R, Python, and Perl Packages",
  "category" : "User Guides",
  "subcategory" : "Analysis",
  "set" : "",
  "url" : "/guides/analysis/r-perl-python",
  "description" : "Installing R, Python, and Perl packages",
  "date" : "Mar 06, 2023",
  "content" : "The popular R, Perl and Python languages have many packages/modules available. Some of the packages are installed on Ceres and are available with the r/perl/python_2/python_3 modules. To see the list of installed packages, visit the Preinstalled Software List page or use  module help &lt;module_name&gt;  command. If users need packages that are not available, they can either request VRSC to add packages, or they can download and install packages in their home/project directories. We recommend installing packages in the project directories since collaborators on the same project most probably would need same packages. In addition, home quotas are much lower than quotas for project directories. The following instructions are for a few commonly used software packages to help users install these packages without admin intervention.Installing R PackagesUsers can install R packages using the  install.packages()  command.The default location is the user’s home directory. User can specify other directories to which they have access privileges, such as project directory. Installing R packages in a project directory is recommended as users working on the same project have access to the same environment.To install packages in a project directory, go to the project directory:cd &lt;path/to/the/project/directory&gt; and create a “.Renviron” file containing the following line:R_LIBS_USER=&lt;path/to/the/project/directory&gt;/R_packages/%vThen when R is started from the directory &lt;path/to/the/project/directory&gt;, the R packages for the given version of R will be saved in the above location.Load an environment module for the desired version of R and start R:$ module load r/4.0.3 $ R R version 4.0.3 (2020-10-10) -- Bunny-Wunnies Freak Out To install A3 package issue the  install.packages()  command and answer “y” to both questions:&gt; install.packages(A3,repos=http://cran.r-project.org) Warning in install.packages(A3) : 'lib = /lustre/project/software/7/apps/r/4.0.3/lib64/R/library' is not writable Would you like to use a personal library instead? (yes/No/cancel) yes Would you like to create a personal library &lt;path/to/the/project/directory&gt;/R_packages/4.0 to install packages into? (yes/No/cancel) yes To see the library paths, issue  .libPaths()  command from within R:&gt; .libPaths(); [1] &lt;path/to/the/project/directory&gt;/R_packages/4.0 [2] /lustre/project/software/7/apps/r/4.0.3/lib64/R/library PythonUsers can install python packages(via pip) to their home directory. This creates a “site-packages” directory within the user’s home directory.$ module load python_3 $ pip3 install --user &lt;package.name&gt; By default the packages for python 3.5 are stored in ~/.local/lib/python3.5/site-packagesIf users intend to share the python environment with users working on the same project (or to install packages in a non-standard location), using virtualenv is a much better way to deal with managing python packages.After loading the python module, create a virtual environment by:$ virtualenv &lt;path/to/the/project/directory/name.of.the.project&gt; This creates a directory in your current project directory. This example below shows a virtualenv “virt_test” being created and activated.$ virtualenv virt_test Using base prefix '/software/apps/python_3/gcc/64/3.5.0' New python executable in /root/virt_test/bin/python3.5 Also creating executable in /root/virt_test/bin/python Installing setuptools, pip, wheel...done. $ source virt_test/bin/activate $ source virt_test/bin/activate (virt_test) $ pip3 list pip (9.0.1) setuptools (36.0.1) wheel (0.29.0) Note that this virtual environment starts clean without using any global python site-packages that are already installed. Use  --system-site-packages  if you want to use global site-packages.To exit this environment, run deactivate:(virt_test) $ deactivate This virtual environment can be activated again anytime.For instructions on how to manage Python packages using Conda, see User-installed Software on Ceres with Conda.PerlThere are multiple ways to install modules in Perl and depending on the use case, one may be preferable over the others.cpanm is a convenient tool to install modules in the home directory.$ module load perl $ cpanm Test::More Running the above command creates a “perl5” directory within the users’ home directory and contains all the required binaries and libraries associated with the “Test::More” module. Add PERL5LIB to the environment$ export PERL5LIB=$HOME/perl5/lib/perl5:$PERL5LIB Perl modules can also be installed in other directories. For example, the user can have the modules available to the rest of the project members so that everyone involved works in the same environment.local::lib provides the flexibility for users to install modules in any custom location (as long as they have write permissions).$ eval $(perl -I/path/to/the/project/dir/perl5/lib/perl5 - Mlocal::lib=/path/to/the/project/dir/perl5) In addition to creating “perl5” directory within the specified project directory, the above command adds perl local::lib environment variables.Then use cpanm to install the required modules.$ cpanm Test::More Note: The package source files will still be downloaded to your home directory (~/.cpanm/sources)Note: All users working on the project can access modules that were installed using local:lib and perlbrew, but only the original user can perform perl module installs.perlbrew is another tool that can create perl environments. perlbrew offers a lot of flexibility to users as they can install different versions of perl based on the requirements of the project.$ module load perl $ perlbrew availableperl-5.27.1 perl-5.26.0 perl-5.24.1 ...... Set the PERLBREW_ROOT variable to a preferred location.$ export PERLBREW_ROOT=/path/to/project/dir/perlbrew $ perlbew init To install the latest stable release$ perlbrew --notest install stable To install a specific version of perl$ perlbrew --notest install perl-5.24.0 $ perlbrew list perl-5.26.0 perl-5.24.0 To use the latest perl,$ perlbrew use 5.26.0 Users can then use cpanm to install modules based on their requirements.It is also possible to switch between different perl installs(if you have them installed)$ perlbrew switch 5.24.0 This switches from 5.26.0 to 5.24.0Note: All users working on the project can access modules that were installed using local:lib and perlbrew, but only the original user can perform perl module installs."
},

    
      
      
      
      
      
    
{
  "title" : "Rstudio Server",
  "category" : "User Guides",
  "subcategory" : "Analysis",
  "set" : "",
  "url" : "/guides/analysis/r-studio",
  "description" : "Using Rstudio",
  "date" : "Mar 06, 2023",
  "content" : "RStudio is an integrated development environment (IDE) for the R programming language, with limited support for other programming languages (including Python, bash, and SQL). RStudio provides a powerful graphical environment for importing data in a number of formats (including CSV, Excel spreadsheets, SAS, and SPSS); manipulating, analyzing, and visualizing data; version control with git or SVN; a graphical R package manager that provides point/click search/installation/uninstallation of R packages from its substantial ecosystem (including the Bioconductor repository, which provides almost 1500 software tools “for the analysis and comprehension of high-throughput genomic data.”); and many other features.RStudio Server is a client/server version of RStudio that runs on a remote server and is accessed via the client’s web browser. A graphical file manager allows file upload/download from Ceres via web browser.SCINet Options for RStudioSCINet users can use RStudio in one of the following ways:   To run RStudio and access data on your local workstation, download the open source RStudio Desktop.   To run an RStudio Server virtual machine on and access data in Amazon Web Services, contact the SCINet VRSC.   NEW: RStudio Server on Ceres in Open OnDemand.   RStudio Server on Ceres through SCINet VPN or via ssh-tunnel. RStudio Server on CeresRStudio Server is currently available on Ceres using a Docker image (imported into Singularity) provided by the Rocker project. The provided geospatial image provides not only geospatial libraries, but also LaTeX / publishing libraries, and Tidyverse data science libraries. Other R packages can be easily installed into your home directory from within RStudio.Running RStudio Server on Ceres allows SCINet users to access any data on Ceres that they can access from the command line (SSH).Users can either run RStudio Server in Open OnDemand or by manually submiting a SLURM job script and connecting to the server through SCINet VPN or via ssh-tunnel. The first method (via Open OnDemand) is much easier-to-use and is recommended over the second method.A few Ceres-specific notes:   RStudio terminal (bash command shell): since RStudio Server is running in a container with a Debian base image, you won’t be able to access software environment modules (e.g., that you would normally see when logging into Ceres and issuing the  module list  command), as those are installed on the (CentOS) host.   Data access: your home directory is mounted inside the RStudio Server container, and the VRSC has configured Singularity to mount the /project directory.  $TMPDIR (which on a compute node is per-job local scratch on the compute node’s direct attached storage that gets deleted at the end of SLURM job) is mounted inside the container at /tmp.   Software installation: The provided SLURM job script creates a ~/.Renviron file in your home directory that allows RStudio to install additional R packages into your home directory (the container image is immutable). Installing a lot of R libraries may contribute to the default 5G soft limit quota on your home directory being surpassed. To overcome this issue you can move R directory from your home directory to your project directory and create a symbolic link to the new location. In RStudio click on “Terminal” and enter the following commands substituting your values: cd mkdir /project/&lt;your_project_dir&gt;/&lt;account_name&gt; mv R /project/&lt;your_project_dir&gt;/&lt;account_name&gt;/ chgrp -R proj-&lt;your_project_dir&gt; /project/&lt;your_project_dir&gt;/&lt;account_name&gt;/R ln -s /project/&lt;your_project_dir&gt;/&lt;account_name&gt;/R R The mv and chgrp commands may take longer time depending on how much data you have in the R directory. You will need to restart R for this new location to take effect. In the “console” window type quit() and click on “Start New Session”.   Default working directory: By default RStudio sets working directory to your home directory. You can change the default by clicking on Tools -&gt; Global Options… In the General tab click on Browse, then on the three dots on the right and enter path to the folder, e.g. /project/. You can then navigate further to a subdirectory. When done click on Choose button, and then on OK. This will be the default working directory that will show in the panel on the right next time you start RStudio or restart R in the RStudio. RStudio Server in Open OnDemandIn your browser go to https://ceres-ood.scinet.usda.gov/, enter your SCINet user name and SCINet password, followed by the GA code. See MFA Guide for more information about GA codes.After logging into Open OnDemand, click on “Interactive Apps” in the menu on the top and select “RStudio Server: Ceres”.Modify default values if needed and click on the “Launch” button at the bottom of the page. It may take up to several minutes for the new session to start. Once it starts a new button “Connect to RStudio Server” will appear. Clicking on the button will open RStudio IDE in a new browser tab. When done using RStudio, return to the previous tab to stop the RStudio session by clicking on the red “Delete” button.Note that while the RStudio session is running you can connect and disconnect to/from the RStudio Server multiple times. Every click on the “Connect to RStudio Server” button will open a new browser tab.Also note that once you click on the “Launch” button and a new session starts, requested resources will be allocated to your job and won’t be available to other users even if you don’t run anything in the RStudio. Please be considerate, request only resources that you need for your tasks and remember to delete the session when done.For tips and tricks refer to the previous section and to the RStudio Tutorial at https://www.dataquest.io/blog/tutorial-getting-started-with-r-and-rstudio/#tve-jump-173bb2584fe.RStudio Server through VPN or ssh tunnelTo use RStudio Server on Ceres, a user submits a SLURM job script. A default job script that should suffice for most users is provided.After a user is done using RStudio Server, they should save their work in RStudio, and then stop RStudio Server by cancelling the job with the slurm  scancel  command.Starting RStudio ServerThe instructions below tell to use RStudio version 3.6.0 . To see other versions available on Ceres issue:ls -l /reference/containers/RStudio/ If you wish to use a different version, replace 3.6.0 below with the version number you choose.        (If using VPN) Connect to SCINet VPN (see video instructions):   VPN Using the OpenConnect Client  VPN Using the Cisco AnyConnect Client           Log into Ceres via SSH (see the Quick Start Guide for instructions).      Submit the RStudio SLURM job script with the following command:     sbatch /reference/containers/RStudio/3.6.0/rstudio.job          (Optional) By default, this SLURM job is limited to a 4 hour time limit, 1 processor core, and 6600 MB memory. To customize, see the section Requesting Additional Compute Resources below.      After the job has started, view the “$HOME/rstudio-JOBID.out” file for login information (where JOBID is the SLURM job ID reported by the sbatch command). Here is an example of such a file. Your file will have different job number, user name, password, compute node name and port number.     [jane.user@sn-cn-8-1 ~]$ sbatch /reference/containers/RStudio/3.6.0/rstudio.job Submitted batch job 214664  [jane.user@sn-cn-8-1 ~]$ cat ~/rstudio-214664.out VPN Users:  1. Connect to SCINet VPN and point your web browser to http://ceres14-compute-3-eth.scinet.local:44200  2. log in to RStudio Server using the following credentials:     user: jane.user    password: 4wjRJfpIvQDtKdDZpmzY  SSH users:  1. SSH tunnel from your workstation using the following command (macOS or Linux only;    for how to enter this in PuTTY on Windows see the Ceres RStudio User Guide)     ssh -N -L 8787:ceres14-compute-3-eth.scinet.local:44200 jane.user@ceres.scinet.usda.gov     and point your web browser to http://localhost:8787  2. log in to RStudio Server using the following credentials:     user: jane.user    password: 4wjRJfpIvQDtKdDZpmzY  When done using RStudio Server, terminate the job by:  1. Exit the RStudio Session (power button in the top right corner of the RStudio window) 2. On the Ceres command line, issue the command     scancel -f 214664           Instructions for this step differ for when you use SCINet VPN or SSH Port Forwarding. Using VPNIf using VPN point your web browser to the listed hostname / port (in the example above, http://ceres14-compute-3-eth.scinet.local:44200), then enter your SCINet user name and the temporary password (valid only for this job only; in this example 4wjRJfpIvQDtKdDZpmzY) SSH Port Forwarding (instead of VPN)Note Before performing the instructions below, first read and follow the instructions in steps 2-4.Windows + PuTTY users   Open a new PuTTY window   In Session &gt; Host Name, enter: ceres.scinet.usda.gov   In the category: Connection &gt; SSH &gt; Tunnels, enter 8787 in Source Port, the Destination hostname:port listed in the job script output (in this example: ceres14-compute-3-eth:44200), click “Add”, then click “Open”.    Point your browser to http://localhost:8787. Enter your SCINet user name, and one-time password listed in the job script output file. macOS / Linux / Windows + Windows PowerShell users   Open a new macOS/Linux terminal window or a new Windows PowerShell window and enter the SSH command listed in the job script output file. In this example:     ssh -N -L 8787:ceres14-compute-3-eth.scinet.local:44200 jane.user@ceres.scinet.usda.gov          There will be no output after logging in. Keep the window / SSH tunnel open for the duration of the RStudio session.      Point your browser to http://localhost:8787. Enter your SCINet user name, and one-time password listed in the job script output file. Stopping RStudio Server   Click the Quit Session (“power”) button in the top-right corner of the RStudio window (see picture below), or select “File &gt; Quit Session…”    After the “R Session has Ended” window appears, cancel the SLURM job from the Ceres command line. E.g., if the job ID is 214664:     [jane.user@sn-cn-8-1 ~]$ scancel -f 214664          Be sure to specify the  scancel -f  /  --full  option as demonstrated above.      (If using SSH Port Forwarding instead of VPN) Close the terminal / PuTTY window in which the SSH tunnel was established. Requesting Additional Compute ResourcesThe default job resources (4 hour time limit, 1 processor core, 6600 MB memory) may be customized by:   sbatch command-line options, e.g., to specify an 8-hour wall time limit, 16 G memory, and 2 processor cores (= 4 hardware threads):     sbatch --time=08:00:00 --mem=16G --cpus-per-task=4 /reference/containers/RStudio/3.6.0/rstudio.job           Copying the job script to a directory one has write access to and modifying the appropriate SLURM #SBATCH directives.   Using the Ceres Job Script Generator to create a new job RStudio Server job script            In the Job Script Template drop down menu, select RStudio Server         "
},

    
      
      
      
      
      
    
{
  "title" : "Rclone: Moving Data To and From Cloud Resources",
  "category" : "User Guides",
  "subcategory" : "Data",
  "set" : "",
  "url" : "/guides/data/rclone",
  "description" : "Using rclone",
  "date" : "Mar 06, 2023",
  "content" : "Rclone is already installed on the DTNS and all of the compute nodes. Please do not use rclone from the headnode. Attempting to do so will remind you to use the others.  The rclone home page is https://rclone.org.Getting ReadyIn order to use Rclone on Ceres its necessary to have it installed on your local machine as well.  This is needed to generate an authentication token. rclone installation on WindowsGo to the web page at https://rclone.org/downloads/ and find the Windows installer. Download it and install rclone. Once installed, proceed to the configuration section below.macOS installationDownload the latest version of rclone.cd &amp;&amp; curl -O https://downloads.rclone.org/rclone-current-osx-amd64.zip Unzip the download and cd to the extracted folder.unzip -a rclone-current-osx-amd64.zip &amp;&amp; cd rclone-*-osx-amd64 Move rclone to your $PATH. You will be prompted for your password.sudo mkdir -p /usr/local/bin sudo mv rclone /usr/local/bin/ (the mkdir command is safe to run, even if the directory already exists) Remove the leftover files.cd .. &amp;&amp; rm -rf rclone-*-osx-amd64 rclone-current-osx-amd64.zip Configuration of rclone on windows or osX   Open a Windows command prompt (cmd) or macOS Terminal   Type     rclone authorize box           On the web page that shows up, click on Use Single Sign On (SSO)   Enter your USDA email address   Do the eAuthentication thing   Click on the Grant access to Box button   Go back to the command prompt window, an authentication token should be there. Copy this including the braces {“access_token”:“ABCDEF…} rclone configuration on SciNet   Type     rclone config           Type n for n) New remote   For **** enter any name e.g. **usdabox**   For Storage&gt; you can find the number, but it is easier to just type box   For client_id&gt;, client_secret&gt;, box_config_file, and access_token leave blank, just hit enter   For box_sub_type&gt; enter enterprise   For Edit advanced config enter n   For Remote config, Use auto config? enter n   For result Paste the text from the last step of the above rclone guide section “Configuration of rclone on windows and osX” and hit enter   Type y for y) Yes this is OK   Type q to quit Test   Test directory listing using the name&gt; you selected earlier:     rclone lsd usdabox:           Test file listing     rclone lsl usdabox: | head         Commands   For description of commands available see https://rclone.org/docs/#subcommands   You installed the rclone manual page earlier, so you can also do     man rclone           Acommon case might be to backup your SciNet project directory to box. You could do this with:     rclone copy /project/bogus_genome usdabox:/scinetbackup/bogus_genome --verbose         Advanced commandsThis advanced guide assumes you have read the previous page and have some familiarity with rclone already.Rclone supports “overlay” filesystems which can be then be overlayed in multiple layers.In this guide we will discuss the “crypt” and “chunk” overlays.  Using these 2 in combination can overcome virtually all of the limitations of any of the remote services.  i.e.. no 5, 10, or 15GB limit per file, no restrictions on special characters in filenames, etc. The overlays also provide some data protection by doing md5 checksums on the files transferred, if desired.  You should very much desire this.The process here is:   Create a basic store   Apply a crypt overlay.  This gets you encryption and works around filename limitations.   Apply a chunk overlay.  This gets around file size limitations. We are going to assume you have  already created a basic functional remote, In this case mine is called “google:”First we create a folder in the remote to hold our encrypted data.  I called mine “crypt”$ rclone mkdir google:crypt Next run rclone config choosing crypt as the remote type and then use the name of your new folder as the path.  You will want to encrypt the directory names to avoid character limitation issues in the path.You must remember the password(s) you chose here. Your data will not be recoverable by anyone if your forget or lose it. There is no “password recovery.”$ rclone config  Current remotes:     Name                 Type ====                 ==== boxsecret            crypt boxsecretchunked     chunker cybox                box google               drive onedrive             onedrive onedrivechunker      chunker onedrivecrypt        crypt  e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q&gt; n name&gt; googlecrypt  Type of storage to configure. Enter a string value. Press Enter for the default (). Choose a number from below, or type in your own value   1 / 1Fichier    / fichier  2 / Alias for an existing remote    / alias  3 / Amazon Drive    / amazon cloud drive  4 / Amazon S3 Compliant Storage Provider (AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Minio, etc)    / s3  5 / Backblaze B2    / b2  6 / Box    / box  7 / Cache a remote    / cache  8 / Citrix Sharefile    / sharefile  9 / Dropbox    / dropbox 10 / Encrypt/Decrypt a remote    / crypt 11 / FTP Connection    / ftp 12 / Google Cloud Storage (this is not Google Drive)    / google cloud storage 13 / Google Drive    / drive 14 / Google Photos    / google photos 15 / Hubic    / hubic 16 / JottaCloud    / jottacloud 17 / Koofr    / koofr 18 / Local Disk    / local 19 / Mail.ru Cloud    / mailru 20 / Mega    / mega 21 / Microsoft Azure Blob Storage    / azureblob 22 / Microsoft OneDrive    / onedrive 23 / OpenDrive    / opendrive 24 / Openstack Swift (Rackspace Cloud Files, Memset Memstore, OVH)    / swift 25 / Pcloud    / pcloud 26 / Put.io    / putio 27 / QingCloud Object Storage    / qingstor 28 / SSH/SFTP Connection    / sftp 29 / Transparently chunk/split large files    / chunker 30 / Union merges the contents of several remotes    / union 31 / Webdav    / webdav 32 / Yandex Disk    / yandex 33 / http Connection    / http 34 / premiumize.me    / premiumizeme Storage&gt; crypt  ** See help for crypt backend at: https://rclone.org/crypt/ **  Remote to encrypt/decrypt. Normally should contain a ':' and a path, eg myremote:path/to/dir, myremote:bucket or maybe myremote: (not recommended). Enter a string value. Press Enter for the default (). remote&gt; google:crypt How to encrypt the filenames. Enter a string value. Press Enter for the default (standard). Choose a number from below, or type in your own value  1 / Don't encrypt the file names.  Adds a .bin extension only.    / off  2 / Encrypt the filenames see the docs for the details.    / standard  3 / Very simple filename obfuscation.    / obfuscate filename_encryption&gt;  Option to either encrypt directory names or leave them intact. Enter a boolean value (true or false). Press Enter for the default (true). Choose a number from below, or type in your own value  1 / Encrypt directory names.    / true  2 / Don't encrypt directory names, leave them intact.    / false directory_name_encryption&gt; 1 Password or pass phrase for encryption. y) Yes type in my own password g) Generate random password n) No leave this optional password blank y/g/n&gt; y Enter the password: password: Confirm the password: password: Password or pass phrase for salt. Optional but recommended. Should be different to the previous password. y) Yes type in my own password g) Generate random password n) No leave this optional password blank y/g/n&gt; n Remote config -------------------- [googlecrypt] type = crypt remote = google:crypt directory_name_encryption = true password = *** ENCRYPTED *** -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d&gt; y Current remotes:   Name                 Type ====                 ==== boxsecret            crypt boxsecretchunked     chunker cybox                box google               drive googlecrypt          crypt onedrive             onedrive onedrivechunker      chunker onedrivecrypt        crypt   e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q&gt; q $ rclone ls googlecrypt: $ At this point you have a working encryption overlay.  You will want to add a “chunking” overlay on top on that. As before first create a folder in the encrypted overlay to hold your chunked overlay.  In this case I called mine “chunk”$ rclone mkdir googlecrypt:chunk  Now create a chunk overlay, the defaults should be fine.$ rclone config Current remotes:   Name                 Type ====                 ==== boxsecret            crypt boxsecretchunked     chunker cybox                box google               drive googlecrypt          crypt onedrive             onedrive onedrivechunker      chunker onedrivecrypt        crypt   e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q&gt; n name&gt; googlechunk Type of storage to configure. Enter a string value. Press Enter for the default (). Choose a number from below, or type in your own value  1 / 1Fichier    / fichier  2 / Alias for an existing remote    / alias  3 / Amazon Drive    / amazon cloud drive  4 / Amazon S3 Compliant Storage Provider (AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Minio, etc)    / s3  5 / Backblaze B2    / b2  6 / Box    / box  7 / Cache a remote    / cache  8 / Citrix Sharefile    / sharefile  9 / Dropbox    / dropbox 10 / Encrypt/Decrypt a remote    / crypt 11 / FTP Connection    / ftp 12 / Google Cloud Storage (this is not Google Drive)    / google cloud storage 13 / Google Drive    / drive 14 / Google Photos    / google photos 15 / Hubic    / hubic 16 / JottaCloud    / jottacloud 17 / Koofr    / koofr 18 / Local Disk    / local 19 / Mail.ru Cloud    / mailru 20 / Mega    / mega 21 / Microsoft Azure Blob Storage    / azureblob 22 / Microsoft OneDrive    / onedrive 23 / OpenDrive    / opendrive 24 / Openstack Swift (Rackspace Cloud Files, Memset Memstore, OVH)    / swift 25 / Pcloud    / pcloud 26 / Put.io    / putio 27 / QingCloud Object Storage    / qingstor 28 / SSH/SFTP Connection    / sftp 29 / Transparently chunk/split large files    / chunker 30 / Union merges the contents of several remotes    / union 31 / Webdav    / webdav 32 / Yandex Disk    / yandex 33 / http Connection    / http 34 / premiumize.me    / premiumizeme Storage&gt; chunker ** See help for chunker backend at: https://rclone.org/chunker/ **   Remote to chunk/unchunk. Normally should contain a ':' and a path, eg myremote:path/to/dir, myremote:bucket or maybe myremote: (not recommended). Enter a string value. Press Enter for the default (). remote&gt; googlecrypt:chunk Files larger than chunk size will be split in chunks. Enter a size with suffix k,M,G,T. Press Enter for the default (2G). chunk_size&gt;  Choose how chunker handles hash sums. All modes but none require metadata. Enter a string value. Press Enter for the default (md5). Choose a number from below, or type in your own value  1 / Pass any hash supported by wrapped remote for non-chunked files, return nothing otherwise    / none  2 / MD5 for composite files    / md5  3 / SHA1 for composite files    / sha1  4 / MD5 for all files    / md5all  5 / SHA1 for all files    / sha1all  6 / Copying a file to chunker will request MD5 from the source falling back to SHA1 if unsupported    / md5quick  7 / Similar to md5quick but prefers SHA1 over MD5    / sha1quick hash_type&gt;  Edit advanced config? (y/n) y) Yes n) No y/n&gt;  y/n&gt; n Remote config -------------------- [googlechunk] type = chunker remote = googlecrypt:chunk -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d&gt; y Current remotes:   Name                 Type ====                 ==== boxsecret            crypt boxsecretchunked     chunker cybox                box google               drive googlechunk          chunker googlecrypt          crypt onedrive             onedrive onedrivechunker      chunker onedrivecrypt        crypt   e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q&gt; q  $ rclone ls googlechunk:  You now have an encrypted chunked storage remote, that is fully md5 checksummed."
},

    
      
      
      
      
      
    
{
  "title" : "Running Application Jobs on Compute Nodes",
  "category" : "User Guides",
  "subcategory" : "Use",
  "set" : "",
  "url" : "/guides/use/running-jobs",
  "description" : "Running Application Jobs on Compute Nodes",
  "date" : "Mar 06, 2023",
  "content" : "Users will run their applications on the cluster in either interactive mode or in batch mode. Interactive mode ( salloc  or  srun  command) is familiar to anyone using the command line: the user specifies an application by name and various arguments, hits Enter, and the application runs. However, in interactive mode on a cluster the user is automatically switched from using a login node to using a compute node. This keeps all the intense computation off the login nodes, so that login nodes can have all the resources necessary for managing the cluster. You should always use interactive mode when you are running your application but not using batch mode. Please do not run your applications on the login nodes, use the interactive mode. Interactive mode should only be used when interaction is required, for example when preparing or debugging a pipeline. Otherwise the batch mode should be used. Batch mode requires the user to write a short job script (see examples at section Batch Mode) or use the Ceres Job Script Generator.Ceres uses Simple Linux Utility for Resource Management (SLURM) to submit interactive and batch jobs to the compute nodes. Requested resources can be specified either within the job script or using options with the  salloc,  srun, or  sbatch  commands.Partitions or QueuesCompute jobs are run on functional groups of nodes called partitions or queues. Each different partition has different capabilities (e.g. regular memory versus high memory nodes) and resource restrictions (e.g. time limits on jobs). Nodes may appear in several partitions.Some of the Ceres compute nodes have been purchased by individual researchers or research groups. These nodes are available to the owners in the priority* partitions but can also be used by anyone on the cluster through *-low and scavenger* partitions. These partitions have been introduced to increase usage of the priority nodes while still allowing node owners to have guaranteed fast access to priority nodes. All *-low partitions have 2-hour time limit. Scavenger* partitions have 3-weeks time limit, but jobs in this partition will be killed when resources are requested for the jobs in priority* partitions. Since jobs in the scavenger* partitions can be killed at any moment, running in those partitions does not affect job priorities in the community partitions.The following table lists partitions. Number of nodes in a specific partition can be adjusted from time to time and be different from the one published in this document.Community partitions               Name       Nodes       Logical Cores per Node       Maximum Simulation Time       Default Memory per Core       Function                       short       41       72,96       48 hours       3000 MB       short simulation queue (default)                 medium       32       72,96       7 days       3000 MB       medium length simulation queue                 long       11       72,96       21 days       3000 MB       long simulation queue                 long60       2       72       60 days       3000 MB       extra long simulation queue                 mem       4       80       7 days       16000 MB       large memory queue                 longmem       1       80       1000 hours       16000 MB       long simulation large memory queue                 mem768       1       80       7 days       7900 MB       new node with 768GB of memory                 debug       2       72,96       1 hour       3000 MB       for testing scripts and runs before submitting them         Partitions that allow all users access to priority nodes               Name       Nodes       Logical Cores per Node       Maximum Simulation Time       Default Memory per Core       Function                       mem768-low       3       80       2 hours       7900 MB       priority nodes with 768GB of memory                 mem-low       16       80,96       2 hours       16000 MB       priority nodes with 1.5TB of memory                 gpu-low       1       72       2 hours       3000 MB       priority GPU node                 brief-low       92       72,96       2 hours       3000 MB       all new nodes with 384GB of memory                 scavenger       49       72,80       21 days       3000 MB       non-GPU priority nodes; scavenger jobs can be killed at any moment                 scavenger-gpu       1       72       21 days       3000 MB       GPU priority node; jobs can be killed at any moment         Priority partitions available only to those users who purchased nodes               Name       Nodes       Maximum Simulation Time       Default Memory per Core       Function                       priority       49       2 weeks       3000 MB       priority nodes with 384GB memory                 priority-mem       16       2 weeks       16000 MB       priority nodes with 1.5TB memory                 priority-mem768       3       2 weeks       7900 MB       priority nodes with 768 GB memory                 priority-gpu       1       2 weeks       3000 MB       priority GPU node         At most 1440 cores and 5760 GB of memory can be used by all simultaneously running jobs per user across all community and *-low partitions. In addition, up to 800 cores and 2100 GB of memory can be used by jobs in scavenger* partitions. Any additional jobs will be queued but won’t start. At times these limits can be lowered to prevent a small group of users overtaking the whole cluster.Users that have access to priority partitions are limited by the amount of resources purchased by the group. For example, if a group has purchased one 768GB node, then group members cannot use more than an equivalent of one 768GB node across all jobs simulteniously running in priority-mem768 partition even when there are idle nodes in the partition. However all users on the system can use these idle nodes through *-low and scavenger* partitions. Each group that has purchased nodes on Ceres, has a special QOS created for it. To list QOSes for your account, issue “sacctmgr -Pns show user format=qos”. The group’s QOS needs to be specified when submitting a job to a priority partition via the “-q” salloc/sbatch/srun option. When users submit a job to a priority partition, any node in the partition can be assigned to the job.To get current details on all partitions use the following scontrol command:$ scontrol show partitions Resource AllocationAllocation of CoresOn Ceres hyper-threading is turned on. That means that each physical core on a node appears as two separate processors to the operating system and can run two threads. The smallest unit of allocation per job is a single hyper-threaded core, or 2 logical cores, corresponding to specifying  -n 2  on  salloc/srun/sbatch  commands (i.e. jobs cannot access a single hyper-thread within a core). If a job requests an odd number of cores (-n 1, -n 3,…) SLURM will automatically allocate the next larger even number of cores.Allocation of MemoryEach allocated core comes with a default amount of memory listed in the table above for different SLURM partitions. If a job attempts to use more memory than what was allocated to a job it will be killed by SLURM. In order to make more memory available to a given job, users can either request the appropriate total number of cores or request more memory per core via the  --mem-per-cpu  flag to  salloc/srun/sbatch  commands.For example, to support a job that requires 60GB of memory in the short partition, a user could request 20 logical cores (-n 20) with their default allocation of 3000MB or 2 logical cores with 30GB of memory per core via  --mem-per-cpu 30GB. Please note that a single hyper-threaded core (2 logical cores) is the smallest unit of allocation. Of course, any other mix of memory per core and total number of cores totaling 60GB would work as well depending on the CPU characteristics of the underlying simulation software.Allocation of TimeWhen submitting interactive or batch job users can specify time limit by using the  -t  (–time=) option on  salloc/srun/sbatch  commands. If the time limit is not explicitly specified, it will be set to the partition’s Maximum Simulation Time (see the table above).Slurm accountsTo provide better Ceres usage report all Ceres users have been assigned Slurm accounts based on their project groups. If you don’t have a project, then your default and only Slurm account is sandbox. If you have more than one project, then your default Slurm account is one of the project names. You can specify a different Slurm account when submitting a job by using “-A ” option on salloc/srun/sbatch command or adding “#SBATCH -A ” to the job script.To see all your Slurm accounts and your default account at any time, use “sacctmgr -Pns show user format=account,defaultaccount”You can change your default Slurm account running slurm-account-selector.sh on the login node.Interactive ModeA user can request an interactive session on Ceres using SLURM’s  srun  or  salloc  commands. The simplest way to request an interactive job is by entering the command  salloc:$ salloc which will place you in an interactive shell. This interactive shell has a duration of 2 days and will request a single hyper-threaded core (2 logical cores) with 6000 MB of allocated memory on one of the compute nodes.To prevent users from requesting interactive nodes and then not using them, there is an inactivity timeout set up. If there is no command running on a node for an hour and a half, the job will be terminated. Otherwise the interactive job is terminated when the user types exit or the allocated time runs out.For more fine grained control over the interactive environment you can use the  srun  command. Issue the  srun  command from a login node. Command syntax is:$ srun --pty -p queue -t hh:mm:ss -n tasks -N nodes /bin/bash -l                Option       Value                       -p       queue (partition)                 -t       maximum runtime                 -n       number of cores                 -N       number of nodes         The following example commands illustrate an interactive session where the user requests 1 hour in the short queue, using 1 compute node and 20 logical cores (half of the cores available on the original compute node), using the bash shell, followed by a BLAST search of a protein database.Start the interactive session:$ srun --pty -p short -t 01:00:00 -n 20 -N 1 /bin/bash -l Load NCBI-BLAST+ on the compute node:$ module load blast+ Uncompress the nr.gz FASTA file that contains your sequence database:$ gzip -d nr.gz Generate the blast database:$ makeblastdb -in nr -dbtype prot Search the nr database in serial mode with a set of queries in the FASTA file blastInputs.fa:$ blastp -db nr -query blastInputs.fa -out blastout Return to the login node:$ exit Requesting the Proper Number of Nodes and CoresSLURM allows you to precisely choose the allocation of compute cores across nodes. Below are a number of examples that show different ways to allocate an 8 core job across the Ceres cluster               salloc/srun/sbatch  options       core distribution across nodes                       -n 8       pick any available cores across the cluster (may be on several nodes or not)                 -n 8 -N 8       spread 8 cores across 8 distinct nodes (i.e. one core per node)                 -n 8 --ntasks-per-node=1       same as  -n 8 -N 8                 -n 8 -N 4       request 8 cores on 4 nodes (however the spread might be uneven, i.e. one node could end up with 5 cores and one core each for the remaining 3 nodes)                 -n 8 --ntasks-per-node=2       request 8 cores on 4 nodes with 2 cores per node                 -n 8 -N 1       request 8 cores on a single node                 -n 8 --ntasks-per-node=8       same as  -n 8 -N 1         Batch ModeSerial JobJobs can be submitted to various partitions or queues using SLURM’s sbatch command. The following is an example of how to run a blastp serial job using a job script named “blastSerialJob.sh”. The content of blastSerialJob.sh is as follows:#!/bin/bash #SBATCH --job-name=blastp   #name of this job #SBATCH -p short              #name of the partition (queue) you are submitting to #SBATCH -N 1                  #number of nodes in this job #SBATCH -n 40                 #number of cores/tasks in this job, you get all 20 physical cores with 2 threads per core with hyper-threading #SBATCH -t 01:00:00           #time allocated for this job hours:mins:seconds #SBATCH --mail-user=emailAddress   #enter your email address to receive emails #SBATCH --mail-type=BEGIN,END,FAIL #will receive an email when job starts, ends or fails #SBATCH -o stdout.%j.%N     # standard output, %j adds job number to output file name and %N adds the node name #SBATCH -e stderr.%j.%N     #optional, prints our standard error date                          #optional, prints out timestamp at the start of the job in stdout file module load blast+            #loading latest NCBI BLAST+ module blastp -db nr -query blastInputs -out blastout  # protein blast search against nr database date                          #optional, prints out timestamp when the job ends #End of file Launch the job like this:$ sbatch blastSerialJob.sh Running a Simple OpenMP JobThe following example will demonstrate how to use threads. We will use the following OpenMP C code to print “hello world” on each thread. First copy and paste this code into a file, e.g. “testOpenMP.c”.#include &lt;omp.h&gt; #include &lt;stdio.h&gt; int main(int argc, char* argv[]){  int id;  #pragma omp parallel private(id)   {   id=omp_get_thread_num();   printf(%d: hello world /n,id);  }  return 0; } Now load the gcc module and compile the code :$ module load gcc $ gcc testOpenMP.c -fopenmp -o testOpenMP Now create a batch job script (OMPjob.sh) to test number of threads you requested:#!/bin/bash #SBATCH --job-name=OpenMP #SBATCH -p short #SBATCH -N 1 #SBATCH -n 20 #SBATCH --threads-per-core=1 #SBATCH -t 00:30:00 #SBATCH -o stdout.%j.%N #SET the number of openmp threads export OMP_NUM_THREADS=20 ./testOpenMP # End of file Launch the job using the batch script like this:$ sbatch OMPjob.sh The stdout* file from the above job script should contain 20 lines with “hello world” from each thread.Parallel MPI JobThe following is the example to run Hybrid RAxML which uses both MPI and PTHREADS. It will start 2 MPI processes (one per node) and each process will run 40 threads (one thread per logical core).Create a SLURM script like this (for example, RAxMLjob.sh, but use your own alignment file rather than “align.fasta”):#!/bin/bash #SBATCH --job-name=raxmlMPI #SBATCH -p short #SBATCH -N 2 #SBATCH --ntasks-per-node=40 #SBATCH -t 01:00:00 #SBATCH -o stdout.%j.%N # We requested 2 nodes, 40 logical cores per node for a total of 80 logical cores for this job module load raxml            #loading latest raxml module, which will also load an MPI module mpirun -np 2 raxmlHPC-MPI-AVX -T 40 -n raxmlMPI -f a -x 12345 -p 12345 -m GTRGAMMA -# 100 -s align.fasta # End of file And execute it with sbatch:$ sbatch RAxMLjob.sh Useful SLURM Commands               Command       Description       Example                       squeue       Gives information about jobs       squeue  or  squeue -u jane.webb                 scancel       Stop and remove jobs       scancel &lt;job id&gt;  or  scancel -u jane.webb                 sinfo       Gives information about queues (partitions) or nodes       sinfo  or  sinfo -N -l                 scontrol       Provides more detailed information about jobs, partitions or nodes       scontrol show job &lt;job id&gt;  or  scontrol show partition &lt;partition name&gt;  or  scontrol show nodes                 seff       Provides resource usage report for a finished job       seff &lt;job id&gt;         Local Scratch SpaceAll compute nodes have 1.5 TB of fast local temporary data file storage space supported by SSDs. This local scratch space is significantly faster and supports more input/output operations per second (IOPS) than the mounted filesystems on which the home and project directories reside. A job sets up a unique local space accessible available only with the job script via the environmental $TMPDIR variable. You can use this for any scratch space disk space you need, or if you plan to compute on an existing large data set (such as a sequence assembly job) it might be beneficial to copy all your input data to this space at the beginning of your job, and then do all your computation on $TMPDIR. You must copy any output data you need to keep back to permanent storage before the job ends, since $TMPDIR will be erased upon job exit. The following example shows how to copy data in, and then run from $TMPDIR.#!/bin/bash #SBATCH --job-name=my sequence assembly   #name of the job submitted #SBATCH -p short              #name of the queue you are submitting job to #SBATCH -N 1                  #number of nodes in this job #SBATCH -n 40                 #number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyper-threading #SBATCH -t 01:00:00           #time allocated for this job hours:mins:seconds #SBATCH --mail-user=emailAddress   #enter your email address to receive emails #SBATCH --mail-type=BEGIN,END,FAIL #will receive an email when job starts, ends or fails #SBATCH -o stdout.%j.%N     # standard out %j adds job number to output file name and %N adds the node name #SBATCH -e stderr.%j.%N     #optional, it prints out standard error  # start staging data to the job temporary directory in $TMPDIR MYDIR=`pwd` /bin/cp –r $MYDIR $TMPDIR/ cd $TMPDIR   # add regular job commands like module load and running scientific software   # copy output data off of local scratch /bin/cp -r output $MYDIR/output   # If you do not know the output names, you can issue: #   rsync –a $TMPDIR/*  $MYDIR/ # which will only copy back new or changed files, but rsync takes longer.   #End of file "
},

    
      
      
      
      
      
    
{
  "title" : "Singularity Containers",
  "category" : "User Guides",
  "subcategory" : "Software",
  "set" : "",
  "url" : "/guides/software/singularity",
  "description" : "A guide for building and running containers on Ceres",
  "date" : "Mar 06, 2023",
  "content" : "Some software packages may not be available for the version of Linux running on the HPC cluster. In this case, users may want to run containers. Containers are self-contained application execution environments that contain all necessary software to run an application or workflow, so users don’t need to worry about installing all the dependencies. There are many pre-built container images for scientific applications available for download and use.Singularity https://sylabs.io/ is an application for running containers on an HPC cluster. Containers are self-contained application execution environments that contain all necessary software to run an application or workflow, so you don’t need to worry about installing all the dependencies. There are many pre-built container images for scientific applications available for download and use, see section Container Images. PrerequisitesTo run containers on Ceres, you’ll need to execute the singularity command from a compute node. For example, to run an interactive session on a compute node, use the SLURM salloc command:[user.name@ceres ~]$ salloc salloc: Granted job allocation 1695904 salloc: Waiting for resource configuration salloc: Nodes ceres20-compute-44 are ready for job export TMPDIR=/local/bgfs//1695904 export TMOUT=5400 [user.name@ceres20-compute-44 ]$ module load singularity [user.name@ceres20-compute-44 ]$ NOTE: salloc by default runs on a single hyper-threaded core (2 logical cores) with 6000 MB of allocated memory on one of the compute nodes. The session will last for 2 days, but will timeout after 1.5 hours of inactivity (no commands runnning). See the Running Jobs User Guide for more info on how to request resources for interactive jobs.Container ImagesSingularity executes a container from a Singularity container image either created by the user or downloaded from Singularity Library https://cloud.sylabs.io/library, and can also import and execute Docker https://www.docker.com/ container images, either directly uploaded by the user, or downloaded from Docker Hub https://hub.docker.com/.BioContainers is a “community-driven project that provides the infrastructure and basic guidelines to create, manage and distribute Bioinformatics [Docker] containers with special focus in Proteomics, Genomics, Transcriptomics and Metabolomics.”BioContainers can be obtained either via docker https://hub.docker.com/u/biocontainers/ or via Quay https://quay.io/For leveraging GPU using containers, Nvidia provides a container library NGC. https://ngc.nvidia.com/catalog/all   NGC offers a comprehensive catalog of GPU-accelerated software for deep learning, machine learning, and HPC. NGC containers deliver powerful and easy-to-deploy software proven to deliver the fastest results. By taking care of the plumbing, NGC enables users to focus on building lean models, producing optimal solutions and gathering faster insights. Docker ImagesWhile Singularity can only execute containers from Singularity images, it can easily import Docker images directly from Docker Hub to create a Singularity image. For example, to download a Docker image of the R programming language from Docker Hub [https://hub.docker.com//r-base/](https://hub.docker.com//r-base/) and import it into a Singularity image, change the docker pull rbase command listed at the aforementioned URL to the equivalent Singularity command  singularity pull docker://r-base  (where specifying “docker://” before the image name lets Singularity know the image is a Docker image, and by default fetch the image from Docker Hub):[user.name@sceres20-compute-44 ~]$ singularity pull docker://r-base INFO:    Converting OCI blobs to SIF format INFO:    Starting build... Getting image source signatures Copying blob 1fcd5305bc72 done  ... INFO:    Creating SIF file... INFO:    Build complete: r-base_latest.sif   The resulting singularity image (r-base.img) contains a complete environment (operating system, libraries, R installation) for running R. A tag may be specified when selecting the Docker image to download; e.g., a list of tags for r-base is at: https://hub.docker.com/r/library/r-base/tags/. To specify a specific tag, append “:TAG” to the image name (e.g.,  singularity pull docker://r-base:3.3.3 ). If the tag is omitted, Singularity will look for an image labeled with the “latest” tag (note that the “latest” tag is merely a Docker Hub convention, and is not guaranteed to exist, nor is it guaranteed to point to the latest image—when in doubt, specify the tag).Note on Home directory and SingularityWhile pulling/building the containers, pay attention to the home directory as the cached image blobs will be saved in ${HOME}/.singularity . Since the home directory has a limited amount of space, this can fill up quite easily. Users can change where the files will be cached by setting SINGULARITY_CACHEDIR and SINGULARITY_TMPDIR environment variables. On Ceres we set SINGULARITY_TMPDIR to $TMPDIR. We also set SINGULARITY_CACHEDIR to $TMPDIR for all Slurm jobs if it’s not set by the user. As of January 13, 2022, these variables are not automatically set on Atlas. We recommend adding the following two commands to the job scripts that use singularity:export SINGULARITY_CACHEDIR=$TMPDIR  export SINGULARITY_TMPDIR=$TMPDIR One can also use –disable-cache option to avoid saving cached image blobs (e.g., singularity pull --disable-cache docker://r-base:3.3.3 ).In case the home directory is full, it is safe to delete the contents of ~/.singularity folder.Singularity ImagesSingularity LibrarySingularity Library is a service that builds Singularity images from community-provided recipes (called “bootstrap files”) stored in GitHub repositories and makes them available for search and download. To download a singularity image, search the Singularity Library container collections, and follow the instructions to download and execute the container. This is similar to the directions for container images from Docker Hub, except replace the “docker://” URI prefix with “library://”.Creating Your Own Singularity ImagesRoot access is needed to create a Singularity image from a bootstrap file. As Ceres users do not have root access, to create your own Singularity image, you can paste the bootstrap file to Singualrity cloud builder https://cloud.sylabs.io/builder.Bootstrap examples - https://github.com/sylabs/singularity/tree/master/examplesExecuting ContainersThree commands may be used to execute applications inside Singularity containers:singularity run,singularity exec,and (less commonly) singularity shellNote that applications executing within a container do not have access to applications installed outside of the container (e.g., environment modules, or executables installed in /usr/bin on the compute node).singularity runSingularity images may have a run script (Docker: ENTRYPOINT) that is executed when the singularity run command is used. To see the run script defined by the container (if any), use the singularity inspect command:[user.name@ceres20-compute-44 ~]$ singularity inspect --runscript r-base.img #!/bin/sh exec R $@ The output above shows that in the aforementioned Docker image for the R programming language retrieved from Docker Hub,  singularity run r-base.img  will run R interactively in the container environment. Your home directory is automatically mounted in the container, so any files therein can be accessed (and R packages installed within the container, as illustrated in the example below):[user.name@ceres20-compute-44 ~]$ singularity run r-base.img R version 3.4.0 (2017-04-21) -- You Stupid Darkness Copyright (C) 2017 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) ... &gt; install.packages(stringr) Installing package into '/usr/local/lib/R/site-library' (as 'lib' is unspecified) Warning in install.packages(stringr) : 'lib = /usr/local/lib/R/site-library' is not writable Would you like to use a personal library instead? (y/n) y Would you like to create a personal library /home/user.name/R/x86_64-pc-linux-gnu-library/3.4 to install packages into? (y/n) y trying URL 'https://cran.rstudio.com/src/contrib/stringr_1.2.0.tar.gz' Content type 'application/x-gzip' length 94095 bytes (91 KB) ================================================== downloaded 91 KB * installing *source* package 'stringr' ... ... * DONE (stringr) The downloaded source packages are in '/tmp/RtmpwP5YNn/downloaded_packages' &gt; library(stringr) &gt; singularity execA container image can contain many executables / scripts. The singularity exec command can be used to select which program to run in the container. For example, to run a simple R script using the Rscript command in the container, prefix the Rscript command with  singularity exec r-base.img:[user.name@ceres20-compute-44 ~]$ cat test.R summary(c(1:10)) [user.name@ceres20-compute-44 ~]$ singularity exec r-base.img Rscript test.R Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 3.25 5.50 5.50 7.75 10.00 singularity shellsingularity shell starts an interactive shell within the container image, allowing you to inspect files (and execute programs) within the container. For example, here we see that the r-base.img container image was created using Debian Linux as the base operating system:[user.name@ceres20-compute-44 ~]$ type R bash: type: R: not found [user.name@ceres20-compute-44 ~]$ singularity shell r-base.img Singularity: Invoking an interactive shell within container... Singularity r-base.img:~&gt; type R R is /usr/bin/R Singularity r-base.img:~&gt; cat /etc/os-release PRETTY_NAME=Debian GNU/Linux 9 (stretch) NAME=Debian GNU/Linux VERSION_ID=9 VERSION=9 (stretch) ID=debian HOME_URL=https://www.debian.org/ SUPPORT_URL=https://www.debian.org/support BUG_REPORT_URL=https://bugs.debian.org/ Singularity r-base.img:~&gt; exit exit [user.name@ceres20-compute-44 ~]$ Ceres Container RepositoryThere are some containers locally available on Ceres at /reference/containersThese are available via modules, so the user doesn’t have to perform any additional tasks.List of containers (may be out of date, ls /reference/containers on Ceres for the most up to date info):   RStudio   agbase_interproscan   antismash   bamm_groopm_checkm_refinem   bigscape   braker   cDNA_cupcake   cactus   cnvnator   combine_gafs   concoct   dastool   envi-jupyter   faststructure   funannotate   gatk   goanna   intarna   itasser   jupyter_notebook   kobas   metawrap   minimac4   nanopolish   nanopolish_500kbp   opendronemap   pb_assembly   plasflow   python36   qiime   qiime2   redundans   revbayes   rgi   roary   salmon   salsa   toil_vg   trinityrnaseq SupportThe SCINet Virtual Research Support Core (VRSC) can provide support for the Singularity application itself, but has no control over the contents of and cannot be expected to support container images.Questions about application-specific container images may be directed to the SCINet community via the SCINet Forum (must have a SCINet account to access), or to the community responsible for maintaining the container image."
},

    
      
      
      
      
      
    
{
  "title" : "SMRTLink/SMRTAnalysis using Command Line",
  "category" : "User Guides",
  "subcategory" : "Analysis",
  "set" : "",
  "url" : "/guides/analysis/smrtlink",
  "description" : "Guide to use SMRTLink via CLI",
  "date" : "Mar 06, 2023",
  "content" : "Instructions for SMRTLink v7Although SMRTLink GUI is useful, it can be very limited. In particular, currently one can not use priority nodes or high memory nodes through SMRTLink GUI. GUI service may also be unavailable at times. This, however, does not affect SMRTLink command line which does not need the GUI service to be alive.View the available pipelinesmodule load smrtlink/7.0.0 pbsmrtpipe show-templates ******************************************************** 21 Registered User Pipelines (name -&gt; version, id, tags) ********************************************************   1. Assembly (HGAP 4)                                        0.2.1    pbsmrtpipe.pipelines.polished_falcon_fat       denovo   2. Base Modification Detection                              0.1.0    pbsmrtpipe.pipelines.ds_modification_detection       modification-detection   3. Base Modification and Motif Analysis                     0.1.0    pbsmrtpipe.pipelines.ds_modification_motif_analysis           motif-analysis   4. CCS with Mapping                                         0.1.0    pbsmrtpipe.pipelines.sl_subreads_to_ccs_align       ccs,mapping   5. Circular Consensus Sequences (CCS)                       0.2.0    pbsmrtpipe.pipelines.sl_subreads_to_ccs       ccs   6. Convert BAM to FASTX                                     0.1.0    pbsmrtpipe.pipelines.sa3_ds_subreads_to_fastx       converters   7. Convert RS to BAM                                        0.1.0    pbsmrtpipe.pipelines.sa3_hdfsubread_to_subread       converters   8. Demultiplex Barcodes                                     0.1.0    pbsmrtpipe.pipelines.sl_ccs_barcode       barcode,ccs   9. Demultiplex Barcodes                                     0.1.0    pbsmrtpipe.pipelines.sa3_ds_barcode2_manual       barcode  10. Iso-Seq                                                  0.1.1    pbsmrtpipe.pipelines.sa3_ds_isoseq3       ccs,isoseq  11. Iso-Seq Classify Only                                    0.1.1    pbsmrtpipe.pipelines.sa3_ds_isoseq3_classify       ccs,isoseq  12. Iso-Seq with Mapping                                     0.1.0    pbsmrtpipe.pipelines.sa3_ds_isoseq3_with_genome       ccs,isoseq  13. Long Amplicon Analysis (LAA)                             0.2.0    pbsmrtpipe.pipelines.sa3_ds_laa       laa  14. Long Amplicon Analysis with Guided Clustering (LAAgc)    0.1.0    pbsmrtpipe.pipelines.sa3_ds_laagc       alpha,laa  15. Mapping                                                  0.1.0    pbsmrtpipe.pipelines.sl_align_ccs       ccs,mapping  16. Minor Variants Analysis                                  0.2.0    pbsmrtpipe.pipelines.sl_minorseq_ccs       minorvariants  17. Minor Variants Analysis                                  0.2.0    pbsmrtpipe.pipelines.sa3_ds_minorseq       beta,minorvariants  18. Resequencing                                             0.2.0    pbsmrtpipe.pipelines.sl_resequencing2       consensus,mapping,reports  19. Site Acceptance Test (SAT)                               0.1.0    pbsmrtpipe.pipelines.sa3_sat       consensus,mapping,reports,sat  20. Structural Variant Calling                               2.0.0    pbsmrtpipe.pipelines.sa3_ds_sv2_ccs       ccs,sv  21. Structural Variant Calling                               2.0.0    pbsmrtpipe.pipelines.sa3_ds_sv2       sv Run with --show-all to display (unsupported) developer/internal pipelines Generate a template file for pipelinespbsmrtpipe show-template-details &lt;pipeline ID&gt; -j &lt;filename&gt;.json Example below shows how to generate a template file for HGAP4 assembly. The generated JSON file will contain options in the form of “KEY”: “VALUE“ that users could edit.pbsmrtpipe show-template-details pbsmrtpipe.pipelines.polished_falcon_fat / -j HGAP4-template.json Check available entry points (inputs) for the pipelinepbsmrtpipe show-template-details pbsmrtpipe.pipelines.polished_falcon_fat | grep -i entry **** Entry points (1) **** $entry:eid_subread The value next to the “Entry points“ indicates the number of entry points. In this case, the entry point is a subreadset file. If a subreadset file does not exist or if there multiple subreadset files that you want to combine, click here for instructions to create datasets.Generate workflow templateThis template is used to talk to SLURM workload manager to divvy up taskspbsmrtpipe show-workflow-options -j workflow-template.json To submit jobs to a different partition replace the value for the pbsmrtpipe.options.cluster_manager KEY in the generated workflow template file with one of the provided options as shown below:pbsmrtpipe.options.cluster_manager: /system/smrtanalysis/7/slurm_template/short pbsmrtpipe.options.cluster_manager: /system/smrtanalysis/7/slurm_template/medium pbsmrtpipe.options.cluster_manager: /system/smrtanalysis/7/slurm_template/mem Users can also copy the template directory and modify values to specify a different partition. For example, to submit jobs to priority partition, in your copy of the template directory make the following changes in file jmsenv_00.ish :JMSCONFIG_SLURM_PARTITION=priority;   # Partition  JMSCONFIG_SLURM_START_ARGS='--qos=gbru --timelimit=7-00:00:00';  # gbru is a example, choose relevant QOS in file start.tmpl :--jmsenv &lt;path_to_your_template_directory&gt;/jmsenv_00.ish # Change the path to your custom template file After these changes, make sure “pbsmrtpipe.options.cluster_manager”: in the workflow template file points to your template directory.   Note that only research groups that purchased nodes on the Ceres cluster have access to priority partitions. Generating a datasetRaw BAM files are usually accompanied by several XML files. In case the users don’t have these XML files, they can use the following command:dataset create This command takes BAM, file of file names (fofn) or XML files as input.For example, to analyze multiple XML SubreadSet files together, issue:dataset create xyz123-combined.subreadset.xml *.subreadset.xml The following types are supported - HdfSubreadSet, TranscriptAlignmentSet, ContigSet, DataSet,ConsensusReadSet, TranscriptSet, BarcodeSet, ReferenceSet, ConsensusAlignmentSet, GmapReferenceSet, AlignmentSet, SubreadSetSample sbatch scriptAfter creating datasets, generating templates and making necessary changes, submit a batch job using sbatch command. Below is a sample job script to submit HGAP4 assembly using smrklink v7#!/bin/bash  #SBATCH --job-name=HGAP4_assembly #SBATCH --cpus-per-task=1 #SBATCH --ntasks=2 #SBATCH --mem-per-cpu=4000 #SBATCH --partition=long #SBATCH --output=HGAP4__%j.std #SBATCH --error=HGAP4__%j.err  module load smrtlink/7.0.0 export HOME=/home/${USER}  pbsmrtpipe pipeline-id pbsmrtpipe.pipelines.polished_falcon_fat -e eid_subread:HGAP4-subreadset.xml / --preset-json HGAP4-template.json --preset-json workflow-template.json / --output-dir /path/to/output/dir  ### Commands to get the above information ###  # Pipeline id - pbsmrtpipe show-templates # Create dataset - dataset create HGAP4-subreadset.xml *.subreadset.xml # Generate template for assembly - pbsmrtpipe show-template-details pbsmrtpipe.pipelines.polished_falcon_fat -j HGAP4-template.json # Generate workflow template - pbsmrtpipe show-workflow-options -j workflow-template.json # To check the available entry points - pbsmrtpipe show-template-details pbsmrtpipe.pipelines.polished_falcon_fat  #**** Pipeline Summary **** #id            : pbsmrtpipe.pipelines.polished_falcon_fat #version       : 0.2.1 #name          : Assembly (HGAP 4) #Tags       : denovo #Description: # Same as polished_falcon_lean, but with reports.  #**** Entry points (1) **** #$entry:eid_subread Instructions for SMRTLink v10Unlike v7 of SMRTLink, v10 uses Cromwell workflow manager which offers additional flexibility and compatibility with SLURM. Commandline version of v10 does not depend on web GUI service and is always available.There are two main steps involved - provide input parameters for your workflow and then submit the job via SLURM.View the available workflowsmodule load smrtlink/10.0.0 $ pbcromwell show-workflows   cromwell.workflows.pb_hgap4: Assembly (HGAP4) cromwell.workflows.pb_basemods: Base Modification Analysis cromwell.workflows.pb_ccs_demux: CCS with Demultiplexing cromwell.workflows.pb_ccs_mapping: CCS with Mapping cromwell.workflows.pb_ccs: Circular Consensus Sequencing (CCS) cromwell.workflows.pb_bam2fastx: Convert BAM to FASTX cromwell.workflows.pb_demux_subreads: Demultiplex Barcodes cromwell.workflows.pb_demux_ccs: Demultiplex Barcodes cromwell.workflows.pb_export_ccs: Export Reads cromwell.workflows.pb_assembly_hifi: Genome Assembly cromwell.workflows.pb_isoseq3_ccsonly: Iso-Seq Analysis cromwell.workflows.pb_isoseq3: Iso-Seq Analysis cromwell.workflows.pb_laa: Long Amplicon Analysis (LAA) cromwell.workflows.pb_align_subreads: Mapping cromwell.workflows.pb_align_ccs: Mapping cromwell.workflows.pb_mark_duplicates: Mark PCR Duplicates cromwell.workflows.pb_assembly_microbial: Microbial Assembly cromwell.workflows.pb_mv_ccs: Minor Variants Analysis cromwell.workflows.pb_resequencing: Resequencing cromwell.workflows.pb_sat: Site Acceptance Test (SAT) cromwell.workflows.pb_sv_ccs: Structural Variant Calling cromwell.workflows.pb_sv_clr: Structural Variant Calling cromwell.workflows.pb_trim_adapters: Trim gDNA Amplification Adapters View input options for a workflowUsing Genome Assembly as an example -$ pbcromwell show-workflow-details pb_assembly_hifi   Workflow Summary Workflow Id    : cromwell.workflows.pb_assembly_hifi Name           : Genome Assembly Description    : Genome assembly at any scale using HiFi reads Required Inputs: Optional Inputs: ConsensusReadSet XML Tags           : ccs, assembly, cromwell Task Options:   reads = None     Reads (file)   ipa2_genome_size = 0     Genome Length (integer)   ipa2_downsampled_coverage = 0     Downsampled coverage (integer)   ipa2_advanced_options =     Advanced Assembly Options (string)   ipa2_run_polishing = True     Run polishing (boolean)   ipa2_run_phasing = True     Run phasing (boolean)   ipa2_run_purge_dups = True     Purge duplicate contigs from the assembly (boolean)   ipa2_ctg_prefix = ctg.     Ipa2 ctg prefix (string)   ipa2_reads_db_prefix = reads     Ipa2 reads db prefix (string)   ipa2_cleanup_intermediate_files = True     Cleanup intermediate files (boolean)   dataset_filters =     Filters to Add to the Data Set (string)   filter_min_qv = 20     Min. CCS Predicted Accuracy (Phred Scale) (integer)   Example Usage:    $ pbcromwell run pb_assembly_hifi /    $ pbcromwell run pb_assembly_hifi /       -e input1.consensusreadset.xml /       --task-option reads=None /       --task-option ipa2_genome_size=0 /       --task-option ipa2_downsampled_coverage=0 /       --task-option ipa2_advanced_options= /       --task-option ipa2_run_polishing=True /       --task-option ipa2_run_phasing=True /       --task-option ipa2_run_purge_dups=True /       --task-option ipa2_ctg_prefix=ctg. /       --task-option ipa2_reads_db_prefix=reads /       --task-option ipa2_cleanup_intermediate_files=True /       --task-option dataset_filters= /       --task-option filter_min_qv=20 /       --config cromwell.conf /       --nproc 8 Use cromwell config files for CeresAs shown above, the pbcromwell run command requires a cromwell config file for the jobs to be submitted via SLURM. On ceres, the config files are avaiable in a central location. Users can point to the files directly or can copy and modify based on their individual requirements. The config files are located at/system/smrtanalysis/10/slurm_template/cromwell-slurm-short.conf /system/smrtanalysis/10/slurm_template/cromwell-slurm-medium.conf /system/smrtanalysis/10/slurm_template/cromwell-slurm-mem.conf The file names correspond to the partitions the jobs will be submitted to.Priority users can copy those files to their work directory and modify the following (lines 130-131)        runtime-attributes =          Int cpu = 8         Int requested_memory_mb_per_core = 8000         String queue_name = short         String? jms_args          to        runtime-attributes =          Int cpu = 8         Int requested_memory_mb_per_core = 8000         String queue_name = priority         String? jms_args = --qos=your_QOS --time=14:00:00           Users can also modify the CPU threads or memory per core values but these default values should suffice for most workflows.Sample batch script#!/bin/bash  #SBATCH -N 1 # No. of nodes used #SBATCH -n 4      # Threads  #SBATCH -t 240    # Minutes  module load smrtlink/10  pbcromwell run pb_assembly_hifi /       -e input1.consensusreadset.xml /       --task-option reads=None /                       # Task options vary based on the workflow       --task-option ipa2_genome_size=0 /               # These task options are optional and will use default values if not specified       --task-option ipa2_downsampled_coverage=0 /       --task-option ipa2_advanced_options= /       --task-option ipa2_run_polishing=True /       --task-option ipa2_run_phasing=True /       --task-option ipa2_run_purge_dups=True /       --task-option ipa2_ctg_prefix=ctg. /       --task-option ipa2_reads_db_prefix=reads /       --task-option ipa2_cleanup_intermediate_files=True /       --task-option dataset_filters= /       --task-option filter_min_qv=20 /       --config /system/smrtanalysis/10/slurm_template/cromwell-slurm-short.conf /       --nproc 8 /                                     # this option is required for some stages in the pipeline       --backend SLURM /                               # Set the default backend       --tmp-dir /${TMPDIR} /                          # Use TMPDIR variable       -c 8 /                                          # Number of chunks       --output-dir hifi-out      #  "
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Storage",
  "category" : "User Guides",
  "subcategory" : "Data",
  "set" : "",
  "url" : "/guides/data/storage",
  "description" : "Guide to storage options on SCINet HPC clusters",
  "date" : "Mar 06, 2023",
  "content" : "This document provides detailed information about the storage options provided by SCINet and how to use them.  For a simpler overview of suggested procedures for managing data on SCINet, please see Managing Data on ARS HPC and Storage Infrastructure.There are multiple places to store data on the Ceres and Atlas clusters that all serve different purposes.QuotasHome directories, project directories in /project and on Juno Archive Storage have quotas. Home directories have 10GB quota. The default project directory quota in /project is set to 1TB. Note that quotas for project directories on Ceres and Atlas may differ. To see the current usage and quotas for your home and project directories on Ceres, as well as on Juno, issue the my_quotas command on the Ceres login node. On Atlas, issue “/apps/bin/reportFSUsage -p proj1,proj2,proj3”, substituting proj# with project name(s).Quotas on Ceres are based off file group ownership/association. By default files in a home directory are associated with the user’s primary group that has the same name as the user name, while files in a project directory are associated with the project group (proj-). Sometimes when users move files from one directory to another or rsync files using `-a` or `-g` and `-p`  options, files in the new location will retain group from the old location and setgid bit will not be set. (The setgid bit needs to be set so that new files and directories created in the directory in /project would be associated with the project group.) To avoid this, use `cp` and `rm` instead of `mv`  to move data between home and project directories, and use `-rltoD` rsync options instead  of `-a` or explicitly specify `--no-p --no-g` options.The “beegfs-ctl --getquota --gid &lt;first.last&gt; --cfgFile=/etc/beegfs/beegfs-client-project.conf” command will report usage and quota for the user &lt;first.last&gt;’s primary group in /project. This quota is  intentionally set to a small value. The non-zero usage indicates that there are files associated with the user’s primary group in /project . To set ownership of the files in a directory in /project to the project group and to set the setgid bit, the user can issue the following command:find /project/&lt;project_name&gt;/&lt;dir&gt; -exec chgrp proj-&lt;project_name&gt; {} + -a -type d -exec chmod g+s {} +  Home DirectoriesHome directories are private, they are only accessible to the user and the system administrators. When a user logs  into Ceres or Atlas, they are automatically logged into their home directory /home/firstname.lastname.Home directories have 10GB quotas and are intended to be mainly used for configuration and login files. Computations  should be run from project directories in /90daydata or in /project. Software installs that require a lot of space,  such as conda virtual environments, should be done in /project.Files in home directories are automatically compressed and backed up. Due to backup method used on Ceres, space freed  after deleting files in home directories, becomes available only after 6 days.Project DirectoriesProject directories are usually associated with ARS Research Projects. While it’s possible to run simulations on Ceres or Atlas using only home directories and Large Short-term Storage in /90daydata/shared, it is recommended to request a project directory. Having a project directory will allow to install software packages in /project and keep important data on Juno Archive Storage.To request a new project directory see Request Project Storage. Here is a direct link to the form (eAuthentication required) which includes submitting a Data Management Plan:Request a project directoryDefault quota for /project/&lt;project_name&gt; is set to 1TB. Historically, before Juno Archive Storage has been deployed, large space had routinely been allocated in /project, but going forward, additional space in /project will only be allocated on an as-needed basis. Most users should use /90daydata to run analysis, and the results should be copied to Juno Archive Storage.Many software applications are available on the clusters as modules, however sometimes  users need to install software by themselves. Since home directories have a small quota, it is recommended to install software,  such as Python, Perl, R packages and conda virtual environments in /project/&lt;project_name&gt;.  The Conda Guide provides instructions  on how to install conda virtual environments in /project, while  Guide to Installing R, Python, and Perl Packages has examples of installing packages in a project directory.Directories in /project are not automatically backed up. Data that cannot be easily reproduced should be manually copied to Juno.Project directories are usually shared between group members working on the same project. Each project directory has a manager (usually the PI on the ARS project who requested the project directory). Project manager can give and revoke access to the project directory to other SCINet users either in FreeIPA or via command line on Ceres or Atlas:ipa group-add-member proj-&lt;project_name&gt; --users=&lt;scinet_username&gt; ipa group-remove-member proj-&lt;project_name&gt; --users=&lt;scinet_username&gt; Note: When ssh-ing to the cluster from a connected site, you may need to issue “kinit” command and enter your SCINet password before issuing ipa commands above.After being added to the proj- project group, users will be able to access `/project/` and `/90daydata/` both on Ceres and Atlas, as well as `/LTS/project/` on Juno.If you prefer using GUI, connect to SCINet VPN and go to https://aws-ipa-0.scinet.usda.gov/. Login using your SCINet user name and password (do not add the GA verification code). Then click on “Groups” and search for your project. After clicking on the project group (group-), you will see the list of users in the group. To add new member(s) click on “+Add”, this will open a pop-up window where you can search for the user. After selecting user name click on “&gt;”. Once the user name appears on the right side under “Prospective”, click on “Add” button in the bottom right corner of the pop-up window. To revoke user access, check-mark next to the user name and click on “Delete” button.Large Short-term StorageSince project directories in /project have limited quotas, in most cases computations should be run from /90daydata/&lt;project_name&gt; which does  not have quota. However, files with an access time (atime) older than 90 days will be automatically deleted. This is permanent and the files cannot  be recovered. Just like /project there is no backup for this space. Data that cannot be easily reproduced should be manually copied to Juno./90daydata/shared is open to all users on Ceres and Atlas. Anyone can create a directory in /90daydata/shared and put data which will be readable  by everyone on the system unless file owner limits access using chmod command. Files older than 90 days will be automatically deleted.Warning: If you download archived files, they may contain files with an access date from long ago. This date will still trigger deletion,  so make sure that the files have a new access date. For example, when you untar a .tar or .tgz file, use the -m flag. If you use rsync to  the space, do not use the -a flag, as that preserves date stamps.Temporary Local Node StorageOne can use the storage on the disk drive on each of the compute nodes by reading and writing to $TMPDIR (1.5TB on most compute nodes).  This is temporary storage that can  be used only during the execution of your job. Only processes executing on a node have access to this space.  Multiple jobs running on the  same node share this space, so an individual job may be able to use less than total available space. If all local space is needed for a job,  request the whole node.To use this local storage the following workflow should be used.  These steps may be taken interactively (when salloc’d to a compute node)  or in batch-mode. In batch mode the copy commands below should be added to the job script.   Copy calculation input to the local filesystem, e.g.,     cp /project/&lt;project_name&gt;/&lt;input files&gt; $TMPDIR          where &lt;project_name&gt; is the name of your project directory and &lt;input files&gt; contains the folders/files to be used by your job (to copy the  whole folder use -r option).           Run your code, getting input from files located in $TMPDIR and writing output to $TMPDIR      Copy final results to storage location, e.g.:     cp $TMPDIR/&lt;final results&gt; /project/&lt;project_name&gt;/&lt;final results&gt;         Note that files in $TMPDIR will disappear at the conclusion of your job.  Any data which is not copied out of $TMPDIR cannot be recovered  after your job has finished.This storage is useful for workflows that extensively use disk space reading and writing multiple small files.Juno Archive StorageProject directories are not meant to be used as a data archive. Data that cannot be easily reproduced should be manually backed up to Juno. Juno is a large, multi-petabyte ARS storage device at the National Agricultural Library in Maryland. For instructions on how to transfer data to and from Juno, see Managing Data on ARS HPC and Storage Infrastructure"
},

    
      
      
      
      
      
    
{
  "title" : "SCINet VPN",
  "category" : "User Guides",
  "subcategory" : "Access",
  "set" : "",
  "url" : "/guides/access/vpn",
  "description" : "Guide to the SCINet VPN",
  "date" : "Mar 06, 2023",
  "content" : "AnyConnect VPNThe SCINet OpenConnect VPN server is compatible with the Cisco Anyconnect client if you already have that installed. We have tested this with OSX, Windows, Linux, and Android. Use of the SCINet VPN does require 2 factor authentication. If you have not set that up yet please read these instructions and do so before continuing.First open AnyConnect and enter ocvpn.scinet.usda.gov in the box and click [Connect]Enter your SCINet Username.Enter your password with your GA 6-digit code appended to the end. For example if your password is “qwerty” and your GA app is showing “456321” you would enter:qwerty456321 You should now get the welcome banner. This may change over time from what is shown here.  Click [Accept]You are now connected to the SCINet VPN.On OSX this shows up on the status bar (typically at the top of your screen) like this:In Windows the notifications will appear in the tray (typically at the bottom of your screen) and looks like this:In linux there is no notification icon but you should see this window.OpenConnect VPNThese instructions are for the open source OpenConnect clients from: http://www.infradead.org/openconnect/These instructions are for the GUI versions of the installers. There are command line versions available as well, but those are not covered here.Installation for WindowsDownload the OpenConnect Windows Client from here: https://github.com/openconnect/openconnect-gui/releasesInstallation for UnixDownload and installation instructions for assorted UNIX variants are here: http://www.infradead.org/openconnect/packages.htmlInstallation for OSXThe OpenConnect GUI client for OSX is available via MacPorts. Please follow the links on that site to install MacPorts (This involves installing and configuring Xcode for command line support from the apps store as well) . After you have MacPorts working on your system you need to run:sudo port install openconnect-gui This will take a few minutes.Configuring and running on WindowsOpen the client and click the “Edit connection details” button as shown below.for Gateway enter: and click [Save &amp; Connect]Enter your SCINet UsernameEnter your password with your GA 6-digit code appended to the end. For example if your password is “qwerty” and your GA app is showing “456321” you would enter:qwerty456321 The “Green Lock” indicates that you should now be connected.You can also see the connection status in the system trayConfiguring and running on OSXAfter installation openconnect-gui will appear on your Applications menu under “MacPorts”.However this will not work as the the VPN need to run with elevated privileges.To run open a terminal and use:sudo /Applications/MacPorts/openconnect-gui.app/Contents/MacOS/openconnect-gui After opening the client enter ocvpn.scinet.science and click [Connect]Enter your SCINet username.Enter your password with your GA 6-digit code appended to the end. For example if your password is “qwerty” and your GA app is showing “456321” you would enter:qwerty456321 If you have a “Green Light” You should now be connected."
},


  
    
      
      
      
      
      
    
{
  "title" : "Guidemaker: software for CRISPR based gene function discovery",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2022-03-31-Rivers",
  "description" : "",
  "date" : "Mar 31, 2022",
  "content" : "CRISPR-Cas is a powerful tool for gene editing. CRISPR can also be used to discover gene function by disrupting every gene in the genome and screening millions of cells in parallel. This “CRISPR screen” or “CRISPR pool” technique knocks out (disrupts) the function of thousands of genes in a large pool of cells. Each cell has only a single gene knockout, but the cells collectively have nearly every gene in their genome disrupted. This can be done in any bacterial culture or eukaryotic cell line where a vector can be introduced and cells can be grown in culture. The population of mutated cells is then grown in the presence of a compound like a toxin, drug, or metabolite. After growth, we extract DNA from control and treatment cell populations at the beginning and end of the experiment. The Cas cut sites are amplified and sequenced. These data are used to create a tally of the mutations in the control and treatment populations. Mutations that improve fitness in the presence of the compound are enriched and can be identified statistically. At the end of an experiment, we have a small list of genes that are likely involved in responding to the compound. These genes can be investigated in more detail to work out the exact mechanisms of their effect.Cas enzymes in a CRISPR-Cas system target a specific site in the genome for cleavage by recognizing a Protospacer Adjacent Motif (PAM) site and an adjacent target site in the genome. Each Cas enzyme type recognizes its own PAM sites, for example, SpCas9 recognizes sites in the genome with the nucleotides “NGG”. Cas cleavage is site-specific because it only cuts in the presence of a guide RNA sequence that has a specific ~20-25 bp long sequencing matching a point in the genome next to a PAM site. Designing these guides to target every gene in the genome multiple times is challenging. Every guide RNA needs to target a single site in the genome and needs to be somewhat different from similar guides for specificity. There are also design considerations for how efficient a guide will be at knocking out a gene. Once the complicated design process is complete, the guide oligonucleotides can be commercially synthesized in one large pool. The pooled guide oligonucleotides are then cloned, amplified, and put into an expression vector that allows it to be expressed in a population of cells. There are standard molecular tools to do this in most organisms.One of the major challenges of conducting CRISPR screens is designing the guide RNAs that target specific sites in the genome. This is especially true when dealing with a non-model organism or a less common Cas enzyme system. To address this problem, my group, in collaboration with researchers at the University of Florida, created user-friendly guide design software that can rapidly create guides for any genome or Cas enzyme. A web application version of GuideMaker is hosted at https://guidemaker.app.scinet.usda.gov and as a tool in the Cyverse Discovery Environment. A command-line version of the software is also available at https://guidemaker.org. Under the hood, GuideMaker implements several unique machine learning methods including a powerful method for approximate nearest neighbor search (HNSW) and a model to predict on-target binding originally  designed by Microsoft Research. I hope that my software can bring the power of CRISPR functional gene discovery to more scientists at ARS and I am happy to help ARS scientists get started with the method. SCINet resources were critical to the development of the software. We extensively benchmarked the software using multiple compute nodes on Ceres and Atlas. An article describing the software in detail is in press at the journal GigaScience."
},

    
      
      
      
      
      
    
{
  "title" : "SCINet accelerates development of a big-data driven and inter-disciplinary technology platform for soybean post-genomic research",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2022-01-10-An",
  "description" : "",
  "date" : "Jan 10, 2022",
  "content" : "Photo: We sequenced and analyzed seed transcriptomes of 75 soybean lines representing 90 years of North American soybean breeding history for identifying breeder-selected genes and alleles. Ancestral soybeans (blue dots) of Asia were introduced in North America and used to breed the modern soybean cultivars (red dots) in North AmericaSoybean (Glycine max (L.) Merr.) is a versatile, nutrients-laden, economically invaluable crop with capacity to restore soil fertility through atmospheric nitrogen fixation. It holds great significance in ensuring adequate global nutritional food security and environment-friendly sustainable agriculture. With advances in next-generation sequencing and many other high-throughput technologies, worldwide soybean researchers have generated and made available a massive amount of complex multi-disciplinary datasets. These datasets provide an unprecedented opportunity to discover new traits and their causative genes and alleles, infer gene networks and build effective phenotypic prediction models that are critical for modern soybean improvement.  My laboratory has been working on establishing a big-data driven, interdisciplinary technology platform by consolidating the in-house generated with open-access, diverse and massive amount of data. We have been developing a suite of data analysis, mining tools and strategies for translating the continuously increasing large datasets into improved soybean cultivars.SCINet Note: SCINet, since its inception, has been an integral part of our large-scale data driven research. Overall, SCINet has enabled us to effectively produce more than 50 TB of soybean -omics data for mining. Using SCINet’s high performance computing clusters (HPCs; Ceres and/or Atlas), we consolidated and analyzed over 5,000 whole-genome sequences and 3,920 transcriptomes of soybean, which were generated in our laboratory or were available in the public domain.  This venture resulted in a large-scale, expandable, and user-friendly genomic resource constituting structurally and functionally annotated  32 million single nucleotide polymorphisms (SNPs) and 3.3 million DNA structural variants (SVs) in 1,562 diverse soybean accessions. The transcriptome analyses delineated transcript accumulation of 56,000 soybean genes from 1,438 distinct biological treatments. Collectively, these datasets are of immense utility to research community and thus, a complete annotated SNPs dataset has been released in Ag Data Commons (An et al. 2020) and Soybase. A comprehensive description of the dataset and its versatile use will be published (Zhang et al. under review).SCINet has provided access to pre-installed tools and software along with the capacity to download and install our own user-specific tools, which enables us to effectively develop a suite of new data analysis pipelines/ data-mining strategies. SCINet was also instrumental in our discovery of the genes underlying two large-effect protein and oil QTLs, which soybean researchers attempted to clone in past three decades, and demonstrating their key roles in soybean domestication (Zhang, et al. 2020).References:An Y-qC, Zhang H, Meryer R. 2020. Data from: Development of a versatile resource from 1500 diverse genomes for post-genomics research. Ag Data Commons. doi:10.15482/USDA.ADC/1519167Zhang H, Goettel W, Song Q, Jiang H, Hu Z, Wang ML, An Y-qC. 2020. Selection of GmSWEET39 for oil and protein improvement in soybean. PLOS Genetics 16(11): e1009114. doi:10.1371/journal.pgen.1009114Zhang H, Jiang H, Hu Z, Song Q, An Y. Accepted with monir revisions. Development of a versatile resource from 1500 diverse genomes for post-genomics research. BMC Genomic. bioRxiv: 2020.2011.2016.383950."
},

    
      
      
      
      
      
    
{
  "title" : "Putting flowers on the map: Quantifying spatio-temporal patterns in floral resources for pollinators",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2021-10-13-Kammerer",
  "description" : "",
  "date" : "Oct 13, 2021",
  "content" : "Melanie is an ORISE Fellow within the SCINet Office. Melanie completed her PhD in Ecology at Penn State University where she studied landscape and climate drivers of Wild Bee Communities. She leads the SCINet Pollinator Working Group.Flowers provide critical nectar and pollen resources to support foraging, learning and memory, and reproduction of pollinating insects, but there are very few large-scale maps of floral resources for pollinators. As part of my post-doctoral research supported by SCINet, I am developing a data-driven method to map floral resources at large geographic extents by scaling-up field observations of plant communities. My colleagues and I are integrating previously disparate datasets that describe ‘who, when, where, and how much’ flowering occurs in 23 habitat types in the Northeast USA.Our approach is unique because it generates relatively fine-scale temporal predictions of floral area over large spatial extents, relevant for apicultural, agricultural, and conservation decisions. With this large, integrated dataset on flowering, for a given location in the Northeast, we can estimate monthly (or even weekly) quantities of flowers available for pollinators across a landscape. Some wild-bee species are active for a very short segment of the growing season but forage up to 1-5km from their nests (or more in times of resource scarcity), necessitating a combination of fine temporal and broad spatial data. This work will enable researchers, land owners, and conservation professionals to identify temporal and spatial gaps in available flowers and design pollinator plantings to mitigate resource scarcity (recently highlighted as federal research priority in the 2021 USDA Annual Strategic Pollinator Priorities and Goals Report.SCINet note: Our team used SCINet’s Atlas HPC to create national habitat maps that we could combine with field surveys to map floral area. This workflow involved merging two pre-existing habitat maps (USDA NASS Cropland Data Layer with the LANDFIRE National Vegetation Classification). Using the Atlas ‘big memory’ nodes, we completed the geospatial merge for the conterminous United States in less than one day which will allow us to create annual habitat maps for multiple years (2010-2020 expected output)."
},

    
      
      
      
      
      
    
{
  "title" : "Climate driving change: from dryland agro-ecosystems across the globe to the spread of a livestock disease across the western US",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2021-04-15-Hudson",
  "description" : "",
  "date" : "Apr 15, 2021",
  "content" : "Rising global temperatures have cascading effects on Earth’s agro-ecosystems. To better understand and contextualize these effects, we leveraged the long records of observational and experimental data collected across a suite of sites in the Long-Term Ecological Research (LTER) Network, including 2 Long Term Agroecosystem Research sites. Debra Peters and I organized a large team of researchers from eight dryland sites across a wide geographic and production gradient (shown by images above). Drylands receive low amounts of rainfall, and plants at these sites are limited in how much they can grow by the amount of water available. Warming amplifies water limitations at these sites, and increases the frequency and intensity of wildfires, dust, and flooding events. We highlight and compare the importance of these multiple drivers to primary production with feedbacks to global climate in an upcoming issue in BioScience special. Kellogg Biological Station and Jornada Experimental Range are two sites from this study that are also part of the 18-site Long-Term Agroecosystem Research (LTAR) Network.Climate can also drive the spatial spread of livestock disease, such as Vesicular Stomatitis (VS), that I am studying with the VSV Grand Challenge group, whose multi-disciplinary members include ARS scientists Debra Peters, Luis Rodriguez, Lee Cohnstaedt, Barbara Drolet, Justin Derner, and Emile Elias along with our collaborator from USDA APHIS, Angela Pelzel-McCluskey. The virus, VSV, enters the US from Mexico every few years and then spreads to cover much of the western US with a tendency towards regions that are cooler and wetter the following year. These results are consistent with the characteristics of habitat conditions and life-cycles of the black flies and biting midges insect vectors for this disease. Large-scale modes of climate variability, such as the El Niño Southern Oscillation (ENSO), can drive changes in regional climate to influence the location and timing of VSV across the Mexico border. The SCINet HPC resources allow us to use a large number of datasets (&gt;400) in AI frameworks towards better prediction of the spread of VSV as a model for predictive disease ecology across the US."
},

    
      
      
      
      
      
    
{
  "title" : "Faster, Better Metagenome Analysis: One genome, one contig for metagenome samples sequenced with PacBio HiFi reads",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2021-04-15-Bickhart",
  "description" : "",
  "date" : "Apr 15, 2021",
  "content" : "All plant and animal species must interact with microbes to survive and thrive. Examples of these interactions include the symbiosis of nitrogen-fixing bacteria in plant roots or the presence of beneficial microbes in the gastrointestinal tracts of animals. Even slight changes in microbial systems can have substantial effects on animal and plant productivity.The study of all microbial genomes in a system is called “metagenomics.” Many agriculturally relevant metagenomes are incredibly complex and can have more unique DNA sequences than their plant or animal host. The amount of any one microbial species may vary from sample to sample, making it hard to identify and characterize species that may be present at very low levels. It is also difficult to resolve a “core” genome for each microbial species because the genome is constantly changing due to the transfer of DNA from one microbial species to another,  a phenomenon known as “horizontal gene transfer”. Despite these issues, recent advances in technology have made metagenome assembly and characterization much more practical.Improved length and quality of DNA sequence “reads” provide the best resolution of complex microbial genomes. Recent improvements in a technology known as “circular consensus sequencing” (CCS), by the company PacBio, have resulted in reads that are longer than 5,000 bases and have error rates less than 1% per base content. Previous error rates were 15-17%. While this error correction may not seem substantial, the reliability of CCS reads allows us to confidently detect low-level microbial strains in a sample. Since each read is derived from a single DNA molecule, there are several interesting biological facts about microbial genome structure that we can investigate using CCS technology. For example, we can detect certain types of horizontal gene transfer events that have integrated into microbial genomes on a single-molecule basis. This can include the detection of viruses that infect specific strains of microbes by identifying specific cells where viral genetic information has been integrated into the host genome. All of these analyses are incredibly complex and require sifting through mountains of data, so researchers at ARS need high-performance computing to be able to identify these features within a reasonable timeframe.SCINet Note: The Ceres cluster has been instrumental in our attempts to characterize agricultural metagenomes by providing the computational power necessary for each project. A complicated metagenome can require over 100 billion bases of DNA sequence to be generated in order to identify a large majority of the microbes in the system. These billions of bases of DNA are spread among hundreds of millions of DNA sequence reads, which are akin to puzzle pieces of their original microbial genomes. To recreate the original genome from these pieces, we use computer algorithms to compare them all against each other. Just like a complex jigsaw puzzle, we can identify some pieces that naturally fit together and then stitch them all together into a larger portion of the puzzle. Imagine, though, that you have hundreds of millions of pieces and have no idea how the end result should look! We calculated that if we used just one CPU processor core, it would take more than a year for our algorithm to assemble the final metagenome from one of our samples. Using the distributed computing power of the Ceres cluster, we can reduce that time down to several days, leaving us with more time to analyze the results for interesting biological information. Already, the methods we have developed have been used by the research community and have been recognized with a 2020 Federal Laboratory Consortium for Technology Transfer (FLC) Midwest Regional Award for outstanding research."
},

    
      
      
      
      
      
    
{
  "title" : "SCINet resources for creating livestock reference genomes",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2021-01-10-Rosen",
  "description" : "",
  "date" : "Jan 10, 2021",
  "content" : "As a scientist in USDA-ARS’s Animal Genomics and Improvement Lab, one of our missions is to generate scientific resources used to improve our ability to raise cattle, sheep and goats to their fullest potential. We have sequencing technologies that can be used to sequence the DNA of these animals and generate a blueprint of their genomes. A genome consists of long strings of DNA called chromosomes. The problem we face is that the sequencing technologies can’t sequence full chromosomes, but rather they sequence millions of pieces of DNA or fragments of the chromosome that have to be pieced together like a puzzle using HPCs (high performance computer [HPC] clusters).My main focus is to improve the accuracy and completeness of reference genome that are then used to, for example, search for disease-causing mutations or breed for a beneficial trait like milk production. Our most recent cattle genome that we pieced together had more unbroken chromosomes compared to the more highly fragmented (contigs) previous reference genome for cattle (see above image; Rosen et al. 2020, GigaScience). This more complete genome might have DNA that was missed in earlier assembled genomes and is important to researchers studying large-scale chromosome biology and its role in gene regulation. For example, we know that different breeds possess novel sequences and variation in how the chromosomes are arranged that are difficult or impossible to detect in a single reference genome based upon a single breed (Low et al. 2020, Nature Communications). Multiple assembled genomes are expanding the range of genetic diversity we are able to interrogate. Our ultimate goal is to incorporate all known genetic diversity into a single reference for each species called a pan-genome. This will require significant super computing resources that are only possible thanks to SCINet.SCINet note: Not all genomes are assembled equally. Genome size and complexity have an enormous impact on the computational time required for assembly. The HPCs available through SCINet, named Ceres and Atlas, allow our research group to put together the pieces of DNA using different parameters of the program we use to determine the best possible solution. SCINet is a shared computing resource where users can purchase access to the HPC, but are able to use way more than they could afford for short periods of time. This approach has enabled us to triple our compute capacity without increasing the burden on cost or local IT resources. SCINet also has amazing staff that support the machines, software installation and domain experts as part of the Virtual Research Support Core (VRSC). This in-house ARS capacity is essential because we can now accomplish objectives that would never have been possible before."
},

    
      
      
      
      
      
    
{
  "title" : "Selecting Robust Climate Change Projections for Agricultural Systems",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2021-01-10-Geil",
  "description" : "",
  "date" : "Jan 10, 2021",
  "content" : "Scientists of many disciplines, including agricultural fields, often use climate change projections in their research. For example, these projections have been used to estimate how crops or ecological systems may be impacted by future climate conditions, or to predict the spread of diseases and pests. Many sources of climate change projections exist for these research applications: over 100 different global general circulations models (GCMs) in the Coupled Model Intercomparison Project (CMIP) archives; large ensembles generated from a single GCM (such as NCAR’s CESM large ensemble); dynamically downscaled GCM products that generate higher spatial resolution information using regional climate models (such as CORDEX); and statistically downscaled GCM products that generate higher resolution information using empirical equations (such as the MACA product). Unfortunately, there are no quality standards or model performance thresholds implemented for any of this data. How should a scientist choose the most appropriate and robust source of projections for their particular research application considering the many sources and varied quality of available data?The best practice is to spend time evaluating the performance of climate model simulations using metrics that are relevant to each particular research application and to then avoid the use of climate projections from any model that doesn’t perform well. In other words, a scientist should ensure that a model can simulate, fairly realistically, the phenomenon of interest/impact before using its projections. This applies to “bias-corrected” downscaled products as well as GCMs because 1) a poorly performing model shouldn’t be downscaled in the first place and 2) many model biases exist but only a couple are corrected in the downscaling process.In reality though, this best practice is almost never followed due to the time, computational resources, and evaluation knowledge required for a model performance analysis. Instead, scientists often select one of the many available downscaled GCM products and use a multi-model average of projections, without considering the implications of model bias. For any domain scientist (outside of climate science) this is completely understandable. What ecologist, hydrologist, or rangeland scientist wants to spend months assessing the quality of climate models when they could grab a single climate projection product and instead spend that time focusing on cropland, hydrology, or rangeland science questions? We must work toward a model evaluation solution that produces more robust science while also being much more convenient and understandable for scientists to implement.As part of my SCINet postdoctoral fellowship, I am working with members of the USDA-ARS Vesicular Stomatitis (VS) grand challenge project to determine the most robust climate model projections for predicting changes in the geographic range of the livestock disease, VS, under future climate conditions (forthcoming article in the journal Climate). As part of this Grand Challenge Project, ARS scientists Debra Peters, Luis Rodriguez, Lee Cohnstaedt, Barbara Drolet, Justin Derner, and Emile Elias along with our collaborator from USDA APHIS, Angela Pelzel-McCluskey are developing process-based early warning strategies to predict the spread of vector-borne disease across the US. My research will improve those predictions through a more objective approach to selecting climate data driving the spread of disease. Our experience with this project will have application to other agricultural problems where scientists need to select the climate change model projections for their research.I am a climate scientist trained in climate model evaluation and selection for research and decision making applications. During my time as a postdoc at USDA, I plan to work on a range of projects to evaluate climate model performance and to assist in selection of climate projections for specific research applications. Eventually, I plan to develop a web-hosted tool for selecting robust model projections using the results and knowledge gained from these analyses. For scientists who are currently evaluating model performance, this tool will save countless research hours. For scientists who are not looking at model performance before selecting climate projections, this tool will provide more robust results.I am currently working with the VS Grand Challenge group but am looking for additional collaborations. Please don’t hesitate to contact me if you are interested in collaborating!"
},

    
      
      
      
      
      
    
{
  "title" : "Generating whole-genome data from a single insect: PacBio HiFi genome assembly and HiC scaffolding pipelines for reference quality genomes using SCINet",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2021-01-10-Geib",
  "description" : "",
  "date" : "Jan 10, 2021",
  "content" : "The damage that insect pests cost to human health and agriculture is enormous. For instance, invasive insects cost a minimum of US$70.0 billion per year globally, while associated health costs can exceed US$6.9 billion per year. One method that can help control insect pests is the study of pest insect genomes, because insect genomic data are useful resources for developing alternative and eco‐friendly pest control policies.  Such analyses allow entomologists to discover molecular diversity in insect populations that underlie the causes of pest population outbreaks. But many of the existing insect genome sequences are not of high quality, which means that critical functional data may not be present in these genomes. Until recently, sequencing costs made it impractical to sequence large numbers of genomes, so that the genomes of many insect pests have yet to be sequenced.However, technologies for cheaper, more high-quality genome assemblies are emerging, including the application of PacBio HiFi reads. These single-molecule long reads are processed as circular, consensus sequencing (CCS) reads to generate relatively long single molecule reads (5-20kb+) with exceptionally high accuracy. This technology improves the quality and completeness of genome sequences to capture more contiguous (i.e., not separated by gaps) DNA sequences than older sequencing technologies. Through performing DNA extraction and library preparation in my lab in Hilo, HI, and collaborating with PacBio Sequel II sequencing platforms available within the agency (Stoneville, MS &amp; Clay Center, NE), we generated HiFi data for a number of insect pests as part of the Ag100Pest sequencing initiative.Despite the small physical size and relatively low DNA yield of many insect species, which can inhibit DNA sequencing, we used library preparation methods adapted for low (as low as ~250 ng) and ultralow (as low as 5 ng) DNA quantities, allowing us to generate whole-genome data from single insects. Therefore, despite the low amount of input DNA, we are still able to generate extremely contiguous assemblies that allow us to test several different diploid genome assemblers developed for HiFi reads (HiCanu, HiFiASM, IPA).Even for relatively complex genomes of non-model insects (insects that are not closely related to well studied species), we can build highly accurate assemblies. In some cases, contiguous DNA sequences spanning entire chromosomes or chromosome arms, and accurate resolution of repeat regions including centromere and telomere components have been possible.Another great advantage of this strategy for genome assembly is the extremely short assembly time and relatively low computational requirements to complete the assembly. On a single node of the Ceres system using standard memory requirements, a 500 Mb genome can be assembled in a few hours using the HiFiASM assembler. HiCanu and IPA required somewhat longer compute times, and HiCanu required multiple compute nodes, but all were significantly shorter than previous assemblies with “standard” PacBio CLR data. Additionally, sequencing coverage as low as 20X was able to yield these outcomes, although there is some benefit to sequencing at high coverage, up to 60X was shown to generate improvements in contiguity. By combining these assemblies with HiC data, we have been able to scaffold to chromosome dozens of insect genome, and in many cases, the contiguity of the input HiFi assembly to the HiC scaffolding was already near chromosome scale, and the HiC data serves more to validate the correctness of the assemble, rather than join hundreds of contigs into chromosomes as seen in previous assembly technologies. Through these advances in genome sequencing and assembly methods, hundreds of reference quality assemblies can be performed on Ceres or Atlas each year to meet the needs of researchers across the agency.SCINet Note: This project utilized computational resources on the Ceres HPC. Pipelines for performing pre-assembly filtering, HiFi assembly, post-assembly scaffolding with HiC and final assembly filtering (to remove microbial and mitochondrial components) have been developed by the Ag100Pest assembly team, and now due to the resources available on Ceres, we can go from raw data to assembly in less than a day, and curation to final genome in less than a week (requiring some manual review of the final assembly). This is revolutionary to non-model genomics, and allows expansion of the scope of projects into pan-genome studies, strain level characterizations, and more."
},

    
      
      
      
      
      
    
{
  "title" : "Spatial Disease Modelling on the Ceres HPC",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2020-07-16-Humphreys",
  "description" : "",
  "date" : "Jul 16, 2020",
  "content" : "A fundamental need to meet USDA ARS’s Grand Challenge initiative is to improve agricultural production while reducing the impact of the emerging pests, pathogens, and invasive species that threaten US livestock. Pathogens such as West Nile Virus (WNV), Vesicular Stomatitis Virus (VSV), and others impair livestock health, deplete veterinary resources, and threaten agricultural trade. To better anticipate and prepare for future disease outbreaks caused by viruses, it is essential to model the virus-vector-host interactions and environmental factors that drive disease spread across geographic space and through time. Because disease models must provide high-resolution outputs across expansive geographic extents while simultaneously accounting for the correlations that exist in input variables in both the temporal and spatial dimensions, analyses are often too computationally demanding for traditional hardware and necessitate use of high-performance computing resources like those available through SCINet.As part of the ARS Predictive Disease Ecology Grand Challenge Project led by Drs. Deb Peters and Luis Rodriguez, a spatiotemporal disease model was developed to forecast future West Nile Disease (WND) outbreaks in horses across the continental US. Postdoc Dr. John Humphreys led the analysis to predict the distribution and timing of future WND outbreaks. The Centers for Disease Control and Prevention (CDC) records provided the count of veterinary-reported WND cases for horses between 2000 – 2018 aggregated by county. The team used the USDA National Agricultural Statistics Service database to map horse populations, incorporated CDC mosquito surveillance reports to identify insect vector ranges, and analyzed more than 10 million bird occurrence records from the Cornell Laboratory of Ornithology to map the distributions of avian species known to host WNV. These datasets allowed the model to link the at-risk livestock population (horses) to times and locations with both the WNV reservoirs (birds) and the WNV vectors (mosquitos) that transmit the virus between those reservoirs and livestock. The research team applied a Bayesian hierarchical modeling framework to construct the model and specified that the prediction for any one location be dependent on the disease risk estimated for surrounding areas and past times (manuscript is in preparation).SCINet Note: After 24 hours of processing on a laptop (Intel Core Processor i9-8950HK, 8 Core, 2.9GHz), the model was only approximately 20% completed, with multiple model versions to be run. Conversely, running the model on the Ceres HPC completed in less than 10 hours, dramatically reducing the processing time and freeing up the laptop for other uses. Preparation for running the model on Ceres entailed uploading the existing R script and model input data (using Globus), creating a text file specifying the number of nodes, cores, and memory needed, and then submitting that file to the Ceres job scheduler and management system (Slurm). The team opted to double what was available on their laptop and requested two nodes each with 8 cores. Conveniently, the R package used to run the model (r-INLA) included native multithreading (OpenMP) to handle parallel processing and allowed HPC cores to run concurrently."
},

    
      
      
      
      
      
    
{
  "title" : "The SCINet-LTAR phenology working group: Improving the accuracy of agriculturally-relevant models to meet growing and changing food demand",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2020-04-07-Browning",
  "description" : "",
  "date" : "Apr 07, 2020",
  "content" : "Meeting growing and changing demands for food and agricultural products requires models and forecasts that accurately characterize hydrology-soil-climate interactions and are scalable across diverse production systems. Phenology, or timing of seasonal events (in this case for plants), is the product of the different abiotic drivers and is thus a vital metric to study. Remotely sensed data, such as satellite images, are key to tracking land surface phenology. This project seeks to use well-established phenology metrics to quantify scaling relationships between multiple remote platforms such as eddy-covariance towers, near-surface digital cameras or PhenoCams (see figure), and satellite imagery at diverse U.S. agroecosystem sites as part of the ARS Long-Term Agroecosystem Research (LTAR) network.The working group, led by Dawn Browning, includes ARS researchers as well as multiple University collaborators and was formed in 2019 with an inaugural workshop in Las Cruces, NM to devise collaborative workflows on the SCINet high performance computing (HPC) system, Ceres. The group convenes monthly for teleconferences led by Dr. Browning and plans to convene an annual workshop for the length of the project. The first presentation of the group’s research was presented at the 2019 AGU Fall Meeting in a talk titled “Phenology in U.S. agroecosystems – Integrating data from the Long-Term Agroecosystem Research (LTAR) Network” and on-going collaborative efforts are underway.Currently, the group is evaluating the performance of a coupled vegetation-hydrological model that uses PhenoCam data as a proxy for primary productivity. The first formal report from this project effort will be submitted for peer review this summer.SCINet Note: We use CERES to build models to forecast future trends in grassland productivity. Model building is an extremely resource intensive operation. We must test millions of combinations of parameters affecting plant, soil, and environmental interactions to identify the best combination to apply to future climate scenarios. With CERES, we can iterate through different models in hours rather than the weeks it would take on desktop computers."
},

    
      
      
      
      
      
    
{
  "title" : "The ARS Arthropod Genomics Research (AGR) working group: Agency-wide advancement in research capabilities to address stakeholder concerns",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2020-04-01-Coates",
  "description" : "",
  "date" : "Apr 01, 2020",
  "content" : "Arthropod pests cause damage to crop, livestock and forestry products, as well as ornamental plants and urban structures, that inflicts an economic cost of several billion dollars annually. This occurs not only by direct feeding damage, but also by the spread of disease agents by many arthropods such as mosquitoes and aphids.A group of ARS scientists conducting applied research aimed at developing more effective and sustainable methods of arthropod pest control organized into the Arthropod Genomic Research (AGR) working group in 2014 following two organizational workshops (AGR Workshop 1, AGR Workshop 2) called by NPL, Dr. Kevin Hackett. This working group also played key roles in RNA sequencing (RNA-seq) and population genomics training workshops organized by former CSIO, Dr. Doreen Ware, and a Data Carpentry training in 2018. AGR has monthly teleconferences organized by Dr. Brad Coates featuring invited speakers that highlight advances in genomic research and applications of genomic technologies, which function as a training and learning resource for members. AGR members from NAL, Drs. Monica Poelchau and Chris Childers, are developing arthropod genomic database resources at the i5KWorkspace@NAL, providing a community data resource and manual annotation platform.These efforts by AGR members have culminated in the Ag100Pest Initiative, which was formed as a subgroup of the AGR with the mission to generate reference-quality genome assemblies for 100 of the most economically- and socially-impactful arthropod species, and develop workflows to support ARS and community research. As part of the Ag100Pest Initiative, Dr. Scott Geib led the genome sequencing of the spotted lanternfly, an invasive and highly destructive pest of several fruit, nut and berry producing plants, which was published in GigaScience and highlighted the impact of this intra-agency effort1,2.SCINet Note: The AGR has used several applications on the SCINet HPC, Ceres, for nucleotide sequence (transcriptome and genome) assembly and RNA quantity estimation. We are also participating with the VRSC to beta test recent long read assemblers in support of Ag100Pest Initiative goals.References        Kingan, S.B., Urban, J., Lambert, C.C., Baybayan, P., Childers, A.K., Coates, B., Scheffler, B., Hackett, K., Korlach, J., Geib, S.M. 2019. A high-quality genome assembly from a single, field-collected spotted lanternfly (Lycorma delicatula) using the PacBio Sequel II system. GigaScience, 8(10):giz122, doi.org/10.1093/gigascience/giz122.           Kaplan, Kim. “First Genome of Spotted Lanternfly Built from a Single Insect”. ARS Research News, 16 Oct. 2019, www.ars.usda.gov/news-events/news/research-news/2019/first-genome-of-spotted-lanternfly-built-from-a-single-insect/.    "
},

    
      
      
      
      
      
    
{
  "title" : "Antigenic and genetic evolution of contemporary swine H1 influenza viruses in the US",
  "category" : "News",
  "subcategory" : "Stories",
  "set" : "",
  "url" : "/posts/2019-10-18-Anderson",
  "description" : "",
  "date" : "Oct 18, 2019",
  "content" : "Influenza A viruses (IAV) that circulate in North American pigs maintain a high degree of diversity determined by the hemagglutinin (HA) gene, especially those of the H1 subtype. Genetic diversity is further increased by the bidirectional transmission of IAV between swine and humans and the subsequent processes of antigenic shift and drift. Such evolution can be the basis for changes in antigenic properties of IAV that lead to vaccine failure. Here, we characterized the genetic and antigenic evolution of contemporary swine H1N1 and H1N2 viruses. We compiled a comprehensive dataset of more than 3000 HA genes, and applied Bayesian phylodynamic methods to infer how immune selection, and changes in viral population size, impacted genetic and antigenic diversity. We achieved these analyses using 1-2 dedicated nodes on Ceres, with jobs taking 5-10 days to complete. Our data revealed contrasting patterns of evolution across the known H1 genetic lineages: while the classical swine H1 viruses evolved within established genetic clades, the human-seasonal swine H1 viruses rapidly diversified into three antigenically distinct genetic clades. Further characterization revealed that a few key amino acid mutations were associated with antigenic divergence. The continued genetic and antigenic evolution of contemporary H1 viruses has resulted in vaccines with reduced efficacy, and the observed genetic and antigenic diversity represents a challenge to public health initiatives that attempt to minimize swine-to-human IAV transmission.Ceres Note: We used 1-2 dedicated nodes on Ceres, run times ranged from 5-10 days (i.e., we used medium/long queues). The advantage here is our prior jobs used CIPRES (max run time of 168 hours/7 days).Citation: Rajao, D.S., Anderson, T.K., Kitikoon, P., Stratton, J., Lewis, N.S., Vincent, A.L. 2018. Antigenic and genetic evolution of contemporary swine H1 influenza viruses in the United States. Virology, 518:45-54, https://doi.org/10.1016/j.virol.2018.02.006."
},


  
    
      
      
      
      
      
    
{
  "title" : "USDA-ARS/University of Florida Machine Learning Training",
  "category" : "Trainings and Events",
  "subcategory" : "Tutorials",
  "set" : "",
  "url" : "/tutorials/ars-uf-machine-learning-training",
  "description" : "training content from Adam River's 2019 training",
  "date" : "Mar 06, 2023",
  "content" : ""
},

    
      
      
      
      
      
    
{
  "title" : "SCINet Onboarding (Intro to SCINet)",
  "category" : "Trainings and Events",
  "subcategory" : "Tutorials",
  "set" : "",
  "url" : "/tutorials/ceres-onboarding",
  "description" : "Ceres basics",
  "date" : "Mar 06, 2023",
  "content" : "Users who are new to the HPC environment may benefit from the following Ceres onboarding video:   Ceres Onboarding (Intro to SCINet Ceres HPC) (length 42:13)        Note: /KEEP storage discussed in the video at 16:20 is no longer available.  Instead, data that cannot be easily reproduced should be manually backed up to Juno.  The instructional video at https://www.youtube.com/watch?v=I3lnsCAfx3Q demonstrates how to transfer files between local computer, Ceres, Atlas, and Juno using Globus.    The onboarding video includes:   logging on to Ceres   changing your password   storage structure   basic SLURM job scheduler commands   accessing Ceres software modules   computing in interactive mode with salloc   computing in batch mode with a batch script The information in this video is also documented in the Ceres User Manual."
},

    
      
      
      
      
      
    
{
  "title" : "NEON High Resolution Hyperspectral Data Exercise",
  "category" : "Trainings and Events",
  "subcategory" : "Tutorials",
  "set" : "",
  "url" : "/tutorials/geospatial-analysis-neon-hyperspectral",
  "description" : "Tutorial example from LTAR webinar series HPC for geospatial analysis",
  "date" : "Mar 06, 2023",
  "content" : "Explore NEON AOP hyperspectral data, calculate/visualize spectral indices, unsupervised ML (clustering) Data: NEON AOP hyperspectral data from LTAR CPER Strategy: Load data and visualize at specific points. Calculate and visualize spectral indices. Subset and scale data to develop kmeans model. Fit a Kmeans model with 8 clusters to data (we will impliment two different methods). Predict and visualize entire dataset. Parallel Algorithm Detail For parts 1 - 3 we will rely on Dask to extract, transform, and load (ETL) the hyperspectral data. Parts 4 - 5 we will use Dask-ML wrappers on top of scikit-learn (a well-known machine learning library) classifiers to parallelize computations across the cluster. %pylab inline Populating the interactive namespace from numpy and matplotlib import os os.environ[‘GDAL_DATA’] = ‘/opt/conda/share/gdal’ import dask_jobqueue as jq #import time from dask.distributed import progress,wait,Client import dask.array as da import dask import dask.dataframe as dd import numpy as np import pandas as pd import xarray as xr from sklearn.cluster import MiniBatchKMeans from dask_ml.wrappers import Incremental from dask_ml.wrappers import ParallelPostFit from dask_ml.preprocessing import StandardScaler from dask_ml.cluster import KMeans partition=’short,debug,brief-low,mem’ num_processes = 3 num_threads_per_processes = 20 mem = 3.2num_processesnum_threads_per_processes n_cores_per_job = num_processes*num_threads_per_processes container = ‘/project/geospatial_tutorials/data_science_im_rs_latest.sif’ clust = jq.SLURMCluster(queue=partition, processes=num_processes, cores=n_cores_per_job, memory=str(mem)+’GB’, interface=’ib0’, local_directory=’$TMPDIR’, death_timeout=60, python=’singularity exec –bind /usr/lib64 –bind /scinet01 –bind /software/7/apps/envi/bin/ ‘+container+’ python’, walltime=’00:30:00’) cl=Client(clust) dash_board = cl.scheduler.address.split(‘//’)[1].split(‘:’)[0]+:+str(cl.scheduler_info()[‘services’][‘dashboard’]) ssh_command = ‘ssh -N -L 8787:’+dash_board+’ ‘+os.environ[USER]+’@login.scinet.science’ print(‘To port forward diagnostics use: ‘+ssh_command) cl To port forward diagnostics use: ssh -N -L 8787:10.1.8.20:8787 rowan.gaffney@login.scinet.science Client Scheduler: tcp://10.1.8.20:46840 Dashboard: http://10.1.8.20:8787/statusCluster Workers: 0 Cores: 0 Memory: 0 B clust.scale(n=18) cl Client Scheduler: tcp://10.1.8.20:46840 Dashboard: http://10.1.8.20:8787/statusCluster Workers: 18 Cores: 360 Memory: 1.15 TB Load and Visualize Spectral Data #Function to exclude bands with H2O or CO2 absorption def band_list(): good_bands1 = np.linspace(0,188,189).astype(int) good_bands3 = np.linspace(211,269,269-211+1).astype(int) good_bands5 = np.linspace(316,425,425-316+1).astype(int) good_bands = np.hstack([good_bands1,good_bands3,good_bands5]) bad_bands2 = np.linspace(189,210,210-189+1).astype(int) bad_bands4 = np.linspace(270,315,315-270+1).astype(int) bad_bands = np.hstack([bad_bands2,bad_bands4]) return(good_bands,bad_bands) #Persist the data to memory d_all = xr.open_zarr(‘/project/geospatial_tutorials/tutorial_1/data/neon_2017_mosaic_brdf_corr.zarr’,group=’reflectance’).isel(wl=band_list()[0]).chunk({‘x’:’auto’,’y’:’auto’,’wl’:-1}).persist() d_all Dimensions:      (wl: 358, x: 12050, y: 13092) Coordinates: * wl           (wl) float64 381.3 386.3 391.3 ... 2.5e+03 2.505e+03 2.51e+03 * x            (x) float64 5.166e+05 5.166e+05 ... 5.287e+05 5.287e+05 * y            (y) float64 4.526e+06 4.526e+06 ... 4.513e+06 4.513e+06 Data variables: reflectance  (y, x, wl) int16 dask.array&lt;shape=(13092, 12050, 358), chunksize=(776, 241, 358)&gt; #Plot the spectral signature at point plt.figure(figsize=(8,6)) d_all.reflectance.interp(x=520002.5554,y=4520000.78758,method='linear').plot.line('b.') plt.ylabel('Reflectance * 10000') plt.xlabel('Wavelength (nanometers)') plt.grid() Calculate and visualize spectral indices #Calculate the spectral indices and add to the d_all variable d_all['indices'] = xr.concat([((d_all.reflectance.sel(wl=858.6,method='nearest')-d_all.reflectance.sel(wl=648.2,method='nearest'))/(d_all.reflectance.sel(wl=858.6,method='nearest')+d_all.reflectance.sel(wl=648.2,method='nearest'))).assign_coords(index='ndvi').expand_dims('index'), ((0.5*(d_all.reflectance.sel(wl=2000,method='nearest')/10000.+d_all.reflectance.sel(wl=2200,method='nearest')/10000.))-d_all.reflectance.sel(wl=2100.,method='nearest')/10000.).drop('wl').assign_coords(index='cai').expand_dims('index'), ((xr.ufuncs.log(1./(d_all.reflectance.sel(wl=1754.,method='nearest')/10000.))-xr.ufuncs.log(1./(d_all.reflectance.sel(wl=1680.,method='nearest')/10000.)))/(xr.ufuncs.log(d_all.reflectance.sel(wl=1754.,method='nearest')/10000.)+xr.ufuncs.log(d_all.reflectance.sel(wl=1680,method='nearest')/10000.))).assign_coords(index='ndli').expand_dims('index'), ((d_all.reflectance.sel(wl=750.,method='nearest')-d_all.reflectance.sel(wl=705.,method='nearest'))/(d_all.reflectance.sel(wl=750.,method='nearest')+d_all.reflectance.sel(wl=705.,method='nearest')-(2.*d_all.reflectance.sel(wl=445.,method='nearest')))).drop('wl').assign_coords(index='mrendvi').expand_dims('index'), ((d_all.reflectance.sel(wl=800.,method='nearest')-d_all.reflectance.sel(wl=445.,method='nearest'))/(d_all.reflectance.sel(wl=800.,method='nearest')-d_all.reflectance.sel(wl=680.,method='nearest'))).assign_coords(index='sipi').expand_dims('index'), ((xr.ufuncs.log(10000./d_all.reflectance.sel(wl=1510.,method='nearest'))-xr.ufuncs.log(10000./d_all.reflectance.sel(wl=1680.,method='nearest')))/(xr.ufuncs.log(10000./d_all.reflectance.sel(wl=1510.,method='nearest'))+xr.ufuncs.log(10000./d_all.reflectance.sel(wl=1680.,method='nearest')))).assign_coords(index='ndni').expand_dims('index'), ((1./(d_all.reflectance.sel(wl=510.,method='nearest')/10000.))-(1./(d_all.reflectance.sel(wl=550.,method='nearest')/10000.))).assign_coords(index='cri1').expand_dims('index'), ((1./(d_all.reflectance.sel(wl=510.,method='nearest')/10000.))-(1./(d_all.reflectance.sel(wl=700.,method='nearest')/10000.))).assign_coords(index='cri2').expand_dims('index')],dim='index').chunk((1.,d_all.reflectance.data.chunksize[0],d_all.reflectance.data.chunksize[1])).transpose('y','x','index').chunk(('auto','auto',1)) #persist the data to memory d_all['indices'].data = d_all['indices'].data.persist() #wait for the computation to complete wait(d_all) #show the xarray d_all  Dimensions:      (index: 8, wl: 358, x: 12050, y: 13092) Coordinates: * wl           (wl) float64 381.3 386.3 391.3 ... 2.5e+03 2.505e+03 2.51e+03 * x            (x) float64 5.166e+05 5.166e+05 ... 5.287e+05 5.287e+05 * y            (y) float64 4.526e+06 4.526e+06 ... 4.513e+06 4.513e+06 * index        (index) object 'ndvi' 'cai' 'ndli' ... 'ndni' 'cri1' 'cri2' Data variables: reflectance  (y, x, wl) int16 dask.array&lt;shape=(13092, 12050, 358), chunksize=(776, 241, 358)&gt; indices      (y, x, index) float64 dask.array&lt;shape=(13092, 12050, 8), chunksize=(6984, 2169, 1)&gt; #Plot the Indices fig, axes = plt.subplots(nrows=4,ncols=2,figsize=(24,30)) ax = axes.flatten() #Loop through each indices a plot at a 1/(50*50) resolution i=-1 for ind in d_all.coords['index'].values: print('Plotting '+ind+' ...') i=i+1 d_all.indices.sel(index=ind).where(d_all.reflectance.isel(wl=22)&gt;-10.).isel(x=slice(None,None,50),y=slice(None,None,50)).plot(ax=ax[i]) Plotting ndvi ... Plotting cai ... Plotting ndli ... Plotting mrendvi ... Plotting sipi ... Plotting ndni ... Plotting cri1 ... Plotting cri2 ... Subset and Scale Data #Select an observation in each 5*5 region d_sl = d_all.isel(x=slice(None,None,4),y=slice(None,None,4)) #Reshape the data from 3d (x,y,wavelength) to 2d (x*y,wavelength) wl_col = d_sl.wl.values d_sl = d_sl.reflectance.data.reshape((-1,len(wl_col))).rechunk(('auto',-1)) #Exclude Null Values and convert to regular partitions/blocks (this is somewhat convoluted, but will be streamlined in future versions) d_sl = dd.from_dask_array(d_sl[(~da.any(d_sl==-9999,axis=1))&amp;(~da.any(da.isnan(d_sl),axis=1))],columns=wl_col).reset_index(drop=True).to_dask_array(lengths=True).rechunk(('auto',-1)).persist() #Check for null values print('All the data is finite: '+str(da.all(da.isfinite(d_sl)).compute())) print('There are NaNs in the data: '+str(da.any(da.isnan(d_sl)).compute())) All the data is finite: True There are NaNs in the data: False #Scale the Data with a standard scaler scaler = StandardScaler().fit(d_sl) d_train = scaler.transform(d_sl).rechunk(('auto',-1)).persist() print('There are '+str(d_train.shape[0])+' records in the fit data spread over '+str(d_train.npartitions)+' partitions.') There are 7365717 records in the fit data spread over 158 partitions. Fit a Kmeans model Here we will fit two different implimentations of the KMeans cluster. The first version (est1) uses a parallel implimentation of the KMEANS algorithm. It is an iterative method, so time to convergence is quite variable. The second method (est2) uses a minibatch approach. Each partition fits the model sequentially, and then models results are combined. This approach uses the scikit-learn package, and then is wrapped with dask_ml ParallelPostFit so the resulting model can be applied to the entire dataset (prediction) in a distributed manner. The same methods can apply to any scikit-learn model that impliments a partial fit method. #Define the estimator (from dask-learn library - iterative method) est1 = KMeans() #Fit the model and time s = time.time() k1 = est1.fit(d_train) print('Total time to complete fitting the model: '+str(round((time.time()-s)/60.,2))+' minutes') Total time to complete fitting the model: 7.62 minutes #Define the estimator (from scikit-learn library) and wrap with Dask Functions est2 = ParallelPostFit(Incremental(MiniBatchKMeans(n_clusters=8,compute_labels=False))) #Fit the model and time s = time.time() k2 = est2.fit(d_train) print('Total time to complete fitting the model: '+str(round((time.time()-s)/60.,2))+' minutes') Total time to complete fitting the model: 0.72 minutes Predict and Visualize We can use either model (k1 or k2) to predict on the entire dataset. #Reshape ALL the data from 3d (x,y,wavelength) to 2d (x*y,wavelength) d_final = d_all.reflectance.data.reshape((-1,len(band_list()[0]))).rechunk(('auto',-1)) #Transform the data with the same scaler as used with the fitting d_final = scaler.transform(d_final) #Predict the cluster for the entire domain, and then transform back to a 3d dataarray (x,y,wavelength) s = time.time() f_shape=xr.open_zarr('/project/geospatial_tutorials/tutorial_1/data/neon_2017_mosaic_brdf_corr.zarr',group='reflectance').reflectance.shape d_kmeans = k1.predict(d_final).persist().reshape(f_shape[0:2]) wait(d_kmeans) print('Total time to predict the entire domain: '+str(round((time.time()-s)/60.,2))+' minutes') Total time to predict the entire domain: 1.28 minutes #Convert results to xarray data base_dat=xr.open_zarr('/project/geospatial_tutorials/tutorial_1/data/neon_2017_mosaic_brdf_corr.zarr',group='reflectance') d_final2=xr.DataArray(data=d_kmeans,coords={'x':base_dat.coords['x'].values,'y':base_dat.coords['y'].values},dims=('y','x')).to_dataset(name='kmean_clust') #Plot the results for the entire domain at a 1/(10*10) resolution plt.figure(figsize=(16,12)) d_final2.isel(x=slice(None,None,10),y=slice(None,None,10)).kmean_clust.plot() &lt;matplotlib.collections.QuadMesh at 0x2ad73d757f98&gt; #Close-up plot of the results plt.figure(figsize=(16,12)) d_final2.isel(x=slice(5000,6000,1),y=slice(5000,6000,1)).kmean_clust.plot() &lt;matplotlib.collections.QuadMesh at 0x2ad73d753198&gt; "
},

    
      
      
      
      
      
    
{
  "title" : "Geospatial Analysis with Python on Ceres",
  "category" : "Trainings and Events",
  "subcategory" : "Tutorials",
  "set" : "",
  "url" : "/tutorials/geospatial-analysis-with-python-on-ceres",
  "description" : "Tutorial from LTAR webinar series HPC for geospatial analysis",
  "date" : "Mar 06, 2023",
  "content" : "Efficient data practices, using JupyterHub and Lab on Ceres, using Dask for parallel computing Reducing Data Size Data Type Scaling and changing the data type is an effective way to reduce the overall size of your data. A common datatype is floating point 64 bit, which has a level of precision that is often far greater than the precision in the data. Consider reflectance data ranging from 0.0 to 1.0 in floating point representation. Scaling by 10,000, and converting to int16 (16 bits) perserves the precision of the data and can reduce the size by a factor of 4. Consider the Below Example: import numpy as np array1 = np.random.random((1000,10000)).astype(np.float64) print(‘This array, in ‘+str(array1.dtype)+’, is ‘+str(array1.nbytes1e-6)+’ MB’) array2 = np.round(array110000.,0).astype(np.int16) print(‘Scaling and converting the array to ‘+str(array2.dtype)+’ results in a size of ‘+str(array2.nbytes*1e-6)+’ MB’) This array, in float64, is 80.0 MB Scaling and converting the array to int16 results in a size of 20.0 MB Data Compression Most raster formats have internal lossless compression options. Depending on the nature of your data, this can reduce the size substantially. For detailed specifications of raster formats see the GDAL specifications. Other common file formats, such as Zarr (Zarr Compression) and NetCDF (NetCDF Compression) have internal compression options as well. Virtual Rasters A common issue with geospatial analysis is working with data in different projections. A typical workflow may be to reproject data into a common projection, which results in duplication. An alternative is to use Virtual Raster Files (.VRT), which a simple .xml files that describe how the data should be transformed when opened. In addition to re-projecting, Virtual Raster Files can be used to mosaic, alter resolutions, resample, etc… An efficient tool for building VRT files is gdalbuildvrt  Python Setup Overall Setup - Background Interface JupyterLab: Web-based user interface (IDE for Python, R, IDL, etc…) JupyterHub A multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server Packages / Resouces Dask: Parallel Computing Library Xarray: Labelled multi-dimensional array package Numpy: Fundamental package for scientific computing with Python Rasterio: Raster IO and processing. Pangeo : NSF funded project for Big Data Geoscience. Implements a system very similiar to the container used on Ceres (titled: data_science_im_rs) EarthML: Examples of machine learning and visualization for Earth science. Supported and maintained by Anaconda, as a collaboration with the NASA Goddard Space Flight Center. Cluster Dask Distributed (a python library for parallel and distributed computing) uses Direct Acyclic Graphs to distribute data and processing across the cluster (very different from a MPI style cluster). Components of a dask cluster include: Client Scheduler Workers Container Singularity Container which can be found on Docker Hub Location URL: docker://rowangaffney/data_science_im_rs:latest Modified from the data_science docker stack Thanks to Yasasvy (Yash) Nanyam (Scinet VRSC) for helping getting it working Github: https://github.com/rmg55/data_science_im_rs/blob/master/data_science_im_rs Dockerhub: https://cloud.docker.com/repository/docker/rowangaffney/data_science_im_rs/general Step by Step Instructions Below are the steps/commands to setup the Python/JupyterLab environment followed by a soundless video of the process on Ceres. Note that you will need to already have a SCINet account. Please visit the SCINet website for detailed instructions to setup an account. Access JupyterHub Currently, to access JupyterHub, you need to port forward the application to your local system. However, in the future there will be a public URL, and you will not longer need to do this step. To port forward JupyterHub, run the following command in the PowerShell (windows) or terminal (linux). Note that you will need to replace your USER.NAME with your SciNET user name. ssh -N -L 8000:jupyterhub.scinet.local:80 USER.NAME@login.scinet.science Open in Browser Open a web browser (firefox, chrome, edge, etc…) and go to localhost:8000 Spawn JupyterLab Once logged into JupyterHub, you are given a set of options when launching JupyterLab. Below are brief descriptions of each option, followed by the value to use for this tutorial. If this is the first time spawning a notebook from a container on Docker or Singularity Hub (as in this example), it will take 4-10 minutes to donwload and build the container. The container is then cached in your home directory, so on subsequent tries, JupyterLab should spawn in 10 - 30 seconds. Node Type: Ceres partition to use when running JupyterLab       short OR brief-low (ethier works) Number of Cores: Number of Cores to allocate       4 Job Duration: Length of Job (HH:MM:SS)       00:30:00 Additional Slurm Options: Sbatch Options       (leave blank) Notebook/Lab Options: Additional JupyterLab or Jupyter Notebook options       –notebook-dir=/project/geospatial_tutorials/ Enter the full path to the container image: Location of the container to use       docker://rowangaffney/data_science_im_rs:latest Container Exec Options: Additional options for the singularity exec command       –bind /etc/munge –bind /var/log/munge –bind /var/run/munge –bind /usr/bin/squeue –bind /usr/bin/sinfo –bind /usr/bin/scancel –bind /usr/bin/sbatch –bind /usr/bin/scontrol –bind /scinet01/gov/usda/ars/scinet/system/slurm:/etc/slurm –bind /run/munge –bind /usr/lib64 –bind /scinet01 –bind $HOME –bind /software/7/apps/envi -H $HOME:/home/jovyan Please be cognizant of the compute resources you are requesting (see best practices below). Best Practices For short sessions (2hrs or less) please choose the brief-low partition in the Node Type drop down, if available. For serial computing (non-parallel code) enter 2 or 4 for number of cores. For parallel computing choose a reasonable number of cores to meet your needs. Choose a reasonable job duration (e.g. Do not choose 48hr job duration so you can leave your session open overnight). Remember to stop the jupyter server when you are done working (file –&gt; Hub Control Panel –&gt; Stop Server). Note that this data_science_im_rs container has two main environments which include (as of March 18, 2020): ► geo (python geospatial - click to see all packages) ► r_geo (R geospatial - click to see all packages) Furthermore, you can launch the RStudio (which uses the r_geo environment), a terminal, a help window, markdown, text file, and IDL kernel. To access the IDL library, you need to check-out the license from SCINet license server and bind-mounted properly (this is not shown in this example). The following silent video is a media alternative for the text in steps 1-3 in the Python Setup Section above. Link To Video  Cluster Setup Overall Setup - Background Uses the Dask Jobqueue Library to submit jobs to SLURM. Each Slurm job has X number of Python workers. Scales across nodes and partitions. Number of workers can be scaled up or down dynamically. Subject to SLURM resource allocation. JupyterLab has a Dask add-on to monitor the cluster. Dask includes a Dataframe (ie: Pandas) and Array (ie: Numpy) equivalent features. Dask Dataframes Dask Array Dask is used by Xarray - a geospatial/multidimensionial data package. Step by Step Instructions Below are the steps/commands to setup the cluster. Below these steps is a gif of the process on Ceres. Load Relevant Libraries import os import time import dask_jobqueue as jq from dask.distributed import Client,wait import dask.array as da Setup the Client Need to specify: Partition: You may want to change the partition (short, mem, brief-low, etc…) to whatever is available. Location of Singularity Image/Container SLURM job and python worker structure. In this example, for each SLURM JOB there are: 2 Python workers (i.e. processes) 6 cores per Python worker 3.2 GB per core The SLURM job will last 2 hours (wall time) The SLURM job will be run on the short and brief-low partitions Dask will launch using the docker://rowangaffney/data_science_im_rs:latest image and the geo environment partition=’short,brief-low’ container_url = ‘docker://rowangaffney/data_science_im_rs:latest’ conda_env = ‘geo’ num_processes = 2 num_threads_per_processes = 6 mem = 3.2num_processesnum_threads_per_processes n_cores_per_job = num_processes*num_threads_per_processes clust = jq.SLURMCluster(queue=partition, processes=num_processes, cores=n_cores_per_job, memory=str(mem)+’GB’, interface=’ib0’, local_directory=’$TMPDIR’, tmpdir_ssh=’/project/cper_neon_aop/neon_2017/analysis/prepocessing/’, death_timeout=30, python=singularity -vv exec –bind /usr/lib64 –bind /scinet01 –bind /software/7/apps/envi/bin/ {} /opt/conda/envs/{}/bin/python.format(container_url,conda_env), walltime=’02:00:00’, job_extra=[–output=/dev/null,–error=/dev/null]) cl=Client(clust) dash_addr = ‘’‘/user/{}/proxy/{}/status’’‘.format(os.environ[‘USER’],cl.scheduler_info()[‘services’][‘dashboard’]) print(‘Dask Lab Extention Address (paste into the dask search box): ‘+dash_addr) cl Dask Lab Extention Address (paste into the dask search box): /user/rowan.gaffney/proxy/8787/status Client Scheduler: tcp://10.1.8.38:43284 Dashboard: http://10.1.8.38:8787/statusCluster Workers: 0 Cores: 0 Memory: 0 B num_jobs=12 clust.scale(n=num_jobsnum_processes) while (((cl.status == running) and (len(cl.scheduler_info()[workers]) &lt; num_jobsnum_processes))): time.sleep(.1) cl Client Scheduler: tcp://10.1.8.38:43284 Dashboard: http://10.1.8.38:8787/statusCluster Workers: 24 Cores: 144 Memory: 460.80 GB A few quick example. 60 GB data: Calculate the mean without holding the data in memory 600 GB data: Calculate the mean without holding the data in memory 60 GB data: Persist the data to memory and calculate the mean t = da.random.random((10000,7500,100),chunks=(400,400,-1)) t Array Chunk Bytes 60.00 GB 128.00 MB Shape (10000, 7500, 100) (400, 400, 100) Count 475 Tasks 475 Chunks Type float64 numpy.ndarray 100 7500 10000 t2 = t.mean() t2 Array Chunk Bytes 8 B 8 B Shape () () Count 1132 Tasks 1 Chunks Type float64 numpy.ndarrayNow we will dynamically load the data, compute the results, and drop the data. t2.compute() 0.49999816307535666 Lets try working with data larger than memory t = da.random.random((100000,7500,100),chunks=(400,400,-1)) t Array Chunk Bytes 600.00 GB 128.00 MB Shape (100000, 7500, 100) (400, 400, 100) Count 4750 Tasks 4750 Chunks Type float64 numpy.ndarray 100 7500 100000 t2 = t.mean() t2 Array Chunk Bytes 8 B 8 B Shape () () Count 11208 Tasks 1 Chunks Type float64 numpy.ndarrayt2.compute() 0.5000005197202411 Alternatively, we can load the data to the cluster with the persist option t = da.random.random((10000,7500,100),chunks=(400,400,-1)).persist() wait(t) t Array Chunk Bytes 60.00 GB 128.00 MB Shape (10000, 7500, 100) (400, 400, 100) Count 475 Tasks 475 Chunks Type float64 numpy.ndarray 100 7500 10000 t.mean().compute() 0.4999999739855532 The following silent video is a media alternative for the text in the Cluster Setup Section above. Link To Video"
},

    
      
      
      
      
      
    
{
  "title" : "Quantitative Trait Locus (QTL) Analysis for Breeding",
  "category" : "Trainings and Events",
  "subcategory" : "Tutorials",
  "set" : "",
  "url" : "/tutorials/plant-breeding",
  "description" : "Quantitative Trait Locus Analysis for Breeding research",
  "date" : "Mar 06, 2023",
  "content" : "Performing QTL-seq data processing on CeresWhat is QTL-seq?Bulk segregant analysis (BSA) is used to find genes by identifying biased allele frequencies between two population that have been pooled based on distinctive phenotypes.  The technique has historically been limited by the qualitative nature of older genotyping platforms.  Next-generation sequencing allows for more precise characterization of allele frequencies in a population. This capacity was first realized in yeast studies, where extremely large populations sizes (&gt;100,000) individuals were used and extreme individuals within the population were pooled and resequenced (Ehrenreich et al., 2010).  Plant researchers quickly attempted to apply the above approach, commonly referred to as “QTL-seq”, to quantitative traits such as cold-tolerance (Yang et al., 2013), fungal rice blast (Takagi et al., 2013), and wheat grain protein (Trick et al., 2012), among others.  Though much of the work in QTL-seq is in the field and lab, the last portion does require bioinformatics.  To that end, we provide the following work-flow.Getting everything ready to run this pipeline(aka: things I won’t discuss how to do)   Get sequencing data on Ceres and in a working directory   Combine reads across sequencing lanes, if needed   Create and run job scripts Processing raw reads to get allele depth in each bulkSequencing data for each bulk should be mapped to a reference genome using bowtie or bwa with appropriate parameters.  A plausible job script might look something like this:#!/bin/bash  #SBATCH --job-name=qtl_seq #SBATCH -p short #SBATCH -N 1 #SBATCH -n 40 #SBATCH -t 48:00:00 #SBATCH --mail-user=your.email@usda.gov #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH -o stdout.%j.%N #SBATCH -e stderr.%j.%N  module load bwa module load samtools  #map reads from low bulk sample bwa mem -t 40 reference.fasta lowBulk_R1.fastq.gz lowBulk_R2.fastq.gz | samtools view - bS - | samtools sort - &gt; lowBulk.sort.bam samtools index lowBulk.sort.bam  #map reads from high bulk sample bwa mem -t 40 reference.fasta highBulk_R1.fastq.gz highBulk_R2.fastq.gz | samtools view - bS - | samtools sort - &gt; highBulk.sort.bam samtools index highBulk.sort.bam  #End of file Call variants and add information about the allele frequency in each sample:#!/bin/bash  #SBATCH --job-name=qtl_seq #SBATCH -p short #SBATCH -N 1 #SBATCH -n 40 #SBATCH -t 48:00:00 #SBATCH --mail-user=your.email@usda.gov #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH -o stdout.%j.%N #SBATCH -e stderr.%j.%N  module load samtools module load bcftools  #map reads to reference.fa using bwa or bowtie to generate highBulk.bam and lowBulk.bam samtools faidx reference.fasta #creates index samtools mpileup -g -t AD -f reference.fasta highBulk.sort.bam lowBulk.sort.bam &gt; output.bcf #calls variants and adds fequency information bcftools call -vc -V indels output.bcf &gt; output_snps.bcf #filter to snps #The inclusion of repeats in your experiment can dramatically reduce your signal strength; therefore, poor mapping quality and excess depth of coverage are two key features to filter on.  So the next step is optional but something like it is highly recommended bcftools filter -i 'MQ&gt;50 &amp;&amp; DP&gt;5 &amp;&amp; DP&lt;60' -O b output_snps.bcf &gt; filt_output_snps.bcf bcftools convert filt_output_snps.bcf -O z -o output_snps.vcf.gz #convert to compressed VCF Examining differential frequency of alleles visually and identifying peak genomic regionsFrom this point, it may be worthwile to download your resultant compressed VCF file and then run processing and viewing programs locally. Alternatively, you can also use RStudio on Ceres as documented in the auxiliary section below Using Rstudio on Scinet CERES in 3 steps.We focus on a QTL-seq analysis software called QTLSurge. Many others exist, such as QTLseqr.  Importantly, these other programs may not be completely compatible with the pipeline we have described above.QTLsurge is designed to be run under RStudio as a Shiny app and requires some dependent libraries. These are already installed on Ceres, so you can skip step 1 if working on Ceres:   Install dependencies: RStudio then, using Rstudio package manager, install zoo, ggplot2, and shiny libraries.   Download the programs: QTLsurge.R and vcf2freq.pl from QTLsurge and put in an appropriate folder.   Open QTLsurge.R in RStudio   Press “Run App” button   If using a Windows machine (not Ceres), you may also have to install Perl. QTLsurge requires a tab delimited file containing chromosome ID in the first column, SNP location in the second column, high-bulk allele frequency in the third column, low-bulk allele frequency in the fourth column, deltaSNP (difference between high and low frequency) value in the fifth column, and cycle number in sixth column (see test/test.freq). Regardless of sign, all deltaSNP values are converted to absolute values for plotting and window calculation.A testing file (test/test.freq) is located on the QTLsurge github page. This file demonstrates the required input format and can be used to test that your app is working correctly.Create such a file from the data produced above:gunzip output_snps.vcf.gz #uncompresses as output_snps.vcf and removes output_snps.vcf.gz perl vcf2freq.pl output_snps.vcf 0 &gt;output_frequency_file.txt #vcf2freq.pl is supplied as a helper program, converts to QTLsurge format.  The last argument is the cycle you are on.  Use 0 if this is your initial, standard QTL-seq experiment.  This script is not robust to variation in genotype format and only accepts GT:PL:AD format that results from the pipeline described above. Browse to this file and open it. A graph, similar to the one below will appear momentarily. You can step through each chromosome using the interface and look for peaks that are above the genome-wide threshold (95th percentile based on raw deltaSNP values). Because raw data of this kind is very noisy, a sliding window average is supplied. The appropriate window size is a function of population size, recombination rate, marker density, and read depth. Generally, if read depth is &gt;40x, your window size should decrease as your population size increases. A good rule-of-thumb is that few windows should have an average that extends beyond 3 standard error units of a directly adjacent window, assuming your overlap-to-window-size is ~20%. The red line indicates the average of window and the gray shading indicates 3 standard error units. Note: Setting the overlap size very low will cause QTLSurge to respond slowly and should only be used when zoomed in to a &lt;1MB range.For a standard QTL-seq experiment, this might be a stopping point for publication. QTLsurge provides further support for iterative rounds of genotyping that allow a researcher to further pinpoint genes and/or confirm the peaks that they have discovered. The QTLsurge page describes these possible experiments and analyses in more detail.Using Rstudio on Scinet CERES in 3 stepssee also the RStudio Server User GuideLog into CERES, and run a pre-made Rstudio scriptOpen up two terminals or putty windows (important for later) and use one to connect to &lt;username&gt;@scinet.science. Then, either use the file /reference/containers/RStudio/3.6.0/rstudio.job, or create an Rstudio script using the Ceres Job Script Generator (eAuthentication required).If using the generator, make sure you select “Rstudio” in the “Job Script template” dropdown menu). I recommend copying the /reference/ or generated script into your desired working directory. You should now edit the script to request the time and resources you want. Some R modules, like those for coexpression analysis, need many threads and lots of memory, so you’ll want to edit the “–mem” and “–nodes” lines in the script to request the memory and CPUs you need. Also, for the next step, you may wish to edit the “–output” line to change the default location for the output instructions from /home/&lt;username&gt; to whichever working directory you want. The output instructions will tell you what to do next in detail.Submit the job and read the instructions in the generated rstudio-out fileThe outputted “rstudio-&lt;lots of numbers&gt;.out” file will have instructions on how to connect to your now-running Rstudio session. You can either a) create an ssh tunnel with an  ssh -N -L 8787:hostname:port  command, then open a browser window at “localhost:8787” (you can then minimize the terminal window you used to do the ssh command); or b) connect to the SCINet VPN, and go to the hostname:8787 address indicated in the rstudio.out file in your browser of choice.Log into the Rstudio serverIf you did the above step correctly, your browser should be presenting you with an Rstudio login screen. Following the instructions in the same output file above, log into the server in the browser window using the credentials supplied in that file (your Scinet user name and a randomly-generated one-time password)."
},

    
      
      
      
      
      
    
{
  "title" : "Introduction to Unix (from the Bioinformatics Workbook)",
  "category" : "Trainings and Events",
  "subcategory" : "Tutorials",
  "set" : "",
  "url" : "/tutorials/unix-basics",
  "description" : "Unix basics from the bioinformatics workbook",
  "date" : "Mar 06, 2023",
  "content" : ""
},


  
    
      
      
      
      
      
    
{
  "title" : "Itinerary",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-00-Geospatial-Workshop-Itinerary",
  "description" : "",
  "date" : "Mar 06, 2023",
  "content" : "2022 Geospatial Workshop ItineraryPlease review the pre-meeting checklist and background information on the Pre-meeting page to ensure you are prepared for the workshop sessions. "
},

    
      
      
      
      
      
    
{
  "title" : "Pre-meeting Tech Support",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-00-Geospatial-Workshop-Premeeting",
  "description" : "",
  "date" : "Mar 06, 2023",
  "content" : " Pre-meeting checklistPlease complete this pre-meeting checklist ahead of time if you plan to participate in any of the interactive follow-along tutorials (Sessions 4-10).                     Pre-Meeting Checklist                                      Have a SCINet account and be able to login. If you don't have one, apply for one ASAP.           If you need help accessing your SCINet account, attend Session 0: Pre-meeting Technical Support on 8/25/2020 at 10am MDT.           Register for the sessions you wish to attend before 8/25/2022.           Familiarize yourself with the background information on this page.           Optional but suggested: If using your laptop for these tutorials you may want to connect to a larger monitor so that you will have enough monitor space to see the zoom demonstration while also having your browser or terminal workspace open at the same time.                         TutorialsThe tutorials span the range from beginner to more advanced. If you have questions about the material, please do not hesitate to contact the organizing committee.In general, these tutorials assume some level of knowledge in scientific programming, but beginners are still welcome. Access to the SCINet Ceres or Atlas HPC SystemsYou must apply for a SCINet account to get access to the Ceres or Atlas HPC systems. After your account is approved, there are a couple more steps to complete before you will be able to successfully login to your account. You will receive instructions by email on how to access your account for the first time upon account approval. Please make sure you can login to your account before joining our tutorial sessions.Visit the Ceres and Atlas user guides for more information about how to login and brief information about the HPC systems.If you have issues accessing Ceres or Atlas, please attend Session 0: Pre-meeting SCINet Account Login Assistance on 8/25/2022 at 10am MDT. We cannot provide individual assistance for login issues in any other session. Software + Hardware + Nomenclature OverviewThe software discussed and shown in the workshop is largely open source, can run on a desktop, HPC, or cloud environment. Below is a quick overview of some of the software, hardware, and confusing nomenclature that will be used during this workshop.SCINet vs. Ceres vs. AtlasSCINet is the USDA ARS Scientific Computing INitiative that aims to improve access to high performance and cloud computing, improve networking to facilitate high speed data transfer, and facilitate scientific computational training. A part of SCINet is its computing infrastructure, including the Ceres and Atlas high-performance computing (HPC) clusters. For most of the tutorials, we will default to running them on Ceres but session material will provide information about both clusters. For more information on computing systems that SCINet offers, see the Computer Systems page of the SCINet website..Open OnDemandOpen OnDemand (https://openondemand.org) allows end users to use a web browser to access resources with graphical applications at HPCs such as Ceres and Atlas. This workshop will use Open OnDemand on Ceres to access three applications:   RStudio Server: A web-based version of RStudio, an integrated development environment for the language R.   JupyterLab: A web-based interactive development environment which we will use for python.   Desktop: A virtual desktop which we will use to run the QGIS graphical user interface. SLURMSLURM (Simple Linux Utility for Resource Management) is the workload manager used on the Ceres and Atlas HPC systems to allocate computational resources. From the SLURM documentation, SLURM is “an open source… cluster management and job scheduling system for large and small Linux clusters. As a cluster workload manager, SLURM has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.”Scientific Coding Languages - Python and RThe tutorials in this workshop will use both R and Python due to previous working group survey results indicating that most members either use these languages or are interested in learning them."
},

    
      
      
      
      
      
    
{
  "title" : "Session 0: Pre-meeting technical support for account logins and data uploads",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-8-25-Geospatial-Workshop-0",
  "description" : "",
  "date" : "Aug 25, 2022",
  "content" : " Login Support This pre-workshop session will have a VRSC team member available to address any issues related to SCINet account logins. If you don’t currently have a SCINet account, apply for one ASAP.You must be able to login to your SCINet account to actively participate in the tutorial sessions. If it has been a while since you logged in to your account, please confirm that you are able to login. Your password has likely expired. If your password is expired, please reset it. If you are unable to reset your password because you do not remember your expired password, you will need to ask VRSC for help by emailing scinet_vrsc@usda.gov and/or attending this session.We can also address questions related to uploading files to Ceres or Atlas for the Bring-Your-Own-Problem sessions."
},

    
      
      
      
      
      
    
{
  "title" : "Session 1: Introduction to SCINet with lightning presentations",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-8-29-Geospatial-Workshop-1",
  "description" : "",
  "date" : "Aug 29, 2022",
  "content" : " Agenda    Overview of SCINet resources   Introduce SCINet office, organizing committee, and helpers   Describe past workshops, current goals, and overview of sessions   Lightning presentations on geospatial workflows What is the SCINet Geospatial Research Working Group? See our working group page for general information about the working group.Our main goals are to:   provide continued input on the development of SCINet,   improve the computational capacity of ARS geospatial researchers, and   develop collaborative research projects. The goals of the 2022 workshop are to:   provide hands-on learning on how to modify geospatial research workflows to take advantage of SCINet HPC systems,   foster research efforts that had previously been un-attainable due to technical limitations or inexperience, and   inspire new working group objectives. Review of previous workshops 2019 Workshop (Las Cruces, NM)At the 2019 workshop, we:   heard from ARS scientists about successes and challenges of using SCINet for geospatial analysis,   identified common issues and barriers to using SCINet for geospatial research workflows,   generated recommendations for overcoming these issues/barriers,   created the SCINet Geospatial Research Working Group to follow through with workshop recommendations, and   learned about machine learning, deep learning, and some of the ways ML/DL are being used for research at New Mexico State University and the Jornada Experimental Range. The resulting recommendations for improving access to SCINet HPC/cloud computing and enhancing general computational skill in the ARS geospatial research community are as follows:A) Improve data access, sharing, and storage.   Implement better methods for getting massive input data into the computational environment.   Build/expand higher speed connections to SCINet.   Build a common data library on SCINet to reduce duplication of popular datasets and reduce the barrier to adopting HPC workflows.   Hold trainings in data management for long-term datasets and archiving of data from retired scientists. B) Enhance reproducibility of our science and increase collaboration.   Hold trainings on using Git, containers, and other reproducibility tools.   Hold hands-on group work sessions in R and Python. C) Build capacity for using HPC/cloud computing.   Hold trainings on how to access and compute on the SCINet HPC systems (including how to parallelize code) to reduce the learning curve.   Develop a collection of tutorials from ag/geo-relevant applications with example scripts that run on the SCINet HPCs, including ML/DL techniques. (Geospatial Workbook)   Hire personnel, including postdocs, with multi-disciplinary computational backgrounds.   Hold trainings in R, Python, and Unix through The Carpentries or similar.   Repeat the 2019 AI Training by Adam Rivers and University of Florida collaborators.   Hold trainings in image processing. D) Expand SCINet HPC system software, hardware, and support to focus on our research community needs.   Contract a “GeoTeam” that would help our research community with workflow development, parallelization, and code optimization.   Deploy a better forum for searchable SCINet questions/answers.   Purchase community GPUs when the priority GPU gets to be a limited resource.   Install Pix4D Engine software on SCINet. 2020 Workshop (Virtual)The 2020 Workshop was planned following the key needs identified in the community in 2019 workshop. The workshop opened with short introductions of the first cohort of SCINet postdocs and a discussion on the status and needs for the SCINet geospatial common data library and workbooks. Participants were introduced to the Ceres HPC and worked through several tutorials ranging in complexity from the basic various ways to run jobs on Ceres, to running on clusters in parallel, reproducible research best practices, and a real world example of running an analysis on Ceres. Most of these tutorials were in a Python and JupyterLab framework. The workshop closed with seminars and a panel of researchers in academia and industry discussing their machine learning workflows to improve agricultural practices.Lightning talksWatch a recording of this presentation and lightning talks. "
},

    
      
      
      
      
      
    
{
  "title" : "Session 2: Fundamentals of geospatial data",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-8-29-Geospatial-Workshop-2",
  "description" : "",
  "date" : "Aug 29, 2022",
  "content" : " Learning objectives With only one hour, we are limited in how many geospatial basics we can cover. Since this workshop is geared towards addressing computationally expensive geospatial analyses, we will prioritize content relevant to that topic to provide foundation for the tutorials in upcoming sessions. Please see the links listed in Session 11 for additional geospatial training opportunities.   Classify geospatial data as raster or vector   Recognize geospatial operations that can be computationally expensive   Identify if a geospatial operation has:            many independent tasks       some locally-dependent tasks       one or few globally-dependent tasks         Agenda This session will be a lecture with slides covering the following content:   Define scope and objectives: content most relevant to computational concerns   Raster data            Structure, file formats, and I/O concerns       Common computationally expensive operations           Vector data            Geometry types, file formats, and I/O concerns       Common computationally expensive operations           Geospatial Data Abstraction Library (GDAL) Watch a recording of this presentation. "
},

    
      
      
      
      
      
    
{
  "title" : "Session 3: Fundamentals of parallel processing",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-8-30-Geospatial-Workshop-3",
  "description" : "",
  "date" : "Aug 30, 2022",
  "content" : " Learning objectives The overall objective of this session is to provide a foundation in parallel processing terminology and HPC-usage basics before applying these concepts in the upcoming tutorials. Please see the links listed in Session 11 for additional parallel processing training opportunities.   Define an ‘embarassingly parallel’ problem   Apply a parallelization approach flowchart to a geospatial problem   Describe the basic SLURM job submission parameters   Identify the appropriate Ceres or Atlas partition for a job Agenda This session will be a lecture with slides covering the following content:   Trade-offs in parallel processing: effort, time, communication   How to approach parallelization: a geospatial example            Determine where in your workflow the most time is spent       Flow chart: parallelization approaches       Choosing number of cores           SLURM jobs, nodes, and partitions Watch a recording of this presentation. "
},

    
      
      
      
      
      
    
{
  "title" : "Session 4: SCINet interactive environments",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-8-30-Geospatial-Workshop-4",
  "description" : "",
  "date" : "Aug 30, 2022",
  "content" : " Learning objectives Many of us typically develop our geospatial processing workflows in Integrated Development Environments (IDEs), e.g. RStudio, or other Graphical User Interfaces (GUIs). In the past, using HPC resources required connecting to the cluster, transferring files, and submitting jobs to be all done in Unix commands. Although that is still an option, there are now many options for web access to HPC resources using applications to which users are  accustomed. This session will cover a few geospatial-relevent applications available via Open OnDemand portals on Ceres and Atlas.   Launch an interactive session of RStudio and JupyterLab on Ceres from your web browser   Launch an interactive desktop session on Ceres from your web browser and run QGIS   Identify scenarios when interactive environments are or are not advantageous  Agenda This is the first interactive tutorial session. We will be accessing the interactive environments available via Open OnDemand on Ceres. There will be a few introductory slides, a period of screen-sharing to navigate the portal in a browser window, and then example scripts to execute within the environments.  We will cover the following content:   When to use interactive environments (advantages and limitations)   Overview of Open OnDemand layout and Ceres/Atlas specifics   Launch parameters   RStudio   Jupyter   Desktops with QGIS A recording of this presentation and discussion summary notes will be made available here after the workshop is complete. Tutorial material Coming soon "
},

    
      
      
      
      
      
    
{
  "title" : "Session 5: SCINet Geospatial Common Data Library",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-8-30-Geospatial-Workshop-5",
  "description" : "",
  "date" : "Aug 30, 2022",
  "content" : " Learning objectives The Geospatial Common Data Library (GeoCDL) is a community-driven product inspired by the computational needs discussions in the previous working group workshops. This effort is motivated by the desire to reduce duplication of effort across ARS scientists in downloading and curating large geospatial datasets, avoid the storage of duplicate data on HPC resources, and lower the barrier of entry to new users interested in leveraging SCINet computing resources to analyze large and complex geospatial data. This session will include tutorials using the pilot version of the GeoCDL on Ceres.   Access the GeoCDL API via the rgeocdl R package   Query the API to list the datasets currently available   Download raster data within an area of interest   Download extracted raster data at points of interest Agenda    Purpose and current status   Interface options   Tutorial 1: Area of interest   Tutorial 2: Points of interest   Discussion: desired datasets and features  Tutorial material A recording of this tutorial is coming soon.A written version of this tutorial, modified to be accessible to any SCINet user, is available on the Geospatial Workbook. "
},

    
      
      
      
      
      
    
{
  "title" : "Session 6: Geospatial analyses and how to parallelize them in R",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-8-31-Geospatial-Workshop-6",
  "description" : "",
  "date" : "Aug 31, 2022",
  "content" : " Learning objectives This session will include tutorials exploring examples of handling geospatial data, performing geospatial calculations, and applying parallel processing approaches to geospatial processing workflows in R. RStudio via Open OnDemand (see Session 4) will be used for a portion of the tutorials.   Read in and manipulate raster data with the terra and stars packages   Read in and manipulate vector data with the sf package   Time chunks of code in your R script   Identify package functions with parallelization options built-in   Parallelize R code of many independent geospatial tasks Agenda This session will be an interactive tutorial:   Geospatial packages   Parallel processing packages   Vector tutorial   Raster tutorial   Vector-raster tutorial  Tutorial material Written versions of these tutorials, modified to be accessible to any SCINet user, are available on the Geospatial WorkbookThe workshop-specific instructions are kept below.Steps to prepare for the tutorial:        Login to Ceres Open OnDemand at https://ceres-ood.scinet.usda.gov. Your username is typically firstname.lastname. For the password, enter your SCINet account password followed by the 6-digit verification code, e.g. from a Google Authenticator app on your phone, with no spaces. Do not add a ‘+’ between your password and code.           Copy the Session 6 material from the workshop project space to your temporary workshop folder. To get to a shell to do so, you can use the Clusters tab at the top of your Open OnDemand page to select ‘Ceres Shell Access’ (if prompted for a password, enter your SCINet account password without the verification code). If you are comfortable ssh-ing in instead from terminal or powershell, feel free to do so.      If you have already made your workshop folder in previous sessions, you will only need to run the following commands, replacing firstname.lastname with your actual name:       cd /90daydata/shared/firstname.lastname  cp -r /project/geospatialworkshop/session6/           If you have not created your workshop folder yet, run these commands instead, replacing firstname.lastname with your actual name:       cd /90daydata/shared  mkdir firstname.lastname  cd firstname.lastname  cp -r /project/geospatialworkshop/session6/                 Launch a RStudio session. Choose the following values from the menu:             Account: geospatialworkshop       Slurm Partition: workshop       R version: 4.2       Number of hours: 3       Number of cores: 16       Memory required: 64G           Click Launch.           The tutorials: The first two tutorials will follow Rmarkdown documents in RStudio. For the third tutorial, we will submit a job to SLURM directly. If your shell from Step 2 has expired when we start the third tutorial, please reconnect, and change directory to your session 6 folder:       cd /90daydata/shared/firstname.lastname/session6          "
},

    
      
      
      
      
      
    
{
  "title" : "Session 7: Interactive BYOP session + Open Drone Map demo",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-8-31-Geospatial-Workshop-7",
  "description" : "",
  "date" : "Aug 31, 2022",
  "content" : "Agenda The objective of the Bring Your Own Problem (BYOP) sessions is to allow workshop participants to discuss their workflow needs, raise follow-up questions about session material, or initiate any discussion related to geospatial analyses on SCINet. Multiple BYOP sessions are scheduled to give everyone adequate time to participate and to allow questions to be re-visited if time outside of sessions is needed for follow-up.For this initial BYOP session, we will have a demonstration of using Open Drone Map on Atlas.Post-workshop updateWatch a recording of the Open Drone Map on Atlas demonstration. "
},

    
      
      
      
      
      
    
{
  "title" : "Session 8: Geospatial analyses and how to parallelize them in python",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-9-1-Geospatial-Workshop-8",
  "description" : "",
  "date" : "Sep 01, 2022",
  "content" : " Learning objectives This session will include tutorials exploring examples of handling geospatial data, performing geospatial calculations, and applying parallel processing approaches to geospatial processing workflows in python. JupyterLab via Open OnDemand (see Session 4) will be used for a portion of the tutorials.   Read in and manipulate raster data with the rioxarray package   Read in and manipulate vector data with the geopandas package   Time chunks of code in your python script   Identify package functions with parallelization options built-in   Parallelize python code of many independent geospatial tasks Agenda This session will be an interactive tutorial:   Geospatial packages   Parallel processing packages   Vector tutorial   Raster tutorial   Vector-raster tutorial  Tutorial material Written versions of these tutorials, modified to be accessible to any SCINet user, are available on the Geospatial WorkbookThe workshop-specific instructions are kept below.Steps to prepare for the tutorial:        Login to Ceres Open OnDemand at https://ceres-ood.scinet.usda.gov. Your username is typically firstname.lastname. For the password, enter your SCINet account password followed by the 6-digit verification code, e.g. from a Google Authenticator app on your phone, with no spaces. Do not add a ‘+’ between your password and code.           Copy the Session 6-8 material from the workshop project space to your temporary workshop folder. The contents of this session have been added to the Session 6 folder since they share the same data. To get to a shell to do the copying, you can use the Clusters tab at the top of your Open OnDemand page to select ‘Ceres Shell Access’ (if prompted for a password, enter your SCINet account password without the verification code). If you are comfortable ssh-ing in instead from terminal or powershell, feel free to do so.      If you have already made your workshop folder in previous sessions, you will only need to run the following commands, replacing firstname.lastname with your actual name:       cd /90daydata/shared/firstname.lastname  cp -r /project/geospatialworkshop/session6/ .  module load miniconda  source activate /project/geospatialworkshop/gwenv  ipython kernel install --user --name=grwg_workshop           If you have not created your workshop folder yet, run these commands instead, replacing firstname.lastname with your actual name:       cd /90daydata/shared  mkdir firstname.lastname  cd firstname.lastname  cp -r /project/geospatialworkshop/session6/ .  module load miniconda  source activate /project/geospatialworkshop/gwenv  ipython kernel install --user --name=grwg_workshop                Launch a JupyterLab session. Choose the following values from the menu:             Account: geospatialworkshop       Slurm Partition: workshop       Number of hours: 3       Number of cores: 16       Jupyter Notebook vs Lab: Lab       Working Directory: /90daydata/shared/firstname.lastname           Click Launch.           The tutorials: Two tutorials will follow python notebooks in JupyterLab. For the third tutorial, we will submit a job to SLURM directly. If your shell from Step 2 has expired when we start this tutorial, please reconnect, and change directory to your session 6 folder:       cd /90daydata/shared/firstname.lastname/session6          "
},

    
      
      
      
      
      
    
{
  "title" : "Session 9: Interactive BYOP session",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-9-1-Geospatial-Workshop-9",
  "description" : "",
  "date" : "Sep 01, 2022",
  "content" : " Agenda The objective of the Bring Your Own Problem (BYOP) sessions is to allow workshop participants to discuss their workflow needs, raise follow-up questions about session material, or initiate any discussion related to geospatial analyses on SCINet. Multiple BYOP sessions are scheduled to give everyone adequate time to participate and to allow questions to be re-visited if time outside of sessions is needed for follow-up. "
},

    
      
      
      
      
      
    
{
  "title" : "Session 10: Interactive BYOP session + TBD special topic based on previous sessions' discussions",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-9-2-Geospatial-Workshop-10",
  "description" : "",
  "date" : "Sep 02, 2022",
  "content" : " The objective of the Bring Your Own Problem (BYOP) sessions is to allow workshop participants to discuss their workflow needs, raise follow-up questions about session material, or initiate any discussion related to geospatial analyses on SCINet. Multiple BYOP sessions are scheduled to give everyone adequate time to participate and to allow questions to be re-visited if time outside of sessions is needed for follow-up.In this final BYOP session, we will also address a TBD topic that was raised in previous sessions, e.g. how to handle a particular dataset or use a certain package.Post-workshop updateThe topic was how to use R to loop through input data files and submit SLURM jobs per file.  A recording of this tutorial is coming soon. A written version of this tutorial, modified to be accessible to any SCINet user, is available on the Geospatial Workbook."
},

    
      
      
      
      
      
    
{
  "title" : "Session 11: Closing discussion and future planning",
  "category" : "Trainings and Events",
  "subcategory" : "Workshops",
  "set" : "2022 Geospatial Workshop",
  "url" : "/workshops/2022-9-2-Geospatial-Workshop-11",
  "description" : "",
  "date" : "Sep 02, 2022",
  "content" : " Agenda    Review of objectives and content covered   Survey: feedback on this workshop   Further related training opportunities   Discussion of future needs for the working group Feedback surveyThe survey for providing feedback about this workshop will be available here during the session and for a limited time afterwards.Further trainingsComing soon: links to suggested trainings. For general information about SCINet trainings, see these opportunities.Future needsSummary notes of discussion will be provided here after the workshop. "
},



{
  "title" : "Maintenance -  - Thursday, March 2 2023",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Thursday, March 2 2023",
  "content" : "The Albany site location will experience loss of connectivity to SCINet intermittently during the hours of 4:00 pm to 6:00 pm EST on March 2, 2023."
},

{
  "title" : "Maintenance - Ceres - Monday, February 20 2023",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/announcements/2023-01-24-Ceres",
  "description" : "Maintenance",
  "date" : "Monday, February 20 2023",
  "content" : ""
},

{
  "title" : "Maintenance - Ceres - Thursday, October 27 2022",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Thursday, October 27 2022",
  "content" : "Due to recent issues with Ceres' /project storage hardware, it needs to be replaced. The replacement hardware is expected to be delivered by end of the day on 10/26/2022 and the works will probably be done on 10/27/2022. Before replacing the hardware, we will post on the SCINet Forum and update the message of the day displayed at login to Ceres. While replacing the hardware, Ceres' /project will not be accessible. We plan to suspend all running jobs before unmounting /project and resume the jobs once the maintenance completes. While we expect this will not affect running jobs, we recommend submitting new jobs to run on /90daydata to minimize the risk of the job dying due to this maintenance."
},

{
  "title" : "Maintenance - Ceres - Monday, October 10 2022",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Monday, October 10 2022",
  "content" : ""
},

{
  "title" : "Maintenance - Ceres - Monday, June 20 2022",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Monday, June 20 2022",
  "content" : ""
},

{
  "title" : "Maintenance - Atlas - Tuesday, May 17 2022",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Tuesday, May 17 2022",
  "content" : ""
},

{
  "title" : "Maintenance - Ceres - Monday, February 21 2022",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Monday, February 21 2022",
  "content" : ""
},

{
  "title" : "Maintenance - SCINet - Thursday, January 20 2022",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Thursday, January 20 2022",
  "content" : "The maintenance window is one (1) hour in duration. This will impact service to the Stoneville site only."
},

{
  "title" : "Full cluster Maintenance - Atlas - Wednesday, December 8 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Full cluster Maintenance",
  "date" : "Wednesday, December 8 2021",
  "content" : "Wednesday, December 8, beginning at 8am CST, the HPC2 Computing Office has scheduled maintenance for the atlas compute cluster. During this maintenance window, the login, devel, dtn, ood, and compute nodes for atlas will be unavailable and all associated cron jobs will be disabled. Downtime is expected to last most of the day. For any associated problems, submit a help desk ticket: help-usda@hpc.msstate.edu - specific atlas issues scinet_vrsc@usda.gov - general operational issues"
},

{
  "title" : "Network Maintenance in Ames - SCINet - Thursday, November 18 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Network Maintenance in Ames",
  "date" : "Thursday, November 18 2021",
  "content" : "SCINet network maintenance has been scheduled for Ames, IA. The maintenance window is from 8:30 to 10:30 Central Time (1430-1630 UTC) on 18 November 2021. Connectivity to SCINet will be sporadic during the maintenance window."
},

{
  "title" : "Network Maintenance in Ames - SCINet - Tuesday, November 16 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Network Maintenance in Ames",
  "date" : "Tuesday, November 16 2021",
  "content" : "Connectivity to SCINet will be sporadic during the maintenance window."
},

{
  "title" : "Network Maintenance in Albany - SCINet - Monday, November 15 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Network Maintenance in Albany",
  "date" : "Monday, November 15 2021",
  "content" : "Local connectivity to SCINet will be sporadic during the maintenance window."
},

{
  "title" : "Maintenance - Ceres - Thursday, November 11 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Thursday, November 11 2021",
  "content" : "Ceres maintenance is scheduled for Thursday, November 11, 2021 to upgrade internal cluster network. Queued jobs will not start if they cannot complete by 6AM November 11. These include jobs submitted to the long partition with the default 3-weeks long time limit. In the output of the squeue command the reason for those jobs will state (ReqNodeNotAvail, Reserved for maintenance). The jobs will start after the scheduled outage completes.  The Atlas cluster will stay up and running during Ceres downtime. All Ceres users can run jobs on Atlas and use /90daydata that has no quotas."
},

{
  "title" : "Fiber relocation - Ceres - Thursday, November 4 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Fiber relocation",
  "date" : "Thursday, November 4 2021",
  "content" : "The listed asset will be unavailable while Lumen engineers perform preventative fiber relocation work. Outage is expected to be two hours each day, but up to 5 hours is possible. The entire window is reserved"
},

{
  "title" : "Network update - Ceres, Juno - Thursday, October 28 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Network update",
  "date" : "Thursday, October 28 2021",
  "content" : "A maintenance window has been scheduled for 28 October 2021 from 1530 - 1730 UTC (10:30am to 12:30pm Central time) to stabilize router (Albany MX480 RE Downgrade).  Periodic outages will be experienced as equipment is rebooted. Connectivity to Ceres and Juno cannot be guaranteed during the maintenance window."
},

{
  "title" : "Network update - Ceres, Juno - Tuesday, October 26 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Network update",
  "date" : "Tuesday, October 26 2021",
  "content" : "A maintenance window has been scheduled for 26 October 2021 from 4:30pm to 8:30pm Central time to stabilize the SCINet Network. Periodic outages will be experienced as equipment is rebooted. Connectivity to Ceres and Juno cannot be guaranteed during the maintenance window."
},

{
  "title" : "Router update - Ceres - Tuesday, October 19 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Router update",
  "date" : "Tuesday, October 19 2021",
  "content" : "The router at Ames will be rebooted on or about 4:30 CT. The reboot should be about 15 minutes. After that the router will be upgraded to the latest OS. Outages may occur during that process."
},

{
  "title" : "Router update -  - Wednesday, September 22 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/announcements/2021-09-21-scinet-hardware",
  "description" : "Router update",
  "date" : "Wednesday, September 22 2021",
  "content" : "More SCINet network hardware OS updates.  Check the announcement page for more details"
},

{
  "title" : "OS Upgrade - SCINet - Sunday, September 16 2012",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "OS Upgrade",
  "date" : "Sunday, September 16 2012",
  "content" : "GNOC plans to upgrade the OS on the SCINet gear at the 6 locations. This will result in connectivity interruptions during the upgrade. The upgrade schedule is the following: Albany - 9/16 8AM PST Clay Center - 9/16 4PM CST Ames - 9/17 8AM CST Stoneville - 9/20 8AM CST NAL - 9/20 3PM CST CSU - 9/21 9AM CST"
},

{
  "title" : "Maintenance - Ceres - Monday, August 23 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Monday, August 23 2021",
  "content" : "This maintenance window will be longer than normal as there are several important hardware upgrades occurring during this window to enhance the overall power and capacity of the CERES HPC cluster.  These upgrades include the remaining new priority nodes, sixty eight additional compute nodes, two additional high memory compute nodes, six management nodes, and faster Infiniband switching technology used by the HPC nodes to access storage. VRSC will re-rack and re-wire the whole cluster to accommodate additional hardware while adhering to power and cooling limits.   Queued jobs will not start if they cannot complete by 7AM August 23. These include jobs submitted to the long partition with the default 3-weeks long time limit. In the output of the squeue command the reason for those jobs will state (ReqNodeNotAvail, Reserved for maintenance). The jobs will start after the scheduled outage completes.  The Atlas cluster will stay up and running during Ceres downtime. All Ceres users can run jobs on Atlas. If you don't have a large enough project quota on Atlas, remember that you can use /90daydata on Atlas that has no quotas"
},

{
  "title" : "Outage - Ceres - Wednesday, July 7 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/announcements/2021-07-06-scinet-down",
  "description" : "Outage",
  "date" : "Wednesday, July 7 2021",
  "content" : "Connection Restored on 07-21-2021"
},

{
  "title" : "Maintenance - Ceres - Monday, May 24 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Monday, May 24 2021",
  "content" : "The listed assets will be unavailable while contractors perform testing on the elecrtical service switchgear, generators, and turbine. Outages throughout the window are expected. The entire window is reserved."
},

{
  "title" : "Maintenance - Atlas - Tuesday, February 23 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Tuesday, February 23 2021",
  "content" : "The HPC2 Computing Office has scheduled a maintenance for its core networking services.  During this time all network connectivity both inside and outside the HPC2 will be unavailable including access to the atlas cluster systems."
},

{
  "title" : "Maintenance - Ceres - Tuesday, February 16 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Tuesday, February 16 2021",
  "content" : ""
},

{
  "title" : "Maintenance - Ceres - Monday, February 15 2021",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Monday, February 15 2021",
  "content" : ""
},

{
  "title" : "Maintenance - Ceres - Monday, October 12 2020",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Monday, October 12 2020",
  "content" : ""
},

{
  "title" : "UPS Maintenance - SCINet - Tuesday, August 25 2020",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "UPS Maintenance",
  "date" : "Tuesday, August 25 2020",
  "content" : "SCINet equipment will be shutdown in order to perform Maintenance to the UPS. SCINet connectivity at the Stoneville location will be impacted. The Maintenance window is reserved from 0700 to 1600 Central Time."
},

{
  "title" : "Maintenance - Ceres - Tuesday, June 16 2020",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Tuesday, June 16 2020",
  "content" : ""
},

{
  "title" : "Planned power outage - SCINet & AWS - Friday, April 17 2020",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Planned power outage",
  "date" : "Friday, April 17 2020",
  "content" : "SCINet equipment at the National Agricultural Library will be powered down in advance of a planned power outage to the NAL building. The outage is expected to last for 24 hrs or less. We expect that normal access to SCINet resources will be restored on or before Monday, April 20. Please check Basecamp during the outage period for updates."
},

{
  "title" : "Router migration - SCINet - Thursday, March 19 2020",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Router migration",
  "date" : "Thursday, March 19 2020",
  "content" : ""
},

{
  "title" : "Router replacement - SCINet - Thursday, March 12 2020",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Router replacement",
  "date" : "Thursday, March 12 2020",
  "content" : ""
},

{
  "title" : "Router replacement - SCINet - Monday, March 2 2020",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Router replacement",
  "date" : "Monday, March 2 2020",
  "content" : ""
},

{
  "title" : "Maintenance - Ceres - Monday, February 17 2020",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Maintenance",
  "date" : "Monday, February 17 2020",
  "content" : ""
},

{
  "title" : "Upgrades/expansion - Ceres - Monday, December 2 2019",
  "category" : "News",
  "subcategory" : "Downtime",
  "set" : "",
  "url" : "/news/downtime/archive",
  "description" : "Upgrades/expansion",
  "date" : "Monday, December 2 2019",
  "content" : "Ceres downtime is scheduled for  Monday, December 2 – Friday, December 6. This downtime is to rewire both power and networking on Ceres for the addition of additional compute nodes and to ready it for storage expansion.  We do not anticipate any further extended downtimes for rewiring, as this should allow us to maximize the size of Ceres simply by adding additional compute nodes. Since this affects the Authentication for SCINet, this will also affect logins to Data Transfer nodes at Ames, StoneVille, Fort Collins, Clay Center, Albany CA, and Beltsville.GlobalNoc will also be upgrading software on the SCINet network infrastructure during this time."
},

    {
      "title" : "",
      "category" : "",
      "subcategory" : "",
      "set" : "",
      "url" : "",
      "description" : "",
      "date" : "",
      "content" : ""
  }
]